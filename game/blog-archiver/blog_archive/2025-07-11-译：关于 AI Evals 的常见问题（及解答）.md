---
title: "è¯‘ï¼šå…³äº AI Evals çš„å¸¸è§é—®é¢˜ï¼ˆåŠè§£ç­”ï¼‰"
date: 2025-07-11
url: https://sorrycc.com/evals-faq
---

å‘å¸ƒäº 2025å¹´7æœˆ11æ—¥

# è¯‘ï¼šå…³äº AI Evals çš„å¸¸è§é—®é¢˜ï¼ˆåŠè§£ç­”ï¼‰

> åŸæ–‡ï¼š [https://hamel.dev/blog/posts/evals-faq/](https://hamel.dev/blog/posts/evals-faq/)  
> ä½œè€…ï¼š Hamel Husain, Shreya Shankar  
> è¯‘è€…ï¼š Gemini 2.5 Pro

## ç›®å½•

*   [Q: What are LLM Evals?](https://hamel.dev/blog/posts/evals-faq/#q-what-are-llm-evals)
*   [Q: Is RAG dead?](https://hamel.dev/blog/posts/evals-faq/#q-is-rag-dead)
*   [Q: Can I use the same model for both the main task and evaluation?](https://hamel.dev/blog/posts/evals-faq/#q-can-i-use-the-same-model-for-both-the-main-task-and-evaluation)
*   [Q: How much time should I spend on model selection?](https://hamel.dev/blog/posts/evals-faq/#q-how-much-time-should-i-spend-on-model-selection)
*   [Q: Should I build a custom annotation tool or use something off-the-shelf?](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf)
*   [Q: Why do you recommend binary (pass/fail) evaluations instead of 1-5 ratings (Likert scales)?](https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales)
*   [Q: How do I debug multi-turn conversation traces?](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-debug-multi-turn-conversation-traces)
*   [Q: Should I build automated evaluators for every failure mode I find?](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find)
*   [Q: How many people should annotate my LLM outputs?](https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs)
*   [Q: What gaps in eval tooling should I be prepared to fill myself?](https://hamel.dev/blog/posts/evals-faq/#q-what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself)
*   [Q: What is the best approach for generating synthetic data?](https://hamel.dev/blog/posts/evals-faq/#q-what-is-the-best-approach-for-generating-synthetic-data)
*   [Q: How do I approach evaluation when my system handles diverse user queries?](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries)
*   [Q: How do I choose the right chunk size for my document processing tasks?](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-choose-the-right-chunk-size-for-my-document-processing-tasks)
*   [Q: How should I approach evaluating my RAG system?](https://hamel.dev/blog/posts/evals-faq/#q-how-should-i-approach-evaluating-my-rag-system)
*   [Q: What makes a good custom interface for reviewing LLM outputs?](https://hamel.dev/blog/posts/evals-faq/#q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs)
*   [Q: How much of my development budget should I allocate to evals?](https://hamel.dev/blog/posts/evals-faq/#q-how-much-of-my-development-budget-should-i-allocate-to-evals)
*   [Q: Why is â€œerror analysisâ€ so important in LLM evals, and how is it performed?](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed)
*   [Q: Whatâ€™s the difference between guardrails & evaluators?](https://hamel.dev/blog/posts/evals-faq/#q-whats-the-difference-between-guardrails-evaluators)
*   [Q: Whatâ€™s a minimum viable evaluation setup?](https://hamel.dev/blog/posts/evals-faq/#q-whats-a-minimum-viable-evaluation-setup)
*   [Q: How do I evaluate agentic workflows?](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-evaluate-agentic-workflows)
*   [Q: Seriously Hamel. Stop the bullshit. Whatâ€™s your favorite eval vendor?](https://hamel.dev/blog/posts/evals-faq/#q-seriously-hamel.-stop-the-bullshit.-whats-your-favorite-eval-vendor)
*   [Q: How are evaluations used differently in CI/CD vs.Â monitoring production?](https://hamel.dev/blog/posts/evals-faq/#q-how-are-evaluations-used-differently-in-cicd-vs.-monitoring-production)
*   [Q: Are similarity metrics (BERTScore, ROUGE, etc.) useful for evaluating LLM outputs?](https://hamel.dev/blog/posts/evals-faq/#q-are-similarity-metrics-bertscore-rouge-etc.-useful-for-evaluating-llm-outputs)
*   [Q: Should I use â€œready-to-useâ€ evaluation metrics?](https://hamel.dev/blog/posts/evals-faq/#q-should-i-use-ready-to-use-evaluation-metrics)
*   [Q: How can I efficiently sample production traces for review?](https://hamel.dev/blog/posts/evals-faq/#q-how-can-i-efficiently-sample-production-traces-for-review)
*   [é—®ï¼šä»€ä¹ˆæ˜¯ LLM Evalsï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-what-are-llm-evals)
*   [é—®ï¼šRAG å·²æ­»å—ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-is-rag-dead)
*   [é—®ï¼šæˆ‘èƒ½ç”¨åŒä¸€ä¸ªæ¨¡å‹æ¥å¤„ç†ä¸»ä»»åŠ¡å’Œè¯„ä¼°å—ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-can-i-use-the-same-model-for-both-the-main-task-and-evaluation)
*   [é—®ï¼šæˆ‘åº”è¯¥èŠ±å¤šå°‘æ—¶é—´åœ¨æ¨¡å‹é€‰æ‹©ä¸Šï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-much-time-should-i-spend-on-model-selection)
*   [é—®ï¼šæˆ‘åº”è¯¥è‡ªå»ºæ ‡æ³¨å·¥å…·è¿˜æ˜¯ç”¨ç°æˆçš„ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf)
*   [é—®ï¼šä¸ºä»€ä¹ˆä½ æ¨èäºŒå…ƒï¼ˆé€šè¿‡/å¤±è´¥ï¼‰è¯„ä¼°ï¼Œè€Œä¸æ˜¯ 1-5 åˆ†åˆ¶ï¼ˆæå…‹ç‰¹é‡è¡¨ï¼‰ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales)
*   [é—®ï¼šæˆ‘è¯¥å¦‚ä½•è°ƒè¯•å¤šè½®å¯¹è¯çš„ traceï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-debug-multi-turn-conversation-traces)
*   [é—®ï¼šæˆ‘åº”è¯¥ä¸ºå‘ç°çš„æ¯ä¸€ç§å¤±è´¥æ¨¡å¼éƒ½æ„å»ºè‡ªåŠ¨åŒ–è¯„ä¼°å™¨å—ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find)
*   [é—®ï¼šåº”è¯¥æœ‰å¤šå°‘äººæ¥æ ‡æ³¨æˆ‘çš„ LLM è¾“å‡ºï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs)
*   [é—®ï¼šæˆ‘åº”è¯¥å‡†å¤‡å¥½è‡ªå·±å¡«è¡¥è¯„ä¼°å·¥å…·ä¸­çš„å“ªäº›ç©ºç™½ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself)
*   [é—®ï¼šç”Ÿæˆåˆæˆæ•°æ®çš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-what-is-the-best-approach-for-generating-synthetic-data)
*   [é—®ï¼šå½“æˆ‘çš„ç³»ç»Ÿå¤„ç†å¤šæ ·åŒ–çš„ç”¨æˆ·æŸ¥è¯¢æ—¶ï¼Œæˆ‘è¯¥å¦‚ä½•è¿›è¡Œè¯„ä¼°ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries)
*   [é—®ï¼šå¦‚ä½•ä¸ºæˆ‘çš„æ–‡æ¡£å¤„ç†ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ chunk sizeï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-choose-the-right-chunk-size-for-my-document-processing-tasks)
*   [é—®ï¼šæˆ‘åº”è¯¥å¦‚ä½•è¯„ä¼°æˆ‘çš„ RAG ç³»ç»Ÿï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-should-i-approach-evaluating-my-rag-system)
*   [é—®ï¼šä¸€ä¸ªå¥½çš„ç”¨äºå®¡æŸ¥ LLM è¾“å‡ºçš„è‡ªå®šä¹‰ç•Œé¢æ˜¯æ€æ ·çš„ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs)
*   [é—®ï¼šæˆ‘åº”è¯¥å°†å¤šå°‘å¼€å‘é¢„ç®—åˆ†é…ç»™è¯„ä¼°ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-much-of-my-development-budget-should-i-allocate-to-evals)
*   [é—®ï¼šä¸ºä»€ä¹ˆâ€œé”™è¯¯åˆ†æâ€åœ¨ LLM è¯„ä¼°ä¸­å¦‚æ­¤é‡è¦ï¼Œä»¥åŠå¦‚ä½•è¿›è¡Œï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed)
*   [é—®ï¼šGuardrails å’Œ Evaluators æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-whats-the-difference-between-guardrails-evaluators)
*   [é—®ï¼šæœ€å°å¯è¡Œçš„è¯„ä¼°é…ç½®æ˜¯æ€æ ·çš„ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-whats-a-minimum-viable-evaluation-setup)
*   [é—®ï¼šæˆ‘è¯¥å¦‚ä½•è¯„ä¼° Agentic å·¥ä½œæµï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-evaluate-agentic-workflows)
*   [é—®ï¼šè¯´çœŸçš„ï¼ŒHamelï¼Œåˆ«æ‰¯æ·¡äº†ã€‚ä½ æœ€å–œæ¬¢çš„è¯„ä¼°æœåŠ¡å•†æ˜¯å“ªå®¶ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-seriously-hamel.-stop-the-bullshit.-whats-your-favorite-eval-vendor)
*   [é—®ï¼šè¯„ä¼°åœ¨ CI/CD å’Œç”Ÿäº§ç›‘æ§ä¸­çš„ä½¿ç”¨æœ‰ä½•ä¸åŒï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-are-evaluations-used-differently-in-cicd-vs.-monitoring-production)
*   [é—®ï¼šç›¸ä¼¼åº¦æŒ‡æ ‡ï¼ˆBERTScoreã€ROUGE ç­‰ï¼‰å¯¹è¯„ä¼° LLM è¾“å‡ºæœ‰ç”¨å—ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-are-similarity-metrics-bertscore-rouge-etc.-useful-for-evaluating-llm-outputs)
*   [é—®ï¼šæˆ‘åº”è¯¥ä½¿ç”¨â€œå¼€ç®±å³ç”¨â€çš„è¯„ä¼°æŒ‡æ ‡å—ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-should-i-use-ready-to-use-evaluation-metrics)
*   [é—®ï¼šæˆ‘å¦‚ä½•èƒ½æœ‰æ•ˆåœ°ä»ç”Ÿäº§ trace ä¸­æŠ½æ ·è¿›è¡Œå®¡æŸ¥ï¼Ÿ](https://hamel.dev/blog/posts/evals-faq/#q-how-can-i-efficiently-sample-production-traces-for-review)

This document curates the most common questions Shreya and I received while [teaching](https://bit.ly/evals-ai) 700+ engineers & PMs AI Evals. _Warning: These are sharp opinions about what works in most cases. They are not universal truths. Use your judgment._

è¿™ä»½æ–‡æ¡£æ•´ç†äº† Shreya å’Œæˆ‘åœ¨å‘ 700 å¤šåå·¥ç¨‹å¸ˆå’Œäº§å“ç»ç†[æ•™æˆ](https://bit.ly/evals-ai) AI Evals æ—¶æ”¶åˆ°çš„æœ€å¸¸è§é—®é¢˜ã€‚_è­¦å‘Šï¼šè¿™äº›æ˜¯å…³äºåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä»€ä¹ˆæ–¹æ³•æœ‰æ•ˆçš„å°–é”è§‚ç‚¹ã€‚å®ƒä»¬å¹¶éæ™®é€‚çœŸç†ã€‚è¯·è‡ªè¡Œåˆ¤æ–­ã€‚_

* * *

**ğŸ‘‰ _We are teaching our last and final cohort of our [AI Evals course](https://bit.ly/evals-ai) next month_** _(we have to get back to building). Here is a [35% discount code](https://bit.ly/evals-ai) for readers._ ğŸ‘ˆ

**ğŸ‘‰ _ä¸‹ä¸ªæœˆæˆ‘ä»¬å°†æ•™æˆæˆ‘ä»¬ [AI Evals è¯¾ç¨‹](https://bit.ly/evals-ai)çš„æœ€åä¸€æœŸå­¦å‘˜_** _ï¼ˆæˆ‘ä»¬å¾—å›å»ç»§ç»­æå¼€å‘äº†ï¼‰ã€‚è¿™é‡Œæ˜¯ä¸ºè¯»è€…å‡†å¤‡çš„ [35% æŠ˜æ‰£ç ](https://bit.ly/evals-ai)ã€‚_ ğŸ‘ˆ

* * *

## é—®ï¼šä»€ä¹ˆæ˜¯ LLM Evalsï¼Ÿ

If you are **completely new** to product-specific LLM evals (not foundation model benchmarks), see these posts: [part 1](https://hamel.dev/evals), [part 2](https://hamel.dev/llm-judge/), [part 3](https://hamel.dev/field-guide). Otherwise, keep reading.

å¦‚æœä½ å¯¹é¢å‘ç‰¹å®šäº§å“çš„ LLM è¯„ä¼°ï¼ˆè€ŒéåŸºç¡€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼‰**å®Œå…¨é™Œç”Ÿ**ï¼Œè¯·çœ‹è¿™å‡ ç¯‡æ–‡ç« ï¼š[ç¬¬ä¸€éƒ¨åˆ†](https://hamel.dev/evals)ã€[ç¬¬äºŒéƒ¨åˆ†](https://hamel.dev/llm-judge/)ã€[ç¬¬ä¸‰éƒ¨åˆ†](https://hamel.dev/field-guide)ã€‚å¦åˆ™ï¼Œè¯·ç»§ç»­é˜…è¯»ã€‚

## é—®ï¼šRAG å·²æ­»å—ï¼Ÿ

Question: Should I avoid using RAG for my AI application after reading that [â€œRAG is deadâ€](https://pashpashpash.substack.com/p/why-i-no-longer-recommend-rag-for) for coding agents?

> Many developers are confused about when and how to use RAG after reading articles claiming â€œRAG is dead.â€ Understanding what RAG actually means versus the narrow marketing definitions will help you make better architectural decisions for your AI applications.

é—®ï¼šåœ¨è¯»åˆ°ä¸€ç¯‡å…³äºç¼–ç  agent çš„æ–‡ç« è¯´ [â€œRAG å·²æ­»â€](https://pashpashpash.substack.com/p/why-i-no-longer-recommend-rag-for) ä¹‹åï¼Œæˆ‘åº”è¯¥åœ¨æˆ‘çš„ AI åº”ç”¨ä¸­é¿å…ä½¿ç”¨ RAG å—ï¼Ÿ

> è®¸å¤šå¼€å‘è€…åœ¨è¯»äº†é‚£äº›å£°ç§°â€œRAG å·²æ­»â€çš„æ–‡ç« åï¼Œå¯¹ä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨ RAG æ„Ÿåˆ°å›°æƒ‘ã€‚ç†è§£ RAG çš„çœŸæ­£å«ä¹‰ï¼Œè€Œä¸æ˜¯é‚£äº›ç‹­éš˜çš„è¥é”€å®šä¹‰ï¼Œå°†å¸®åŠ©ä½ ä¸ºä½ çš„ AI åº”ç”¨åšå‡ºæ›´å¥½çš„æ¶æ„å†³ç­–ã€‚

The viral article claiming RAG is dead specifically argues against using _naive vector database retrieval_ for autonomous coding agents, not RAG as a whole. This is a crucial distinction that many developers miss due to misleading marketing.

é‚£ç¯‡å®£ç§° RAG å·²æ­»çš„çˆ†æ¬¾æ–‡ç« ï¼Œç‰¹æŒ‡åå¯¹ä¸ºè‡ªä¸»ç¼–ç  agent ä½¿ç”¨_å¹¼ç¨šçš„å‘é‡æ•°æ®åº“æ£€ç´¢_ï¼Œè€Œä¸æ˜¯åå¯¹æ•´ä¸ª RAGã€‚è¿™æ˜¯ä¸€ä¸ªå…³é”®çš„åŒºåˆ«ï¼Œè®¸å¤šå¼€å‘è€…å› ä¸ºè¯¯å¯¼æ€§çš„è¥é”€è€Œå¿½ç•¥äº†è¿™ä¸€ç‚¹ã€‚

RAG simply means Retrieval-Augmented Generation - using retrieval to provide relevant context that improves your modelâ€™s output. The core principle remains essential: your LLM needs the right context to generate accurate answers. The question isnâ€™t whether to use retrieval, but how to retrieve effectively.

RAG çš„æ„æ€å¾ˆç®€å•ï¼Œå°±æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰â€”â€”åˆ©ç”¨æ£€ç´¢æ¥æä¾›ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä»¥æ”¹å–„æ¨¡å‹çš„è¾“å‡ºã€‚å…¶æ ¸å¿ƒåŸåˆ™å§‹ç»ˆè‡³å…³é‡è¦ï¼šä½ çš„ LLM éœ€è¦æ­£ç¡®çš„ä¸Šä¸‹æ–‡æ‰èƒ½ç”Ÿæˆå‡†ç¡®çš„ç­”æ¡ˆã€‚é—®é¢˜ä¸åœ¨äºæ˜¯å¦ä½¿ç”¨æ£€ç´¢ï¼Œè€Œåœ¨äºå¦‚ä½•æœ‰æ•ˆåœ°æ£€ç´¢ã€‚

For coding applications, naive vector similarity search often fails because code relationships are complex and contextual. Instead of abandoning retrieval entirely, modern coding assistants like Claude Code [still uses retrieval](https://x.com/pashmerepat/status/1926717705660375463?s=46) â€”they just employ agentic search instead of relying solely on vector databases, similar to how human developers work.

å¯¹äºç¼–ç åº”ç”¨ï¼Œå¹¼ç¨šçš„å‘é‡ç›¸ä¼¼æ€§æœç´¢å¸¸å¸¸å¤±è´¥ï¼Œå› ä¸ºä»£ç å…³ç³»å¤æ‚ä¸”ä¾èµ–ä¸Šä¸‹æ–‡ã€‚ç°ä»£çš„ç¼–ç åŠ©æ‰‹ï¼Œå¦‚ Claude Codeï¼Œå¹¶æ²¡æœ‰å®Œå…¨æ”¾å¼ƒæ£€ç´¢ï¼Œå®ƒä»¬[ä»ç„¶åœ¨ä½¿ç”¨æ£€ç´¢](https://x.com/pashmerepat/status/1926717705660375463?s=46)â€”â€”åªæ˜¯å®ƒä»¬é‡‡ç”¨äº† agentic searchï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–å‘é‡æ•°æ®åº“ï¼Œè¿™ä¸äººç±»å¼€å‘è€…çš„å·¥ä½œæ–¹å¼ç›¸ä¼¼ã€‚

You have multiple retrieval strategies available, ranging from simple keyword matching to embedding similarity to LLM-powered relevance filtering. The optimal approach depends on your specific use case, data characteristics, and performance requirements. Many production systems combine multiple strategies or use multi-hop retrieval guided by LLM agents.

ä½ æœ‰å¤šç§æ£€ç´¢ç­–ç•¥å¯é€‰ï¼Œä»ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼Œåˆ° embedding ç›¸ä¼¼åº¦ï¼Œå†åˆ°ç”± LLM é©±åŠ¨çš„ç›¸å…³æ€§è¿‡æ»¤ã€‚æœ€ä½³æ–¹æ³•å–å†³äºä½ çš„å…·ä½“ç”¨ä¾‹ã€æ•°æ®ç‰¹æ€§å’Œæ€§èƒ½è¦æ±‚ã€‚è®¸å¤šç”Ÿäº§ç³»ç»Ÿä¼šç»“åˆå¤šç§ç­–ç•¥ï¼Œæˆ–ä½¿ç”¨ç”± LLM agent æŒ‡å¯¼çš„å¤šè·³æ£€ç´¢ã€‚

Unfortunately, â€œRAGâ€ has become a buzzword with no shared definition. Some people use it to mean any retrieval system, others restrict it to vector databases. Focus on the ultimate goal: getting your LLM the context it needs to succeed. Whether thatâ€™s through vector search, agentic exploration, or hybrid approaches is a product and engineering decision.

ä¸å¹¸çš„æ˜¯ï¼Œâ€œRAGâ€å·²ç»æˆäº†ä¸€ä¸ªæ²¡æœ‰å…±åŒå®šä¹‰çš„æµè¡Œè¯ã€‚æœ‰äº›äººç”¨å®ƒæŒ‡ä»£ä»»ä½•æ£€ç´¢ç³»ç»Ÿï¼Œå¦ä¸€äº›äººåˆ™å°†å…¶é™å®šäºå‘é‡æ•°æ®åº“ã€‚ä½ åº”è¯¥ä¸“æ³¨äºæœ€ç»ˆç›®æ ‡ï¼šä¸ºä½ çš„ LLM æä¾›æˆåŠŸæ‰€éœ€çš„ä¸Šä¸‹æ–‡ã€‚è‡³äºé€šè¿‡å‘é‡æœç´¢ã€agentic æ¢ç´¢è¿˜æ˜¯æ··åˆæ–¹æ³•æ¥å®ç°ï¼Œåˆ™æ˜¯ä¸€ä¸ªäº§å“å’Œå·¥ç¨‹å†³ç­–ã€‚

Rather than following categorical advice to avoid or embrace RAG, experiment with different retrieval approaches and measure what works best for your application.

ä¸å…¶ç›²ä»é‚£äº›è¦ä¹ˆé¿å…è¦ä¹ˆæ‹¥æŠ± RAG çš„ç»å¯¹å»ºè®®ï¼Œä¸å¦‚å»è¯•éªŒä¸åŒçš„æ£€ç´¢æ–¹æ³•ï¼Œå¹¶è¡¡é‡å“ªç§æœ€é€‚åˆä½ çš„åº”ç”¨ã€‚

## é—®ï¼šæˆ‘èƒ½ç”¨åŒä¸€ä¸ªæ¨¡å‹æ¥å¤„ç†ä¸»ä»»åŠ¡å’Œè¯„ä¼°å—ï¼Ÿ

For LLM-as-Judge selection, using the same model is usually fine because the judge is doing a different task than your main LLM pipeline. The judges we recommend building do [scoped binary classification tasks](https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales). Focus on achieving high True Positive Rate (TPR) and True Negative Rate (TNR) with your judge on a held out labeled test set rather than avoiding the same model family. You can use these metrics on the test set to understand how well your judge is doing.

å¯¹äº LLM-as-Judge çš„é€‰æ‹©ï¼Œä½¿ç”¨ç›¸åŒçš„æ¨¡å‹é€šå¸¸æ²¡é—®é¢˜ï¼Œå› ä¸ºè¯„åˆ¤æ¨¡å‹æ‰§è¡Œçš„ä»»åŠ¡ä¸ä½ çš„ä¸» LLM æµæ°´çº¿ä¸åŒã€‚æˆ‘ä»¬æ¨èæ„å»ºçš„è¯„åˆ¤æ¨¡å‹æ‰§è¡Œçš„æ˜¯[é™å®šèŒƒå›´çš„äºŒå…ƒåˆ†ç±»ä»»åŠ¡](https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales)ã€‚ä½ åº”è¯¥ä¸“æ³¨äºåœ¨ç•™å­˜çš„å·²æ ‡æ³¨æµ‹è¯•é›†ä¸Šï¼Œè®©ä½ çš„è¯„åˆ¤æ¨¡å‹è·å¾—è¾ƒé«˜çš„çœŸæ­£ä¾‹ç‡ï¼ˆTPRï¼‰å’ŒçœŸè´Ÿä¾‹ç‡ï¼ˆTNRï¼‰ï¼Œè€Œä¸æ˜¯åˆ»æ„é¿å…ä½¿ç”¨åŒç³»åˆ—çš„æ¨¡å‹ã€‚ä½ å¯ä»¥ç”¨è¿™äº›åœ¨æµ‹è¯•é›†ä¸Šçš„æŒ‡æ ‡æ¥äº†è§£ä½ çš„è¯„åˆ¤æ¨¡å‹è¡¨ç°å¦‚ä½•ã€‚

When selecting judge models, start with the most capable models available to establish strong alignment with human judgments. You can optimize for cost later once youâ€™ve established reliable evaluation criteria. We do not recommend using the same model for open ended preferences or response quality (but we donâ€™t recommend building judges this way in the first place!).

åœ¨é€‰æ‹©è¯„åˆ¤æ¨¡å‹æ—¶ï¼Œå…ˆä»æœ€å¼ºå¤§çš„æ¨¡å‹å¼€å§‹ï¼Œä»¥å»ºç«‹ä¸äººç±»åˆ¤æ–­çš„é«˜åº¦ä¸€è‡´æ€§ã€‚ä¸€æ—¦ä½ å»ºç«‹äº†å¯é çš„è¯„ä¼°æ ‡å‡†ï¼Œä¹‹åå†ä¼˜åŒ–æˆæœ¬ã€‚æˆ‘ä»¬ä¸æ¨èå°†åŒä¸€æ¨¡å‹ç”¨äºå¼€æ”¾å¼åå¥½æˆ–å“åº”è´¨é‡çš„è¯„ä¼°ï¼ˆä½†æˆ‘ä»¬é¦–å…ˆå°±ä¸æ¨èä»¥è¿™ç§æ–¹å¼æ„å»ºè¯„åˆ¤æ¨¡å‹ï¼ï¼‰ã€‚

## é—®ï¼šæˆ‘åº”è¯¥èŠ±å¤šå°‘æ—¶é—´åœ¨æ¨¡å‹é€‰æ‹©ä¸Šï¼Ÿ

Many developers fixate on model selection as the primary way to improve their LLM applications. Start with error analysis to understand your failure modes before considering model switching. As Hamel noted in office hours, â€œI suggest not thinking of switching model as the main axes of how to improve your system off the bat without evidence. Does error analysis suggest that your model is the problem?â€

è®¸å¤šå¼€å‘è€…æ‰§ç€äºå°†æ¨¡å‹é€‰æ‹©ä½œä¸ºæå‡å…¶ LLM åº”ç”¨çš„ä¸»è¦æ–¹å¼ã€‚åœ¨è€ƒè™‘æ›´æ¢æ¨¡å‹ä¹‹å‰ï¼Œå…ˆä»é”™è¯¯åˆ†æå…¥æ‰‹ï¼Œäº†è§£ä½ çš„å¤±è´¥æ¨¡å¼ã€‚æ­£å¦‚ Hamel åœ¨ç­”ç–‘æ—¶é—´æŒ‡å‡ºçš„ï¼šâ€œæˆ‘å»ºè®®ï¼Œåœ¨æ²¡æœ‰è¯æ®çš„æƒ…å†µä¸‹ï¼Œä¸è¦ä¸€å¼€å§‹å°±æŠŠæ›´æ¢æ¨¡å‹å½“ä½œæå‡ç³»ç»Ÿçš„ä¸»è¦é€”å¾„ã€‚é”™è¯¯åˆ†ææ˜¯å¦è¡¨æ˜ä½ çš„æ¨¡å‹æ˜¯é—®é¢˜æ‰€åœ¨ï¼Ÿâ€

## é—®ï¼šæˆ‘åº”è¯¥è‡ªå»ºæ ‡æ³¨å·¥å…·è¿˜æ˜¯ç”¨ç°æˆçš„ï¼Ÿ

**Build a custom annotation tool.** This is the single most impactful investment you can make for your AI evaluation workflow. With AI-assisted development tools like Cursor or Lovable, you can build a tailored interface in hours. I often find that teams with custom annotation tools iterate ~10x faster.

**è‡ªå»ºä¸€ä¸ªå®šåˆ¶çš„æ ‡æ³¨å·¥å…·ã€‚** è¿™æ˜¯ä½ èƒ½ä¸ºä½ çš„ AI è¯„ä¼°å·¥ä½œæµåšçš„æœ€å…·å½±å“åŠ›çš„å•é¡¹æŠ•èµ„ã€‚å€ŸåŠ©åƒ Cursor æˆ– Lovable è¿™æ ·çš„ AI è¾…åŠ©å¼€å‘å·¥å…·ï¼Œä½ å¯ä»¥åœ¨å‡ å°æ—¶å†…æ„å»ºå‡ºä¸€ä¸ªé‡èº«å®šåˆ¶çš„ç•Œé¢ã€‚æˆ‘å¸¸å¸¸å‘ç°ï¼Œæ‹¥æœ‰å®šåˆ¶æ ‡æ³¨å·¥å…·çš„å›¢é˜Ÿè¿­ä»£é€Ÿåº¦å¿«äº†çº¦ 10 å€ã€‚

Custom tools excel because:

*   They show all your context from multiple systems in one place
*   They can render your data in a product specific way (images, widgets, markdown, buttons, etc.)
*   Theyâ€™re designed for your specific workflow (custom filters, sorting, progress bars, etc.)

å®šåˆ¶å·¥å…·ä¹‹æ‰€ä»¥å‡ºè‰²ï¼Œæ˜¯å› ä¸ºï¼š

*   å®ƒä»¬èƒ½å°†æ¥è‡ªå¤šä¸ªç³»ç»Ÿçš„æ‰€æœ‰ä¸Šä¸‹æ–‡é›†ä¸­å±•ç¤ºåœ¨ä¸€å¤„ã€‚
*   å®ƒä»¬èƒ½ä»¥äº§å“ç‰¹å®šçš„æ–¹å¼æ¸²æŸ“ä½ çš„æ•°æ®ï¼ˆå›¾ç‰‡ã€å°éƒ¨ä»¶ã€Markdownã€æŒ‰é’®ç­‰ï¼‰ã€‚
*   å®ƒä»¬æ˜¯ä¸ºä½ ç‰¹å®šçš„å·¥ä½œæµç¨‹è®¾è®¡çš„ï¼ˆè‡ªå®šä¹‰ç­›é€‰ã€æ’åºã€è¿›åº¦æ¡ç­‰ï¼‰ã€‚

Off-the-shelf tools may be justified when you need to coordinate dozens of distributed annotators with enterprise access controls. Even then, many teams find the configuration overhead and limitations arenâ€™t worth it.

åªæœ‰å½“ä½ éœ€è¦åè°ƒæ•°åä¸ªåˆ†å¸ƒå¼æ ‡æ³¨å‘˜å¹¶éœ€è¦ä¼ä¸šçº§è®¿é—®æ§åˆ¶æ—¶ï¼Œä½¿ç”¨ç°æˆå·¥å…·æˆ–è®¸æ‰ç®—åˆç†ã€‚å³ä¾¿å¦‚æ­¤ï¼Œè®¸å¤šå›¢é˜Ÿä¹Ÿå‘ç°å…¶é…ç½®å¼€é”€å’ŒåŠŸèƒ½é™åˆ¶å¾—ä¸å¿å¤±ã€‚

[Isaacâ€™s Anki flashcard annotation app](https://youtu.be/fA4pe9bE0LY) shows the power of custom toolsâ€”handling 400+ results per query with keyboard navigation and domain-specific evaluation criteria that would be nearly impossible to configure in a generic tool.

[Isaac çš„ Anki æŠ½è®¤å¡æ ‡æ³¨åº”ç”¨](https://youtu.be/fA4pe9bE0LY)å±•ç¤ºäº†å®šåˆ¶å·¥å…·çš„åŠ›é‡â€”â€”å®ƒèƒ½å¤„ç†æ¯ä¸ªæŸ¥è¯¢è¶…è¿‡ 400 æ¡ç»“æœï¼Œæ”¯æŒé”®ç›˜å¯¼èˆªï¼Œå¹¶åŒ…å«é¢†åŸŸç‰¹å®šçš„è¯„ä¼°æ ‡å‡†ï¼Œè¿™äº›åœ¨é€šç”¨å·¥å…·ä¸­å‡ ä¹ä¸å¯èƒ½é…ç½®å‡ºæ¥ã€‚

## é—®ï¼šä¸ºä»€ä¹ˆä½ æ¨èäºŒå…ƒï¼ˆé€šè¿‡/å¤±è´¥ï¼‰è¯„ä¼°ï¼Œè€Œä¸æ˜¯ 1-5 åˆ†åˆ¶ï¼ˆæå…‹ç‰¹é‡è¡¨ï¼‰ï¼Ÿ

> Engineers often believe that Likert scales (1-5 ratings) provide more information than binary evaluations, allowing them to track gradual improvements. However, this added complexity often creates more problems than it solves in practice.

> å·¥ç¨‹å¸ˆä»¬å¸¸å¸¸è®¤ä¸ºï¼Œæå…‹ç‰¹é‡è¡¨ï¼ˆ1-5 åˆ†åˆ¶ï¼‰æ¯”äºŒå…ƒè¯„ä¼°æä¾›æ›´å¤šä¿¡æ¯ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿè¿½è¸ªæ¸è¿›å¼çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œè¿™ç§å¢åŠ çš„å¤æ‚æ€§å¾€å¾€å¼Šå¤§äºåˆ©ã€‚

Binary evaluations force clearer thinking and more consistent labeling. Likert scales introduce significant challenges: the difference between adjacent points (like 3 vs 4) is subjective and inconsistent across annotators, detecting statistical differences requires larger sample sizes, and annotators often default to middle values to avoid making hard decisions.

äºŒå…ƒè¯„ä¼°è¿«ä½¿ä½ è¿›è¡Œæ›´æ¸…æ™°çš„æ€è€ƒå’Œæ›´ä¸€è‡´çš„æ ‡æ³¨ã€‚æå…‹ç‰¹é‡è¡¨åˆ™å¸¦æ¥äº†æ˜¾è‘—çš„æŒ‘æˆ˜ï¼šç›¸é‚»åˆ†æ•°ï¼ˆå¦‚ 3 åˆ†ä¸ 4 åˆ†ï¼‰ä¹‹é—´çš„å·®å¼‚æ˜¯ä¸»è§‚çš„ï¼Œåœ¨ä¸åŒæ ‡æ³¨å‘˜ä¹‹é—´ä¸ä¸€è‡´ï¼›æ£€æµ‹ç»Ÿè®¡ä¸Šçš„å·®å¼‚éœ€è¦æ›´å¤§çš„æ ·æœ¬é‡ï¼›è€Œä¸”æ ‡æ³¨å‘˜å¸¸å¸¸ä¸ºäº†å›é¿è‰°éš¾çš„æŠ‰æ‹©è€Œé»˜è®¤é€‰æ‹©ä¸­é—´å€¼ã€‚

Having binary options forces people to make a decision rather than hiding uncertainty in middle values. Binary decisions are also faster to make during error analysis - you donâ€™t waste time debating whether something is a 3 or 4.

äºŒå…ƒé€‰é¡¹è¿«ä½¿äººä»¬åšå‡ºå†³å®šï¼Œè€Œä¸æ˜¯å°†ä¸ç¡®å®šæ€§éšè—åœ¨ä¸­é—´å€¼é‡Œã€‚åœ¨é”™è¯¯åˆ†ææ—¶ï¼ŒåšäºŒå…ƒå†³ç­–ä¹Ÿæ›´å¿«â€”â€”ä½ ä¸ç”¨æµªè´¹æ—¶é—´å»äº‰è®ºæŸä¸ªä¸œè¥¿åˆ°åº•æ˜¯ 3 åˆ†è¿˜æ˜¯ 4 åˆ†ã€‚

For tracking gradual improvements, consider measuring specific sub-components with their own binary checks rather than using a scale. For example, instead of rating factual accuracy 1-5, you could track â€œ4 out of 5 expected facts includedâ€ as separate binary checks. This preserves the ability to measure progress while maintaining clear, objective criteria.

ä¸ºäº†è¿½è¸ªæ¸è¿›å¼çš„æ”¹è¿›ï¼Œå¯ä»¥è€ƒè™‘ç”¨å„è‡ªçš„äºŒå…ƒæ£€æŸ¥æ¥è¡¡é‡ç‰¹å®šçš„å­ç»„ä»¶ï¼Œè€Œä¸æ˜¯ä½¿ç”¨é‡è¡¨ã€‚ä¾‹å¦‚ï¼Œä¸å…¶ç»™äº‹å®å‡†ç¡®æ€§æ‰“ 1-5 åˆ†ï¼Œä½ å¯ä»¥å°†â€œåŒ…å« 5 ä¸ªé¢„æœŸäº‹å®ä¸­çš„ 4 ä¸ªâ€ä½œä¸ºç‹¬ç«‹çš„äºŒå…ƒæ£€æŸ¥æ¥è¿½è¸ªã€‚è¿™æ—¢ä¿ç•™äº†è¡¡é‡è¿›å±•çš„èƒ½åŠ›ï¼Œåˆç»´æŒäº†æ¸…æ™°ã€å®¢è§‚çš„æ ‡å‡†ã€‚

Start with binary labels to understand what â€˜badâ€™ looks like. Numeric labels are advanced and usually not necessary.

ä»äºŒå…ƒæ ‡ç­¾å¼€å§‹ï¼Œä»¥ç†è§£â€œå·®â€æ˜¯ä»€ä¹ˆæ ·çš„ã€‚æ•°å­—æ ‡ç­¾æ˜¯é«˜çº§ç”¨æ³•ï¼Œé€šå¸¸æ²¡æœ‰å¿…è¦ã€‚

## é—®ï¼šæˆ‘è¯¥å¦‚ä½•è°ƒè¯•å¤šè½®å¯¹è¯çš„ traceï¼Ÿ

Start simple. Check if the whole conversation met the userâ€™s goal with a pass/fail judgment. Look at the entire trace and focus on the first upstream failure. Read the user-visible parts first to understand if something went wrong. Only then dig into the technical details like tool calls and intermediate steps.

ä»ç®€å•çš„å¼€å§‹ã€‚ç”¨ä¸€ä¸ªâ€œé€šè¿‡/å¤±è´¥â€çš„åˆ¤æ–­æ¥æ£€æŸ¥æ•´ä¸ªå¯¹è¯æ˜¯å¦è¾¾æˆäº†ç”¨æˆ·çš„ç›®æ ‡ã€‚å®¡è§†æ•´ä¸ª traceï¼Œå¹¶ä¸“æ³¨äºç¬¬ä¸€ä¸ªä¸Šæ¸¸çš„å¤±è´¥ç‚¹ã€‚é¦–å…ˆé˜…è¯»ç”¨æˆ·å¯è§çš„éƒ¨åˆ†ï¼Œçœ‹æ˜¯å¦å‡ºäº†é—®é¢˜ã€‚ä¹‹åå†æ·±å…¥ç ”ç©¶æŠ€æœ¯ç»†èŠ‚ï¼Œå¦‚å·¥å…·è°ƒç”¨å’Œä¸­é—´æ­¥éª¤ã€‚

When you find a failure, reproduce it with the simplest possible test case. Hereâ€™s an example: suppose a shopping bot gives the wrong return policy on turn 4 of a conversation. Before diving into the full multi-turn complexity, simplify it to a single turn: â€œWhat is the return window for product X1000?â€ If it still fails, youâ€™ve proven the error isnâ€™t about conversation context - itâ€™s likely a basic retrieval or knowledge issue you can debug more easily.

å½“ä½ å‘ç°ä¸€ä¸ªå¤±è´¥æ—¶ï¼Œç”¨æœ€ç®€å•çš„æµ‹è¯•ç”¨ä¾‹æ¥å¤ç°å®ƒã€‚ä¸¾ä¸ªä¾‹å­ï¼šå‡è®¾ä¸€ä¸ªè´­ç‰©æœºå™¨äººåœ¨å¯¹è¯çš„ç¬¬å››è½®ç»™å‡ºäº†é”™è¯¯çš„é€€è´§æ”¿ç­–ã€‚åœ¨æ·±å…¥ç ”ç©¶å¤æ‚çš„å¤šè½®å¯¹è¯ä¹‹å‰ï¼Œå…ˆæŠŠå®ƒç®€åŒ–ä¸ºå•è½®å¯¹è¯ï¼šâ€œäº§å“ X1000 çš„é€€è´§çª—å£æ˜¯å¤šä¹…ï¼Ÿâ€å¦‚æœå®ƒä»ç„¶å¤±è´¥ï¼Œä½ å°±è¯æ˜äº†è¿™ä¸ªé”™è¯¯ä¸å¯¹è¯ä¸Šä¸‹æ–‡æ— å…³â€”â€”å®ƒå¾ˆå¯èƒ½æ˜¯ä¸€ä¸ªåŸºæœ¬çš„æ£€ç´¢æˆ–çŸ¥è¯†é—®é¢˜ï¼Œè¿™æ ·è°ƒè¯•èµ·æ¥å°±å®¹æ˜“å¤šäº†ã€‚

For generating test cases, you have two main approaches. First, you can simulate users with another LLM to create realistic multi-turn conversations. Second, use â€œN-1 testingâ€ where you provide the first N-1 turns of a real conversation and test what happens next. The N-1 approach often works better since it uses actual conversation prefixes rather than fully synthetic interactions (but is less flexible and doesnâ€™t test the full conversation). User simulation is getting better as models improve. Keep an eye on this space.

åœ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ–¹é¢ï¼Œä½ æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•ã€‚ç¬¬ä¸€ï¼Œä½ å¯ä»¥ç”¨å¦ä¸€ä¸ª LLM æ¥æ¨¡æ‹Ÿç”¨æˆ·ï¼Œä»¥åˆ›å»ºçœŸå®çš„å¤šè½®å¯¹è¯ã€‚ç¬¬äºŒï¼Œä½¿ç”¨â€œN-1 æµ‹è¯•â€ï¼Œå³ä½ æä¾›çœŸå®å¯¹è¯çš„å‰ N-1 è½®ï¼Œç„¶åæµ‹è¯•æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆã€‚N-1 æ–¹æ³•é€šå¸¸æ•ˆæœæ›´å¥½ï¼Œå› ä¸ºå®ƒä½¿ç”¨çš„æ˜¯çœŸå®çš„å¯¹è¯å‰ç¼€ï¼Œè€Œä¸æ˜¯å®Œå…¨åˆæˆçš„äº¤äº’ï¼ˆä½†å®ƒçµæ´»æ€§è¾ƒå·®ï¼Œä¸”ä¸èƒ½æµ‹è¯•å®Œæ•´çš„å¯¹è¯ï¼‰ã€‚éšç€æ¨¡å‹çš„è¿›æ­¥ï¼Œç”¨æˆ·æ¨¡æ‹Ÿçš„æ•ˆæœæ­£åœ¨å˜å¥½ã€‚è¯·æŒç»­å…³æ³¨è¿™ä¸ªé¢†åŸŸã€‚

The key is balancing thoroughness with efficiency. Not every multi-turn failure requires multi-turn analysis.

å…³é”®æ˜¯åœ¨å½»åº•æ€§å’Œæ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚å¹¶éæ¯ä¸€ä¸ªå¤šè½®å¯¹è¯çš„å¤±è´¥éƒ½éœ€è¦è¿›è¡Œå¤šè½®åˆ†æã€‚

## é—®ï¼šæˆ‘åº”è¯¥ä¸ºå‘ç°çš„æ¯ä¸€ç§å¤±è´¥æ¨¡å¼éƒ½æ„å»ºè‡ªåŠ¨åŒ–è¯„ä¼°å™¨å—ï¼Ÿ

Focus automated evaluators on failures that persist after fixing your prompts. Many teams discover their LLM doesnâ€™t meet preferences they never actually specified - like wanting short responses, specific formatting, or step-by-step reasoning. Fix these obvious gaps first before building complex evaluation infrastructure.

æŠŠè‡ªåŠ¨åŒ–è¯„ä¼°å™¨é›†ä¸­åœ¨é‚£äº›ä¿®å¤äº† prompt ä¹‹åä¾ç„¶å­˜åœ¨çš„å¤±è´¥ä¸Šã€‚è®¸å¤šå›¢é˜Ÿå‘ç°ä»–ä»¬çš„ LLM ä¸ç¬¦åˆä»–ä»¬ä»æœªæ˜ç¡®æŒ‡å®šè¿‡çš„åå¥½â€”â€”æ¯”å¦‚æƒ³è¦ç®€çŸ­çš„å›ç­”ã€ç‰¹å®šçš„æ ¼å¼æˆ–åˆ†æ­¥æ¨ç†ã€‚åœ¨æ„å»ºå¤æ‚çš„è¯„ä¼°åŸºç¡€è®¾æ–½ä¹‹å‰ï¼Œå…ˆä¿®å¤è¿™äº›æ˜æ˜¾çš„ç¼ºå£ã€‚

Consider the cost hierarchy of different evaluator types. Simple assertions and reference-based checks (comparing against known correct answers) are cheap to build and maintain. LLM-as-Judge evaluators require 100+ labeled examples, ongoing weekly maintenance, and coordination between developers, PMs, and domain experts. This cost difference should shape your evaluation strategy.

è€ƒè™‘ä¸åŒç±»å‹è¯„ä¼°å™¨çš„æˆæœ¬å±‚çº§ã€‚ç®€å•çš„æ–­è¨€å’ŒåŸºäºå¼•ç”¨çš„æ£€æŸ¥ï¼ˆä¸å·²çŸ¥çš„æ­£ç¡®ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼‰æ„å»ºå’Œç»´æŠ¤æˆæœ¬ä½å»‰ã€‚è€Œ LLM-as-Judge è¯„ä¼°å™¨åˆ™éœ€è¦ 100 å¤šä¸ªæ ‡æ³¨æ ·æœ¬ã€æŒç»­çš„æ¯å‘¨ç»´æŠ¤ï¼Œä»¥åŠå¼€å‘äººå‘˜ã€äº§å“ç»ç†å’Œé¢†åŸŸä¸“å®¶ä¹‹é—´çš„åè°ƒã€‚è¿™ç§æˆæœ¬å·®å¼‚åº”è¯¥å½±å“ä½ çš„è¯„ä¼°ç­–ç•¥ã€‚

Only build expensive evaluators for problems youâ€™ll iterate on repeatedly. Since LLM-as-Judge comes with significant overhead, save it for persistent generalization failures - not issues you can fix trivially. Start with cheap code-based checks where possible: regex patterns, structural validation, or execution tests. Reserve complex evaluation for subjective qualities that canâ€™t be captured by simple rules.

åªä¸ºé‚£äº›ä½ éœ€è¦åå¤è¿­ä»£çš„é—®é¢˜æ„å»ºæ˜‚è´µçš„è¯„ä¼°å™¨ã€‚ç”±äº LLM-as-Judge å¸¦æœ‰å·¨å¤§çš„å¼€é”€ï¼Œæ‰€ä»¥æŠŠå®ƒç•™ç»™é‚£äº›æŒç»­å­˜åœ¨çš„æ³›åŒ–å¤±è´¥â€”â€”è€Œä¸æ˜¯é‚£äº›ä½ å¯ä»¥è½»æ˜“ä¿®å¤çš„é—®é¢˜ã€‚å°½å¯èƒ½ä»æˆæœ¬ä½å»‰çš„åŸºäºä»£ç çš„æ£€æŸ¥å¼€å§‹ï¼šæ­£åˆ™è¡¨è¾¾å¼ã€ç»“æ„éªŒè¯æˆ–æ‰§è¡Œæµ‹è¯•ã€‚å°†å¤æ‚çš„è¯„ä¼°ç•™ç»™é‚£äº›æ— æ³•ç”¨ç®€å•è§„åˆ™æ•æ‰çš„ä¸»è§‚è´¨é‡ã€‚

## é—®ï¼šåº”è¯¥æœ‰å¤šå°‘äººæ¥æ ‡æ³¨æˆ‘çš„ LLM è¾“å‡ºï¼Ÿ

For most small to medium-sized companies, appointing a single domain expert as a â€œbenevolent dictatorâ€ is the most effective approach. This personâ€”whether itâ€™s a psychologist for a mental health chatbot, a lawyer for legal document analysis, or a customer service director for support automationâ€”becomes the definitive voice on quality standards.

å¯¹äºå¤§å¤šæ•°ä¸­å°å‹å…¬å¸è€Œè¨€ï¼Œä»»å‘½ä¸€ä½é¢†åŸŸä¸“å®¶ä½œä¸ºâ€œä»æ…ˆçš„ç‹¬è£è€…â€æ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ³•ã€‚è¿™ä¸ªäººâ€”â€”æ— è®ºæ˜¯å¿ƒç†å¥åº·èŠå¤©æœºå™¨äººçš„å¿ƒç†å­¦å®¶ã€æ³•å¾‹æ–‡ä»¶åˆ†æçš„å¾‹å¸ˆï¼Œè¿˜æ˜¯æ”¯æŒè‡ªåŠ¨åŒ–çš„å®¢æœæ€»ç›‘â€”â€”å°†æˆä¸ºè´¨é‡æ ‡å‡†çš„æœ€ç»ˆæƒå¨ã€‚

A single expert eliminates annotation conflicts and prevents the paralysis that comes from â€œtoo many cooks in the kitchenâ€. The benevolent dictator can incorporate input and feedback from others, but they drive the process. If you feel like you need five subject matter experts to judge a single interaction, itâ€™s a sign your product scope might be too broad.

å•ä¸€ä¸“å®¶å¯ä»¥æ¶ˆé™¤æ ‡æ³¨å†²çªï¼Œé¿å…â€œå¨å­å¤šäº†çƒ§åæ±¤â€æ‰€å¸¦æ¥çš„ç˜«ç—ªã€‚è¿™ä½â€œä»æ…ˆçš„ç‹¬è£è€…â€å¯ä»¥é‡‡çº³ä»–äººçš„æ„è§å’Œåé¦ˆï¼Œä½†ç”±ä»–ä»¬æ¥ä¸»å¯¼æ•´ä¸ªè¿‡ç¨‹ã€‚å¦‚æœä½ è§‰å¾—ä½ éœ€è¦äº”ä½ä¸»é¢˜ä¸“å®¶æ¥è¯„åˆ¤ä¸€æ¬¡äº¤äº’ï¼Œè¿™å¯èƒ½è¡¨æ˜ä½ çš„äº§å“èŒƒå›´å¤ªå¹¿äº†ã€‚

However, larger organizations or those operating across multiple domains (like a multinational company with different cultural contexts) may need multiple annotators. When you do use multiple people, youâ€™ll need to measure their agreement using metrics like Cohenâ€™s Kappa, which accounts for agreement beyond chance. However, use your judgment. Even in larger companies, a single expert is often enough.

ç„¶è€Œï¼Œå¤§å‹ç»„ç»‡æˆ–è·¨å¤šä¸ªé¢†åŸŸè¿è¥çš„ç»„ç»‡ï¼ˆå¦‚å…·æœ‰ä¸åŒæ–‡åŒ–èƒŒæ™¯çš„è·¨å›½å…¬å¸ï¼‰å¯èƒ½éœ€è¦å¤šä½æ ‡æ³¨å‘˜ã€‚å½“ä½ ç¡®å®ä½¿ç”¨å¤šäººæ—¶ï¼Œä½ éœ€è¦ä½¿ç”¨åƒ Cohenâ€™s Kappa è¿™æ ·çš„æŒ‡æ ‡æ¥è¡¡é‡ä»–ä»¬çš„ä¸€è‡´æ€§ï¼Œè¯¥æŒ‡æ ‡è€ƒè™‘äº†è¶…å‡ºå¶ç„¶çš„ä¸€è‡´æ€§ã€‚ä¸è¿‡ï¼Œè¿˜æ˜¯è¦è¿ç”¨ä½ çš„åˆ¤æ–­ã€‚å³ä½¿åœ¨è¾ƒå¤§çš„å…¬å¸é‡Œï¼Œä¸€ä½ä¸“å®¶ä¹Ÿå¸¸å¸¸è¶³å¤Ÿäº†ã€‚

Start with a benevolent dictator whenever feasible. Only add complexity when your domain demands it.

åªè¦å¯è¡Œï¼Œå°±ä»ä¸€ä½â€œä»æ…ˆçš„ç‹¬è£è€…â€å¼€å§‹ã€‚åªæœ‰å½“ä½ çš„é¢†åŸŸç¡®å®éœ€è¦æ—¶ï¼Œæ‰å¢åŠ å¤æ‚æ€§ã€‚

## é—®ï¼šæˆ‘åº”è¯¥å‡†å¤‡å¥½è‡ªå·±å¡«è¡¥è¯„ä¼°å·¥å…·ä¸­çš„å“ªäº›ç©ºç™½ï¼Ÿ

Most eval tools handle the basics well: logging complete traces, tracking metrics, prompt playgrounds, and annotation queues. These are table stakes. Here are four areas where youâ€™ll likely need to supplement existing tools.

å¤§å¤šæ•°è¯„ä¼°å·¥å…·éƒ½èƒ½å¾ˆå¥½åœ°å¤„ç†åŸºç¡€åŠŸèƒ½ï¼šè®°å½•å®Œæ•´çš„ traceã€è¿½è¸ªæŒ‡æ ‡ã€prompt æ¸¸ä¹åœºå’Œæ ‡æ³¨é˜Ÿåˆ—ã€‚è¿™äº›éƒ½æ˜¯åŸºæœ¬è¦æ±‚ã€‚ä»¥ä¸‹æ˜¯å››ä¸ªä½ å¾ˆå¯èƒ½éœ€è¦è‡ªå·±è¡¥å……ç°æœ‰å·¥å…·çš„é¢†åŸŸã€‚

Watch for vendors addressing these gapsâ€”itâ€™s a strong signal they understand practitioner needs.

ç•™æ„é‚£äº›æ­£åœ¨è§£å†³è¿™äº›ç©ºç™½çš„æœåŠ¡å•†â€”â€”è¿™æ˜¯ä¸€ä¸ªå¼ºçƒˆçš„ä¿¡å·ï¼Œè¡¨æ˜ä»–ä»¬ç†è§£ä»ä¸šè€…çš„éœ€æ±‚ã€‚

#### 1\. Error Analysis and Pattern Discovery

#### 1\. é”™è¯¯åˆ†æä¸æ¨¡å¼å‘ç°

After reviewing traces where your AI fails, can your tooling automatically cluster similar issues? For instance, if multiple traces show the assistant using casual language for luxury clients, you need something that recognizes this broader â€œpersona-tone mismatchâ€ pattern. We recommend building capabilities that use AI to suggest groupings, rewrite your observations into clearer failure taxonomies, help find similar cases through semantic search, etc.

åœ¨å®¡æŸ¥ AI å¤±è´¥çš„ trace åï¼Œä½ çš„å·¥å…·èƒ½å¦è‡ªåŠ¨èšç±»ç›¸ä¼¼çš„é—®é¢˜ï¼Ÿä¾‹å¦‚ï¼Œå¦‚æœå¤šä¸ª trace æ˜¾ç¤ºåŠ©æ‰‹å¯¹å¥¢ä¾ˆå“å®¢æˆ·ä½¿ç”¨äº†éšæ„çš„è¯­è¨€ï¼Œä½ éœ€è¦ä¸€ä¸ªèƒ½è¯†åˆ«å‡ºè¿™ç§æ›´å¹¿æ³›çš„â€œäººè®¾-è¯­è°ƒä¸åŒ¹é…â€æ¨¡å¼çš„å·¥å…·ã€‚æˆ‘ä»¬å»ºè®®æ„å»ºä¸€äº›èƒ½åŠ›ï¼Œåˆ©ç”¨ AI å»ºè®®åˆ†ç»„ã€å°†ä½ çš„è§‚å¯Ÿé‡å†™ä¸ºæ›´æ¸…æ™°çš„å¤±è´¥åˆ†ç±»æ³•ã€é€šè¿‡è¯­ä¹‰æœç´¢å¸®åŠ©æ‰¾åˆ°ç›¸ä¼¼æ¡ˆä¾‹ç­‰ã€‚

#### 2\. AI-Powered Assistance Throughout the Workflow

#### 2\. è´¯ç©¿å·¥ä½œæµçš„ AI è¾…åŠ©

The most effective workflows use AI to accelerate every stage of evaluation. During error analysis, you want an LLM helping categorize your open-ended observations into coherent failure modes. For example, you might annotate several traces with notes like â€œwrong tone for investor,â€ â€œtoo casual for luxury buyer,â€ etc. Your tooling should recognize these as the same underlying pattern and suggest a unified â€œpersona-tone mismatchâ€ category.

æœ€é«˜æ•ˆçš„å·¥ä½œæµä¼šåˆ©ç”¨ AI æ¥åŠ é€Ÿè¯„ä¼°çš„æ¯ä¸€ä¸ªé˜¶æ®µã€‚åœ¨é”™è¯¯åˆ†ææœŸé—´ï¼Œä½ éœ€è¦ä¸€ä¸ª LLM å¸®åŠ©ä½ å°†å¼€æ”¾å¼çš„è§‚å¯Ÿå½’ç±»ä¸ºè¿è´¯çš„å¤±è´¥æ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½ç”¨â€œå¯¹æŠ•èµ„è€…çš„è¯­è°ƒä¸å¯¹â€ã€â€œå¯¹å¥¢ä¾ˆå“ä¹°å®¶å¤ªéšæ„â€ç­‰ç¬”è®°æ ‡æ³¨äº†å‡ ä¸ª traceã€‚ä½ çš„å·¥å…·åº”è¯¥èƒ½è¯†åˆ«å‡ºè¿™äº›æ˜¯åŒä¸€ä¸ªæ½œåœ¨æ¨¡å¼ï¼Œå¹¶å»ºè®®ä¸€ä¸ªç»Ÿä¸€çš„â€œäººè®¾-è¯­è°ƒä¸åŒ¹é…â€ç±»åˆ«ã€‚

Youâ€™ll also want AI assistance in proposing fixes. After identifying 20 cases where your assistant omits pet policies from property summaries, can your workflow analyze these failures and suggest specific prompt modifications? Can it draft refinements to your SQL generation instructions when it notices patterns of missing WHERE clauses?

ä½ è¿˜éœ€è¦ AI è¾…åŠ©æ¥æå‡ºä¿®å¤å»ºè®®ã€‚åœ¨è¯†åˆ«å‡º 20 ä¸ªåŠ©æ‰‹åœ¨æˆ¿äº§æ‘˜è¦ä¸­é—æ¼å® ç‰©æ”¿ç­–çš„æ¡ˆä¾‹åï¼Œä½ çš„å·¥ä½œæµèƒ½å¦åˆ†æè¿™äº›å¤±è´¥å¹¶å»ºè®®å…·ä½“çš„ prompt ä¿®æ”¹ï¼Ÿå½“å®ƒæ³¨æ„åˆ°ç¼ºå°‘ WHERE å­å¥çš„æ¨¡å¼æ—¶ï¼Œèƒ½å¦èµ·è‰å¯¹ SQL ç”ŸæˆæŒ‡ä»¤çš„æ”¹è¿›æ–¹æ¡ˆï¼Ÿ

Additionally, good workflows help you conduct data analysis of your annotations and traces. I like using notebooks with AI in-the-loop like [Julius](https://julius.ai/),[Hex](https://hex.tech/) or [SolveIt](https://solveit.fast.ai/). These help me discover insights like â€œlocation ambiguity errors spike 3x when users mention neighborhood namesâ€ or â€œtone mismatches occur 80% more often in email generation than other modalities.â€

æ­¤å¤–ï¼Œå¥½çš„å·¥ä½œæµè¿˜èƒ½å¸®åŠ©ä½ å¯¹æ ‡æ³¨å’Œ trace è¿›è¡Œæ•°æ®åˆ†æã€‚æˆ‘å–œæ¬¢ä½¿ç”¨å¸¦æœ‰äººåœ¨å›è·¯ä¸­çš„ AI çš„ notebookï¼Œæ¯”å¦‚ [Julius](https://julius.ai/)ã€[Hex](https://hex.tech/) æˆ– [SolveIt](https://solveit.fast.ai/)ã€‚è¿™äº›å·¥å…·å¸®åŠ©æˆ‘å‘ç°åƒâ€œå½“ç”¨æˆ·æåˆ°ç¤¾åŒºåç§°æ—¶ï¼Œä½ç½®æ¨¡ç³Šé”™è¯¯æ¿€å¢ 3 å€â€æˆ–â€œè¯­è°ƒä¸åŒ¹é…åœ¨é‚®ä»¶ç”Ÿæˆä¸­æ¯”å…¶ä»–æ¨¡å¼å¤šå‡ºç° 80%â€è¿™æ ·çš„æ´è§ã€‚

#### 3\. Custom Evaluators Over Generic Metrics

#### 3\. å®šåˆ¶è¯„ä¼°å™¨ä¼˜äºé€šç”¨æŒ‡æ ‡

Be prepared to build most of your evaluators from scratch. Generic metrics like â€œhallucination scoreâ€ or â€œhelpfulness ratingâ€ rarely capture what actually matters for your applicationâ€”like proposing unavailable showing times or omitting budget constraints from emails. In our experience, successful teams spend most of their effort on application-specific metrics.

å‡†å¤‡å¥½ä»é›¶å¼€å§‹æ„å»ºä½ çš„å¤§éƒ¨åˆ†è¯„ä¼°å™¨ã€‚åƒâ€œå¹»è§‰åˆ†æ•°â€æˆ–â€œæœ‰ç”¨æ€§è¯„çº§â€è¿™æ ·çš„é€šç”¨æŒ‡æ ‡ï¼Œå¾ˆå°‘èƒ½æ•æ‰åˆ°å¯¹ä½ çš„åº”ç”¨çœŸæ­£é‡è¦çš„äº‹æƒ…â€”â€”æ¯”å¦‚æè®®äº†æ— æ³•å®‰æ’çš„çœ‹æˆ¿æ—¶é—´ï¼Œæˆ–åœ¨é‚®ä»¶ä¸­é—æ¼äº†é¢„ç®—é™åˆ¶ã€‚æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼ŒæˆåŠŸçš„å›¢é˜Ÿå¤§éƒ¨åˆ†ç²¾åŠ›éƒ½èŠ±åœ¨äº†åº”ç”¨ä¸“å±çš„æŒ‡æ ‡ä¸Šã€‚

#### 4\. APIs That Support Custom Annotation Apps

#### 4\. æ”¯æŒå®šåˆ¶æ ‡æ³¨åº”ç”¨çš„ API

Custom annotation interfaces [work best for most teams](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf). This requires observability platforms with thoughtful APIs. I often have to build my own libraries and abstractions just to make bulk data export manageable. You shouldnâ€™t have to paginate through thousands of requests or handle timeout-prone endpoints just to get your data. Look for platforms that provide true bulk export capabilities and, crucially, APIs that let you write annotations back efficiently.

å®šåˆ¶çš„æ ‡æ³¨ç•Œé¢[å¯¹å¤§å¤šæ•°å›¢é˜Ÿæ¥è¯´æ•ˆæœæœ€å¥½](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf)ã€‚è¿™éœ€è¦å¯è§‚æµ‹æ€§å¹³å°æä¾›è®¾è®¡å‘¨åˆ°çš„ APIã€‚æˆ‘å¸¸å¸¸ä¸å¾—ä¸æ„å»ºè‡ªå·±çš„åº“å’ŒæŠ½è±¡å±‚ï¼Œæ‰èƒ½è®©æ‰¹é‡æ•°æ®å¯¼å‡ºå˜å¾—å¯ç®¡ç†ã€‚ä½ ä¸åº”è¯¥ä¸ºäº†è·å–æ•°æ®è€Œéœ€è¦å¯¹æˆåƒä¸Šä¸‡çš„è¯·æ±‚è¿›è¡Œåˆ†é¡µï¼Œæˆ–è€…å¤„ç†å®¹æ˜“è¶…æ—¶çš„ç«¯ç‚¹ã€‚å¯»æ‰¾é‚£äº›æä¾›çœŸæ­£æ‰¹é‡å¯¼å‡ºåŠŸèƒ½ï¼Œä»¥åŠâ€”â€”è‡³å…³é‡è¦çš„â€”â€”èƒ½è®©ä½ é«˜æ•ˆå†™å›æ ‡æ³¨çš„ API çš„å¹³å°ã€‚

## é—®ï¼šç”Ÿæˆåˆæˆæ•°æ®çš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

A common mistake is prompting an LLM to `"give me test queries"` without structure, resulting in generic, repetitive outputs. A structured approach using dimensions produces far better synthetic data for testing LLM applications.

ä¸€ä¸ªå¸¸è§çš„é”™è¯¯æ˜¯æ— ç»“æ„åœ°æç¤º LLMâ€œç»™æˆ‘ä¸€äº›æµ‹è¯•æŸ¥è¯¢â€ï¼Œè¿™ä¼šå¯¼è‡´æ³›æ³›è€Œé‡å¤çš„è¾“å‡ºã€‚ä½¿ç”¨ç»´åº¦çš„ç»“æ„åŒ–æ–¹æ³•ï¼Œå¯ä»¥ä¸ºæµ‹è¯• LLM åº”ç”¨ç”Ÿæˆè´¨é‡å¥½å¾—å¤šçš„åˆæˆæ•°æ®ã€‚

**Start by defining dimensions**: categories that describe different aspects of user queries. Each dimension captures one type of variation in user behavior. For example:

*   For a recipe app, dimensions might include Dietary Restriction (_vegan_, _gluten-free_, _none_), Cuisine Type (_Italian_, _Asian_, _comfort food_), and Query Complexity (_simple request_, _multi-step_, _edge case_).
*   For a customer support bot, dimensions could be Issue Type (_billing_, _technical_, _general_), Customer Mood (_frustrated_, _neutral_, _happy_), and Prior Context (_new issue_, _follow-up_, _resolved_).

**ä»å®šä¹‰ç»´åº¦å¼€å§‹**ï¼šè¿™äº›ç±»åˆ«æè¿°äº†ç”¨æˆ·æŸ¥è¯¢çš„ä¸åŒæ–¹é¢ã€‚æ¯ä¸ªç»´åº¦æ•æ‰ä¸€ç§ç”¨æˆ·è¡Œä¸ºçš„å˜åŒ–ã€‚ä¾‹å¦‚ï¼š

*   å¯¹äºä¸€ä¸ªé£Ÿè°±åº”ç”¨ï¼Œç»´åº¦å¯èƒ½åŒ…æ‹¬é¥®é£Ÿé™åˆ¶ï¼ˆ_ç´ é£Ÿ_ã€_æ— éº¸è´¨_ã€_æ— _ï¼‰ã€èœç³»ç±»å‹ï¼ˆ_æ„å¤§åˆ©èœ_ã€_äºšæ´²èœ_ã€_å®¶å¸¸èœ_ï¼‰å’ŒæŸ¥è¯¢å¤æ‚åº¦ï¼ˆ_ç®€å•è¯·æ±‚_ã€_å¤šæ­¥éª¤_ã€_è¾¹ç¼˜æ¡ˆä¾‹_ï¼‰ã€‚
*   å¯¹äºä¸€ä¸ªå®¢æœæœºå™¨äººï¼Œç»´åº¦å¯ä»¥æ˜¯é—®é¢˜ç±»å‹ï¼ˆ_è´¦å•_ã€_æŠ€æœ¯_ã€_ä¸€èˆ¬_ï¼‰ã€å®¢æˆ·æƒ…ç»ªï¼ˆ_æ²®ä¸§_ã€_ä¸­æ€§_ã€_å¼€å¿ƒ_ï¼‰å’Œå…ˆå‰èƒŒæ™¯ï¼ˆ_æ–°é—®é¢˜_ã€_è·Ÿè¿›_ã€_å·²è§£å†³_ï¼‰ã€‚

**Choose dimensions that target likely failure modes.** If you suspect your recipe app struggles with scaling ingredients for large groups or your support bot mishandles angry customers, make those dimensions. Use your application firstâ€”you need hypotheses about where failures occur. Without this, youâ€™ll generate useless test data.

**é€‰æ‹©é‚£äº›é’ˆå¯¹å¯èƒ½å¤±è´¥æ¨¡å¼çš„ç»´åº¦ã€‚** å¦‚æœä½ æ€€ç–‘ä½ çš„é£Ÿè°±åº”ç”¨åœ¨ä¸ºå¤§å›¢ä½“è°ƒæ•´é£Ÿæç”¨é‡æ—¶é‡åˆ°å›°éš¾ï¼Œæˆ–è€…ä½ çš„å®¢æœæœºå™¨äººå¤„ç†ä¸å¥½æ„¤æ€’çš„å®¢æˆ·ï¼Œé‚£å°±æŠŠè¿™äº›è®¾ä¸ºç»´åº¦ã€‚å…ˆäº²è‡ªä½¿ç”¨ä½ çš„åº”ç”¨â€”â€”ä½ éœ€è¦å¯¹å¤±è´¥å¯èƒ½å‘ç”Ÿåœ¨å“ªé‡Œæœ‰å‡è®¾ã€‚æ²¡æœ‰è¿™ä¸ªï¼Œä½ ç”Ÿæˆçš„æµ‹è¯•æ•°æ®å°†æ¯«æ— ç”¨å¤„ã€‚

**Once you have dimensions, create tuples:** specific combinations selecting one value from each dimension. A tuple like (_Vegan_, _Italian_, _Multi-step_) represents a particular use case. Write 20 tuples manually to understand your problem space, then use an LLM to scale up.

**æœ‰äº†ç»´åº¦åï¼Œåˆ›å»ºå…ƒç»„ï¼š** ä»æ¯ä¸ªç»´åº¦ä¸­é€‰ä¸€ä¸ªå€¼ç»„æˆçš„ç‰¹å®šç»„åˆã€‚ä¸€ä¸ªåƒï¼ˆ_ç´ é£Ÿ_ã€_æ„å¤§åˆ©èœ_ã€_å¤šæ­¥éª¤_ï¼‰è¿™æ ·çš„å…ƒç»„ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„ç”¨ä¾‹ã€‚å…ˆæ‰‹åŠ¨å†™ 20 ä¸ªå…ƒç»„æ¥ç†è§£ä½ çš„é—®é¢˜ç©ºé—´ï¼Œç„¶åç”¨ LLM æ¥æ‰©å¤§è§„æ¨¡ã€‚

The two-step generation process is important. First, have the LLM generate structured tuples. Then, in a separate prompt, convert each tuple to a natural language query. This separation prevents repetitive phrasing. For the vegan Italian tuple above, you might get `"I need a dairy-free lasagna recipe that I can prep the day before."`

ä¸¤æ­¥ç”Ÿæˆè¿‡ç¨‹å¾ˆé‡è¦ã€‚é¦–å…ˆï¼Œè®© LLM ç”Ÿæˆç»“æ„åŒ–çš„å…ƒç»„ã€‚ç„¶åï¼Œåœ¨å¦ä¸€ä¸ª prompt ä¸­ï¼Œå°†æ¯ä¸ªå…ƒç»„è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€‚è¿™ç§åˆ†ç¦»å¯ä»¥é˜²æ­¢æªè¾é‡å¤ã€‚å¯¹äºä¸Šé¢é‚£ä¸ªç´ é£Ÿæ„å¤§åˆ©èœçš„å…ƒç»„ï¼Œä½ å¯èƒ½ä¼šå¾—åˆ°â€œæˆ‘éœ€è¦ä¸€ä¸ªä¸å«ä¹³åˆ¶å“çš„åƒå±‚é¢é£Ÿè°±ï¼Œå¯ä»¥æå‰ä¸€å¤©å‡†å¤‡ã€‚â€

**Donâ€™t generate synthetic data for problems you can fix immediately.** If your prompt never mentions handling dietary restrictions, fix the prompt rather than generating hundreds of specialized queries. Save synthetic data for complex issues requiring iterationâ€”like an LLM consistently failing at ingredient scaling math or misinterpreting ambiguous requests.

**ä¸è¦ä¸ºé‚£äº›ä½ å¯ä»¥ç«‹å³ä¿®å¤çš„é—®é¢˜ç”Ÿæˆåˆæˆæ•°æ®ã€‚** å¦‚æœä½ çš„ prompt ä»æœªæåŠå¤„ç†é¥®é£Ÿé™åˆ¶ï¼Œé‚£å°±å»ä¿®å¤ promptï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ•°ç™¾ä¸ªä¸“é—¨çš„æŸ¥è¯¢ã€‚æŠŠåˆæˆæ•°æ®ç•™ç»™é‚£äº›éœ€è¦è¿­ä»£çš„å¤æ‚é—®é¢˜â€”â€”æ¯”å¦‚ LLM æ€»æ˜¯ç®—é”™é£Ÿæç¼©æ”¾çš„æ•°å­¦é¢˜ï¼Œæˆ–è€…è¯¯è§£äº†æ¨¡æ£±ä¸¤å¯çš„è¯·æ±‚ã€‚

After iterating on your tuples and prompts, **run these synthetic queries through your actual system to capture full traces**. Sample 100 traces for error analysis. This number provides enough traces to manually review and identify failure patterns without being overwhelming. Rather than generating thousands of similar queries, ensure your 100 traces cover diverse combinations across your dimensionsâ€”this variety will reveal more failure modes than sheer volume.

åœ¨è¿­ä»£äº†ä½ çš„å…ƒç»„å’Œ prompt ä¹‹åï¼Œ**å°†è¿™äº›åˆæˆæŸ¥è¯¢åœ¨ä½ çš„å®é™…ç³»ç»Ÿä¸­è¿è¡Œï¼Œä»¥æ•è·å®Œæ•´çš„ trace**ã€‚æŠ½å– 100 ä¸ª trace ç”¨äºé”™è¯¯åˆ†æã€‚è¿™ä¸ªæ•°é‡è¶³ä»¥è®©ä½ æ‰‹åŠ¨å®¡æŸ¥å¹¶è¯†åˆ«å¤±è´¥æ¨¡å¼ï¼Œè€Œåˆä¸ä¼šä¸å ªé‡è´Ÿã€‚ä¸å…¶ç”Ÿæˆæ•°åƒä¸ªç›¸ä¼¼çš„æŸ¥è¯¢ï¼Œä¸å¦‚ç¡®ä¿ä½ çš„ 100 ä¸ª trace è¦†ç›–äº†ä½ å„ä¸ªç»´åº¦ä¸‹çš„å¤šæ ·åŒ–ç»„åˆâ€”â€”è¿™ç§å¤šæ ·æ€§æ¯”çº¯ç²¹çš„æ•°é‡æ›´èƒ½æ­ç¤ºå¤±è´¥æ¨¡å¼ã€‚

## é—®ï¼šå½“æˆ‘çš„ç³»ç»Ÿå¤„ç†å¤šæ ·åŒ–çš„ç”¨æˆ·æŸ¥è¯¢æ—¶ï¼Œæˆ‘è¯¥å¦‚ä½•è¿›è¡Œè¯„ä¼°ï¼Ÿ

> Complex applications often support vastly different query patternsâ€”from â€œWhatâ€™s the return policy?â€ to â€œCompare pricing trends across regions for products matching these criteria.â€ Each query type exercises different system capabilities, leading to confusion on how to design eval criteria.

> å¤æ‚çš„åº”ç”¨å¸¸å¸¸æ”¯æŒæˆªç„¶ä¸åŒçš„æŸ¥è¯¢æ¨¡å¼â€”â€”ä»â€œé€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿâ€åˆ°â€œæ¯”è¾ƒç¬¦åˆè¿™äº›æ ‡å‡†çš„äº§å“åœ¨ä¸åŒåŒºåŸŸçš„ä»·æ ¼è¶‹åŠ¿ã€‚â€æ¯ç§æŸ¥è¯¢ç±»å‹éƒ½ä¼šè°ƒç”¨ä¸åŒçš„ç³»ç»Ÿèƒ½åŠ›ï¼Œè¿™å¯¼è‡´åœ¨è®¾è®¡è¯„ä¼°æ ‡å‡†æ—¶æ„Ÿåˆ°å›°æƒ‘ã€‚

_**[Error Analysis](https://youtu.be/e2i6JbU2R-s?si=8p5XVxbBiioz69Xc) is all you need.**_ Your evaluation strategy should emerge from observed failure patterns (e.g.Â error analysis), not predetermined query classifications. Rather than creating a massive evaluation matrix covering every query type you can imagine, let your systemâ€™s actual behavior guide where you invest evaluation effort.

_**ä½ æ‰€éœ€è¦çš„åªæ˜¯[é”™è¯¯åˆ†æ](https://youtu.be/e2i6JbU2R-s?si=8p5XVxbBiioz69Xc)ã€‚**_ ä½ çš„è¯„ä¼°ç­–ç•¥åº”è¯¥æºäºè§‚å¯Ÿåˆ°çš„å¤±è´¥æ¨¡å¼ï¼ˆå³é”™è¯¯åˆ†æï¼‰ï¼Œè€Œä¸æ˜¯é¢„å…ˆè®¾å®šçš„æŸ¥è¯¢åˆ†ç±»ã€‚ä¸å…¶åˆ›å»ºä¸€ä¸ªè¦†ç›–ä½ èƒ½æƒ³åˆ°çš„æ¯ä¸€ç§æŸ¥è¯¢ç±»å‹çš„åºå¤§è¯„ä¼°çŸ©é˜µï¼Œä¸å¦‚è®©ä½ ç³»ç»Ÿçš„å®é™…è¡Œä¸ºæ¥æŒ‡å¯¼ä½ å°†è¯„ä¼°ç²¾åŠ›æŠ•å‘ä½•å¤„ã€‚

During error analysis, youâ€™ll likely discover that certain query categories share failure patterns. For instance, all queries requiring temporal reasoning might struggle regardless of whether theyâ€™re simple lookups or complex aggregations. Similarly, queries that need to combine information from multiple sources might fail in consistent ways. These patterns discovered through error analysis should drive your evaluation priorities. It could be that query category is a fine way to group failures, but you donâ€™t know that until youâ€™ve analyzed your data.

åœ¨é”™è¯¯åˆ†ææœŸé—´ï¼Œä½ å¯èƒ½ä¼šå‘ç°æŸäº›æŸ¥è¯¢ç±»åˆ«å…±äº«å¤±è´¥æ¨¡å¼ã€‚ä¾‹å¦‚ï¼Œæ‰€æœ‰éœ€è¦æ—¶é—´æ¨ç†çš„æŸ¥è¯¢éƒ½å¯èƒ½é‡åˆ°å›°éš¾ï¼Œæ— è®ºå®ƒä»¬æ˜¯ç®€å•çš„æŸ¥æ‰¾è¿˜æ˜¯å¤æ‚çš„èšåˆã€‚åŒæ ·ï¼Œéœ€è¦æ•´åˆæ¥è‡ªå¤šä¸ªä¿¡æ¯æºçš„æŸ¥è¯¢å¯èƒ½ä¼šä»¥ä¸€è‡´çš„æ–¹å¼å¤±è´¥ã€‚è¿™äº›é€šè¿‡é”™è¯¯åˆ†æå‘ç°çš„æ¨¡å¼åº”è¯¥é©±åŠ¨ä½ çš„è¯„ä¼°ä¼˜å…ˆçº§ã€‚æŸ¥è¯¢ç±»åˆ«å¯èƒ½æ˜¯ä¸€ç§å¾ˆå¥½çš„å¯¹å¤±è´¥è¿›è¡Œåˆ†ç»„çš„æ–¹å¼ï¼Œä½†åœ¨ä½ åˆ†ææ•°æ®ä¹‹å‰ï¼Œä½ å¹¶ä¸çŸ¥é“è¿™ä¸€ç‚¹ã€‚

To see an example of basic error analysis in action, [see this video](https://youtu.be/e2i6JbU2R-s?si=8p5XVxbBiioz69Xc).

è¦çœ‹ä¸€ä¸ªåŸºç¡€é”™è¯¯åˆ†æçš„å®ä¾‹ï¼Œè¯·[è§‚çœ‹è¿™ä¸ªè§†é¢‘](https://youtu.be/e2i6JbU2R-s?si=8p5XVxbBiioz69Xc)ã€‚

## é—®ï¼šå¦‚ä½•ä¸ºæˆ‘çš„æ–‡æ¡£å¤„ç†ä»»åŠ¡é€‰æ‹©åˆé€‚çš„ chunk sizeï¼Ÿ

Unlike RAG, where chunks are optimized for retrieval, document processing assumes the model will see every chunk. The goal is to split text so the model can reason effectively without being overwhelmed. Even if a document fits within the context window, it might be better to break it up. Long inputs can degrade performance due to attention bottlenecks, especially in the middle of the context. Two task types require different strategies:

ä¸åŒäº RAG ä¸­ chunk æ˜¯ä¸ºæ£€ç´¢è€Œä¼˜åŒ–çš„ï¼Œæ–‡æ¡£å¤„ç†å‡è®¾æ¨¡å‹ä¼šçœ‹åˆ°æ¯ä¸€ä¸ª chunkã€‚å…¶ç›®æ ‡æ˜¯åˆ‡åˆ†æ–‡æœ¬ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½æœ‰æ•ˆæ¨ç†è€Œä¸ä¼šè¢«ä¿¡æ¯æ·¹æ²¡ã€‚å³ä½¿ä¸€ä»½æ–‡æ¡£èƒ½è£…è¿›ä¸Šä¸‹æ–‡çª—å£ï¼ŒæŠŠå®ƒæ‹†å¼€å¯èƒ½æ•ˆæœæ›´å¥½ã€‚ç”±äºæ³¨æ„åŠ›ç“¶é¢ˆï¼Œå°¤å…¶æ˜¯åœ¨ä¸Šä¸‹æ–‡çš„ä¸­é—´éƒ¨åˆ†ï¼Œé•¿è¾“å…¥ä¼šé™ä½æ€§èƒ½ã€‚ä¸¤ç§ä»»åŠ¡ç±»å‹éœ€è¦ä¸åŒçš„ç­–ç•¥ï¼š

### 1\. å›ºå®šè¾“å‡ºä»»åŠ¡ â†’ å¤§å— (Large Chunks)

These are tasks where the output length doesnâ€™t grow with input: extracting a number, answering a specific question, classifying a section. For example:

*   â€œWhatâ€™s the penalty clause in this contract?â€
*   â€œWhat was the CEOâ€™s salary in 2023?â€

è¿™äº›æ˜¯è¾“å‡ºé•¿åº¦ä¸éšè¾“å…¥å¢é•¿çš„ä»»åŠ¡ï¼šæå–ä¸€ä¸ªæ•°å­—ã€å›ç­”ä¸€ä¸ªå…·ä½“é—®é¢˜ã€å¯¹ä¸€ä¸ªç« èŠ‚è¿›è¡Œåˆ†ç±»ã€‚ä¾‹å¦‚ï¼š

*   â€œè¿™ä»½åˆåŒé‡Œçš„ç½šåˆ™æ¡æ¬¾æ˜¯ä»€ä¹ˆï¼Ÿâ€
*   â€œ2023 å¹´ CEO çš„è–ªæ°´æ˜¯å¤šå°‘ï¼Ÿâ€

Use the largest chunk (with caveats) that likely contains the answer. This reduces the number of queries and avoids context fragmentation. However, avoid adding irrelevant text. Models are sensitive to distraction, especially with large inputs. The middle parts of a long input might be under-attended. Furthermore, if cost and latency are a bottleneck, you should consider preprocessing or filtering the document (via keyword search or a lightweight retriever) to isolate relevant sections before feeding a huge chunk.

ä½¿ç”¨å¯èƒ½åŒ…å«ç­”æ¡ˆçš„æœ€å¤§çš„ chunkï¼ˆä½†æœ‰é™„åŠ æ¡ä»¶ï¼‰ã€‚è¿™å¯ä»¥å‡å°‘æŸ¥è¯¢æ¬¡æ•°å¹¶é¿å…ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€‚ç„¶è€Œï¼Œè¦é¿å…åŠ å…¥ä¸ç›¸å…³çš„æ–‡æœ¬ã€‚æ¨¡å‹å¯¹å¹²æ‰°å¾ˆæ•æ„Ÿï¼Œå°¤å…¶æ˜¯åœ¨è¾“å…¥å¾ˆé•¿çš„æƒ…å†µä¸‹ã€‚é•¿è¾“å…¥çš„ä¸­é—´éƒ¨åˆ†å¯èƒ½ä¼šè¢«å¿½ç•¥ã€‚æ­¤å¤–ï¼Œå¦‚æœæˆæœ¬å’Œå»¶è¿Ÿæ˜¯ç“¶é¢ˆï¼Œä½ åº”è¯¥è€ƒè™‘åœ¨å–‚å…¥ä¸€ä¸ªå·¨å¤§çš„ chunk ä¹‹å‰ï¼Œå¯¹æ–‡æ¡£è¿›è¡Œé¢„å¤„ç†æˆ–è¿‡æ»¤ï¼ˆé€šè¿‡å…³é”®è¯æœç´¢æˆ–ä¸€ä¸ªè½»é‡çº§çš„æ£€ç´¢å™¨ï¼‰æ¥åˆ†ç¦»å‡ºç›¸å…³éƒ¨åˆ†ã€‚

### 2\. æ‰©å±•æ€§è¾“å‡ºä»»åŠ¡ â†’ å°å— (Smaller Chunks)

These include summarization, exhaustive extraction, or any task where output grows with input. For example:

*   â€œSummarize each sectionâ€
*   â€œList all customer complaintsâ€

è¿™äº›åŒ…æ‹¬æ‘˜è¦ã€è¯¦å°½æå–ï¼Œæˆ–ä»»ä½•è¾“å‡ºéšè¾“å…¥å¢é•¿çš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼š

*   â€œæ€»ç»“æ¯ä¸ªç« èŠ‚â€
*   â€œåˆ—å‡ºæ‰€æœ‰å®¢æˆ·æŠ•è¯‰â€

In these cases, smaller chunks help preserve reasoning quality and output completeness. The standard approach is to process each chunk independently, then aggregate results (e.g., map-reduce). When sizing your chunks, try to respect content boundaries like paragraphs, sections, or chapters. Chunking also helps mitigate output limits. By breaking the task into pieces, each pieceâ€™s output can stay within limits.

åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ›´å°çš„ chunk æœ‰åŠ©äºä¿æŒæ¨ç†è´¨é‡å’Œè¾“å‡ºçš„å®Œæ•´æ€§ã€‚æ ‡å‡†æ–¹æ³•æ˜¯ç‹¬ç«‹å¤„ç†æ¯ä¸ª chunkï¼Œç„¶åèšåˆç»“æœï¼ˆä¾‹å¦‚ï¼Œmap-reduceï¼‰ã€‚åœ¨ç¡®å®š chunk å¤§å°æ—¶ï¼Œå°½é‡å°Šé‡å†…å®¹è¾¹ç•Œï¼Œå¦‚æ®µè½ã€ç« èŠ‚æˆ–ç« å›ã€‚åˆ†å—ä¹Ÿæœ‰åŠ©äºç¼“è§£è¾“å‡ºé•¿åº¦é™åˆ¶ã€‚é€šè¿‡å°†ä»»åŠ¡åˆ†è§£æˆå°å—ï¼Œæ¯å—çš„è¾“å‡ºéƒ½å¯ä»¥ä¿æŒåœ¨é™åˆ¶ä¹‹å†…ã€‚

### ä¸€èˆ¬æ€§æŒ‡å¯¼

Itâ€™s important to recognize **why chunk size affects results**. A larger chunk means the model has to reason over more information in one go â€“ essentially, a heavier cognitive load. LLMs have limited capacity to **retain and correlate details across a long text**. If too much is packed in, the model might prioritize certain parts (commonly the beginning or end) and overlook or â€œforgetâ€ details in the middle. This can lead to overly coarse summaries or missed facts. In contrast, a smaller chunk bounds the problem: the model can pay full attention to that section. You are trading off **global context for local focus**.

é‡è¦çš„æ˜¯è¦è®¤è¯†åˆ°**ä¸ºä»€ä¹ˆ chunk size ä¼šå½±å“ç»“æœ**ã€‚æ›´å¤§çš„ chunk æ„å‘³ç€æ¨¡å‹éœ€è¦ä¸€æ¬¡æ€§æ¨ç†æ›´å¤šçš„ä¿¡æ¯â€”â€”å®è´¨ä¸Šæ˜¯æ›´é‡çš„è®¤çŸ¥è´Ÿè·ã€‚LLM åœ¨**é•¿æ–‡æœ¬ä¸­ä¿ç•™å’Œå…³è”ç»†èŠ‚**çš„èƒ½åŠ›æ˜¯æœ‰é™çš„ã€‚å¦‚æœå¡å…¥å¤ªå¤šä¿¡æ¯ï¼Œæ¨¡å‹å¯èƒ½ä¼šä¼˜å…ˆå¤„ç†æŸäº›éƒ¨åˆ†ï¼ˆé€šå¸¸æ˜¯å¼€å¤´æˆ–ç»“å°¾ï¼‰ï¼Œè€Œå¿½ç•¥æˆ–â€œå¿˜è®°â€ä¸­é—´çš„ç»†èŠ‚ã€‚è¿™å¯èƒ½å¯¼è‡´è¿‡äºç²—ç³™çš„æ‘˜è¦æˆ–é—æ¼äº‹å®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ›´å°çš„ chunk é™å®šäº†é—®é¢˜èŒƒå›´ï¼šæ¨¡å‹å¯ä»¥å…¨ç¥è´¯æ³¨äºé‚£ä¸€éƒ¨åˆ†ã€‚ä½ æ˜¯åœ¨ç”¨**å…¨å±€ä¸Šä¸‹æ–‡æ¢å–å±€éƒ¨ç„¦ç‚¹**ã€‚

No rule of thumb can perfectly determine the best chunk size for your use case â€“ **you should validate with experiments**. The optimal chunk size can vary by domain and model. I treat chunk size as a hyperparameter to tune.

æ²¡æœ‰ä»»ä½•ç»éªŒæ³•åˆ™èƒ½å®Œç¾åœ°ç¡®å®šä½ ç”¨ä¾‹çš„æœ€ä½³ chunk sizeâ€”â€”**ä½ åº”è¯¥é€šè¿‡å®éªŒæ¥éªŒè¯**ã€‚æœ€ä¼˜çš„ chunk size å¯èƒ½å› é¢†åŸŸå’Œæ¨¡å‹è€Œå¼‚ã€‚æˆ‘æŠŠ chunk size å½“ä½œä¸€ä¸ªéœ€è¦è°ƒæ•´çš„è¶…å‚æ•°ã€‚

## é—®ï¼šæˆ‘åº”è¯¥å¦‚ä½•è¯„ä¼°æˆ‘çš„ RAG ç³»ç»Ÿï¼Ÿ

RAG systems have two distinct components that require different evaluation approaches: retrieval and generation.

RAG ç³»ç»Ÿæœ‰ä¸¤ä¸ªæˆªç„¶ä¸åŒçš„ç»„ä»¶ï¼Œéœ€è¦ä¸åŒçš„è¯„ä¼°æ–¹æ³•ï¼šæ£€ç´¢å’Œç”Ÿæˆã€‚

The retrieval component is a search problem. Evaluate it using traditional information retrieval (IR) metrics. Common examples include Recall@k (of all relevant documents, how many did you retrieve in the top k?), Precision@k (of the k documents retrieved, how many were relevant?), or MRR (how high up was the first relevant document?). The specific metrics you choose depend on your use case. These metrics are pure search metrics that measure whether youâ€™re finding the right documents (more on this below).

æ£€ç´¢ç»„ä»¶æ˜¯ä¸€ä¸ªæœç´¢é—®é¢˜ã€‚ä½¿ç”¨ä¼ ç»Ÿçš„ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æŒ‡æ ‡æ¥è¯„ä¼°å®ƒã€‚å¸¸è§çš„ä¾‹å­åŒ…æ‹¬ Recall@kï¼ˆåœ¨æ‰€æœ‰ç›¸å…³æ–‡æ¡£ä¸­ï¼Œä½ åœ¨å‰ k ä¸ªç»“æœä¸­æ£€ç´¢åˆ°äº†å¤šå°‘ï¼Ÿï¼‰ã€Precision@kï¼ˆåœ¨ä½ æ£€ç´¢çš„ k ä¸ªæ–‡æ¡£ä¸­ï¼Œæœ‰å¤šå°‘æ˜¯ç›¸å…³çš„ï¼Ÿï¼‰ï¼Œæˆ– MRRï¼ˆç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£æ’åœ¨ç¬¬å‡ ä½ï¼Ÿï¼‰ã€‚ä½ é€‰æ‹©çš„å…·ä½“æŒ‡æ ‡å–å†³äºä½ çš„ç”¨ä¾‹ã€‚è¿™äº›æ˜¯çº¯ç²¹çš„æœç´¢æŒ‡æ ‡ï¼Œè¡¡é‡ä½ æ˜¯å¦æ‰¾åˆ°äº†æ­£ç¡®çš„æ–‡æ¡£ï¼ˆä¸‹æ–‡å°†è¯¦è¿°ï¼‰ã€‚

To evaluate retrieval, create a dataset of queries paired with their relevant documents. Generate this synthetically by taking documents from your corpus, extracting key facts, then generating questions those facts would answer. This reverse process gives you query-document pairs for measuring retrieval performance without manual annotation.

è¦è¯„ä¼°æ£€ç´¢ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªåŒ…å«æŸ¥è¯¢åŠå…¶ç›¸å…³æ–‡æ¡£çš„æ•°æ®é›†ã€‚ä½ å¯ä»¥é€šè¿‡ä»ä½ çš„è¯­æ–™åº“ä¸­æå–æ–‡æ¡£ï¼ŒæŠ½å–å‡ºå…³é”®äº‹å®ï¼Œç„¶åç”Ÿæˆè¿™äº›äº‹å®å¯ä»¥å›ç­”çš„é—®é¢˜ï¼Œæ¥åˆæˆè¿™ä¸ªæ•°æ®é›†ã€‚è¿™ä¸ªé€†å‘è¿‡ç¨‹ä¸ºä½ æä¾›äº†ç”¨äºè¡¡é‡æ£€ç´¢æ€§èƒ½çš„æŸ¥è¯¢-æ–‡æ¡£å¯¹ï¼Œè€Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚

For the generation componentâ€”how well the LLM uses retrieved context, whether it hallucinates, whether it answers the questionâ€”use the same evaluation procedures covered throughout this course: error analysis to identify failure modes, collecting human labels, building LLM-as-judge evaluators, and validating those judges against human annotations.

å¯¹äºç”Ÿæˆç»„ä»¶â€”â€”LLM å¦‚ä½•åˆ©ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€æ˜¯å¦äº§ç”Ÿå¹»è§‰ã€æ˜¯å¦å›ç­”äº†é—®é¢˜â€”â€”ä½¿ç”¨æœ¬è¯¾ç¨‹ä¸­ä»‹ç»çš„ç›¸åŒè¯„ä¼°æµç¨‹ï¼šé€šè¿‡é”™è¯¯åˆ†æè¯†åˆ«å¤±è´¥æ¨¡å¼ï¼Œæ”¶é›†äººå·¥æ ‡ç­¾ï¼Œæ„å»º LLM-as-judge è¯„ä¼°å™¨ï¼Œå¹¶æ ¹æ®äººå·¥æ ‡æ³¨æ¥éªŒè¯è¿™äº›è¯„åˆ¤æ¨¡å‹ã€‚

Jason Liuâ€™s [â€œThere Are Only 6 RAG Evalsâ€](https://jxnl.co/writing/2025/05/19/there-are-only-6-rag-evals/) provides a framework that maps well to this separation. His Tier 1 covers traditional IR metrics for retrieval. Tiers 2 and 3 evaluate relationships between Question, Context, and Answerâ€”like whether the context is relevant (C|Q), whether the answer is faithful to context (A|C), and whether the answer addresses the question (A|Q).

Jason Liu çš„[ã€Šåªæœ‰ 6 ç§ RAG è¯„ä¼°ã€‹](https://jxnl.co/writing/2025/05/19/there-are-only-6-rag-evals/)æä¾›äº†ä¸€ä¸ªä¸è¿™ç§åˆ†ç¦»å¾ˆå¥½åœ°å¯¹åº”çš„æ¡†æ¶ã€‚ä»–çš„ç¬¬ä¸€å±‚çº§ï¼ˆTier 1ï¼‰æ¶µç›–äº†ç”¨äºæ£€ç´¢çš„ä¼ ç»Ÿ IR æŒ‡æ ‡ã€‚ç¬¬äºŒå’Œç¬¬ä¸‰å±‚çº§ï¼ˆTiers 2 and 3ï¼‰è¯„ä¼°é—®é¢˜ã€ä¸Šä¸‹æ–‡å’Œç­”æ¡ˆä¹‹é—´çš„å…³ç³»â€”â€”æ¯”å¦‚ä¸Šä¸‹æ–‡æ˜¯å¦ç›¸å…³ï¼ˆC|Qï¼‰ï¼Œç­”æ¡ˆæ˜¯å¦å¿ å®äºä¸Šä¸‹æ–‡ï¼ˆA|Cï¼‰ï¼Œä»¥åŠç­”æ¡ˆæ˜¯å¦è§£å†³äº†é—®é¢˜ï¼ˆA|Qï¼‰ã€‚

In addition to Jasonâ€™s six evals, error analysis on your specific data may reveal domain-specific failure modes that warrant their own metrics. For example, a medical RAG system might consistently fail to distinguish between drug dosages for adults versus children, or a legal RAG might confuse jurisdictional boundaries. These patterns emerge only through systematic review of actual failures. Once identified, you can create targeted evaluators for these specific issues beyond the general framework.

é™¤äº† Jason çš„å…­ç§è¯„ä¼°ä¹‹å¤–ï¼Œå¯¹ä½ ç‰¹å®šæ•°æ®çš„é”™è¯¯åˆ†æå¯èƒ½ä¼šæ­ç¤ºå‡ºé¢†åŸŸç‰¹å®šçš„å¤±è´¥æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼éœ€è¦æœ‰è‡ªå·±çš„æŒ‡æ ‡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªåŒ»ç–— RAG ç³»ç»Ÿå¯èƒ½æ€»æ˜¯æ— æ³•åŒºåˆ†æˆäººå’Œå„¿ç«¥çš„è¯ç‰©å‰‚é‡ï¼Œæˆ–è€…ä¸€ä¸ªæ³•å¾‹ RAG å¯èƒ½ä¼šæ··æ·†å¸æ³•ç®¡è¾–æƒçš„ç•Œé™ã€‚è¿™äº›æ¨¡å¼åªæœ‰é€šè¿‡å¯¹å®é™…å¤±è´¥çš„ç³»ç»Ÿæ€§å®¡æŸ¥æ‰èƒ½æµ®ç°ã€‚ä¸€æ—¦è¯†åˆ«å‡ºæ¥ï¼Œä½ å°±å¯ä»¥åœ¨é€šç”¨æ¡†æ¶ä¹‹å¤–ï¼Œä¸ºè¿™äº›å…·ä½“é—®é¢˜åˆ›å»ºæœ‰é’ˆå¯¹æ€§çš„è¯„ä¼°å™¨ã€‚

Finally, when implementing Jasonâ€™s Tier 2 and 3 metrics, donâ€™t just use prompts off the shelf. The standard LLM-as-judge process requires several steps: error analysis, prompt iteration, creating labeled examples, and measuring your judgeâ€™s accuracy against human labels. Once you know your judgeâ€™s True Positive and True Negative rates, you can correct its estimates to determine the actual failure rate in your system. Skip this validation and your judges may not reflect your actual quality criteria.

æœ€åï¼Œåœ¨å®æ–½ Jason çš„ç¬¬äºŒå’Œç¬¬ä¸‰å±‚çº§æŒ‡æ ‡æ—¶ï¼Œä¸è¦åªæ˜¯ç›´æ¥ä½¿ç”¨ç°æˆçš„ promptã€‚æ ‡å‡†çš„ LLM-as-judge æµç¨‹éœ€è¦å‡ ä¸ªæ­¥éª¤ï¼šé”™è¯¯åˆ†æã€prompt è¿­ä»£ã€åˆ›å»ºæ ‡æ³¨æ ·æœ¬ï¼Œä»¥åŠå¯¹ç…§äººå·¥æ ‡ç­¾æ¥è¡¡é‡ä½ çš„è¯„åˆ¤æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚ä¸€æ—¦ä½ çŸ¥é“äº†ä½ çš„è¯„åˆ¤æ¨¡å‹çš„çœŸæ­£ä¾‹ç‡å’ŒçœŸè´Ÿä¾‹ç‡ï¼Œä½ å°±å¯ä»¥æ ¡æ­£å®ƒçš„ä¼°è®¡å€¼ï¼Œä»¥ç¡®å®šä½ ç³»ç»Ÿä¸­çš„å®é™…å¤±è´¥ç‡ã€‚å¦‚æœè·³è¿‡è¿™ä¸ªéªŒè¯æ­¥éª¤ï¼Œä½ çš„è¯„åˆ¤æ¨¡å‹å¯èƒ½æ— æ³•åæ˜ ä½ çœŸæ­£çš„è´¨é‡æ ‡å‡†ã€‚

In summary, debug retrieval first using IR metrics, then tackle generation quality using properly validated LLM judges.

æ€»ç»“ä¸€ä¸‹ï¼Œé¦–å…ˆä½¿ç”¨ IR æŒ‡æ ‡æ¥è°ƒè¯•æ£€ç´¢éƒ¨åˆ†ï¼Œç„¶åä½¿ç”¨ç»è¿‡é€‚å½“éªŒè¯çš„ LLM è¯„åˆ¤æ¨¡å‹æ¥è§£å†³ç”Ÿæˆè´¨é‡é—®é¢˜ã€‚

## Q: What makes a good custom interface for reviewing LLM outputs?

## é—®ï¼šä¸€ä¸ªå¥½çš„ç”¨äºå®¡æŸ¥ LLM è¾“å‡ºçš„è‡ªå®šä¹‰ç•Œé¢æ˜¯æ€æ ·çš„ï¼Ÿ

Great interfaces make human review fast, clear, and motivating. We recommend [building your own annotation tool](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf) customized to your domain. The following features are possible enhancements weâ€™ve seen work well, but you donâ€™t need all of them. The screenshots shown are illustrative examples to clarify concepts. In practice, I rarely implement all these features in a single app. Itâ€™s ultimately a judgment call based on your specific needs and constraints.

å¥½çš„ç•Œé¢èƒ½ä½¿äººå·¥å®¡æŸ¥å˜å¾—å¿«é€Ÿã€æ¸…æ™°ä¸”å¯Œæœ‰æ¿€åŠ±æ€§ã€‚æˆ‘ä»¬å»ºè®®[æ„å»ºä½ è‡ªå·±çš„ã€é’ˆå¯¹ä½ æ‰€åœ¨é¢†åŸŸå®šåˆ¶çš„æ ‡æ³¨å·¥å…·](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf)ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æˆ‘ä»¬è§è¿‡æ•ˆæœä¸é”™çš„å¯èƒ½å¢å¼ºåŠŸèƒ½ï¼Œä½†ä½ å¹¶ä¸éœ€è¦å…¨éƒ¨å®ç°å®ƒä»¬ã€‚æ‰€å±•ç¤ºçš„æˆªå›¾æ˜¯ä¸ºé˜æ˜æ¦‚å¿µçš„ç¤ºä¾‹ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘å¾ˆå°‘åœ¨ä¸€ä¸ªåº”ç”¨ä¸­å®ç°æ‰€æœ‰è¿™äº›åŠŸèƒ½ã€‚è¿™æœ€ç»ˆæ˜¯ä¸€ä¸ªåŸºäºä½ å…·ä½“éœ€æ±‚å’Œé™åˆ¶çš„åˆ¤æ–­ã€‚

**1\. Render Traces Intelligently, Not Generically**: Present the trace in a way thatâ€™s intuitive for the domain. If youâ€™re evaluating generated emails, render them to look like emails. If the output is code, use syntax highlighting. Allow the reviewer to see the full trace (user input, tool calls, and LLM reasoning), but keep less important details in collapsed sections that can be expanded. Here is an example of a custom annotation tool for reviewing real estate assistant emails:

**1\. æ™ºèƒ½åœ°æ¸²æŸ“ Traceï¼Œè€Œéé€šç”¨åœ°æ¸²æŸ“**ï¼šä»¥å¯¹é¢†åŸŸè€Œè¨€ç›´è§‚çš„æ–¹å¼å‘ˆç° traceã€‚å¦‚æœä½ åœ¨è¯„ä¼°ç”Ÿæˆçš„é‚®ä»¶ï¼Œå°±è®©å®ƒä»¬çœ‹èµ·æ¥åƒé‚®ä»¶ã€‚å¦‚æœè¾“å‡ºæ˜¯ä»£ç ï¼Œå°±ä½¿ç”¨è¯­æ³•é«˜äº®ã€‚å…è®¸å®¡æŸ¥è€…çœ‹åˆ°å®Œæ•´çš„ traceï¼ˆç”¨æˆ·è¾“å…¥ã€å·¥å…·è°ƒç”¨å’Œ LLM æ¨ç†ï¼‰ï¼Œä½†å°†ä¸å¤ªé‡è¦çš„ç»†èŠ‚æ”¾åœ¨å¯å±•å¼€çš„æŠ˜å éƒ¨åˆ†ä¸­ã€‚è¿™æ˜¯ä¸€ä¸ªç”¨äºå®¡æŸ¥æˆ¿åœ°äº§åŠ©ç†é‚®ä»¶çš„è‡ªå®šä¹‰æ ‡æ³¨å·¥å…·ç¤ºä¾‹ï¼š

![](https://hamel.dev/blog/posts/evals-faq/images/emailinterface1.png)

A custom interface for reviewing emails for a real estate assistant.

ä¸€ä¸ªç”¨äºå®¡æŸ¥æˆ¿åœ°äº§åŠ©ç†é‚®ä»¶çš„è‡ªå®šä¹‰ç•Œé¢ã€‚

**2\. Show Progress and Support Keyboard Navigation**: Keep reviewers in a state of flow by minimizing friction and motivating completion. Include progress indicators (e.g., â€œTrace 45 of 100â€) to keep the review session bounded and encourage completion. Enable hotkeys for navigating between traces (e.g., N for next), applying labels, and saving notes quickly. Below is an illustration of these features:

**2\. æ˜¾ç¤ºè¿›åº¦å¹¶æ”¯æŒé”®ç›˜å¯¼èˆª**ï¼šé€šè¿‡å‡å°‘é˜»åŠ›å¹¶æ¿€åŠ±å®Œæˆï¼Œè®©å®¡æŸ¥è€…ä¿æŒå¿ƒæµçŠ¶æ€ã€‚åŒ…å«è¿›åº¦æŒ‡ç¤ºå™¨ï¼ˆä¾‹å¦‚ï¼Œâ€œTrace 45 of 100â€ï¼‰æ¥é™å®šå®¡æŸ¥ä¼šè¯çš„èŒƒå›´å¹¶é¼“åŠ±å®Œæˆã€‚å¯ç”¨å¿«æ·é”®æ¥åœ¨ trace ä¹‹é—´å¯¼èˆªï¼ˆä¾‹å¦‚ï¼ŒN ä»£è¡¨ä¸‹ä¸€ä¸ªï¼‰ã€åº”ç”¨æ ‡ç­¾å’Œå¿«é€Ÿä¿å­˜ç¬”è®°ã€‚ä¸‹é¢æ˜¯è¿™äº›åŠŸèƒ½çš„å›¾ç¤ºï¼š

![](https://hamel.dev/blog/posts/evals-faq/images/hotkey.png)

An annotation interface with a progress bar and hotkey guide

ä¸€ä¸ªå¸¦æœ‰è¿›åº¦æ¡å’Œå¿«æ·é”®æŒ‡å—çš„æ ‡æ³¨ç•Œé¢

**4\. Trace navigation through clustering, filtering, and search**: Allow reviewers to filter traces by metadata or search by keywords. Semantic search helps find conceptually similar problems. Clustering similar traces (like grouping by user persona) lets reviewers spot recurring issues and explore hypotheses. Below is an illustration of these features:

**4\. é€šè¿‡èšç±»ã€è¿‡æ»¤å’Œæœç´¢è¿›è¡Œ Trace å¯¼èˆª**ï¼šå…è®¸å®¡æŸ¥è€…æŒ‰å…ƒæ•°æ®è¿‡æ»¤ trace æˆ–æŒ‰å…³é”®è¯æœç´¢ã€‚è¯­ä¹‰æœç´¢æœ‰åŠ©äºæ‰¾åˆ°æ¦‚å¿µä¸Šç›¸ä¼¼çš„é—®é¢˜ã€‚å¯¹ç›¸ä¼¼çš„ trace è¿›è¡Œèšç±»ï¼ˆæ¯”å¦‚æŒ‰ç”¨æˆ·ç”»åƒåˆ†ç»„ï¼‰èƒ½è®©å®¡æŸ¥è€…å‘ç°é‡å¤å‡ºç°çš„é—®é¢˜å¹¶æ¢ç´¢å‡è®¾ã€‚ä¸‹é¢æ˜¯è¿™äº›åŠŸèƒ½çš„å›¾ç¤ºï¼š

![](https://hamel.dev/blog/posts/evals-faq/images/group1.png)

Cluster view showing groups of emails, such as property-focused or client-focused examples. Reviewers can drill into a group to see individual traces.

èšç±»è§†å›¾æ˜¾ç¤ºäº†é‚®ä»¶åˆ†ç»„ï¼Œä¾‹å¦‚ä»¥æˆ¿äº§ä¸ºä¸­å¿ƒçš„æˆ–ä»¥å®¢æˆ·ä¸ºä¸­å¿ƒçš„ä¾‹å­ã€‚å®¡æŸ¥è€…å¯ä»¥æ·±å…¥ä¸€ä¸ªåˆ†ç»„æŸ¥çœ‹å•ä¸ª traceã€‚

**5\. Prioritize labeling traces you think might be problematic**: Surface traces flagged by guardrails, CI failures, or automated evaluators for review. Provide buttons to take actions like adding to datasets, filing bugs, or re-running pipeline tests. Display relevant context (pipeline version, eval scores, reviewer info) directly in the interface to minimize context switching. Below is an illustration of these ideas:

**5\. ä¼˜å…ˆæ ‡æ³¨ä½ è®¤ä¸ºå¯èƒ½æœ‰é—®é¢˜çš„ trace**ï¼šå°†è¢« guardrailsã€CI å¤±è´¥æˆ–è‡ªåŠ¨åŒ–è¯„ä¼°å™¨æ ‡è®°çš„ trace æµ®ç°å‡ºæ¥ä»¥ä¾›å®¡æŸ¥ã€‚æä¾›æŒ‰é’®æ¥æ‰§è¡Œæ“ä½œï¼Œå¦‚æ·»åŠ åˆ°æ•°æ®é›†ã€æäº¤ bug æˆ–é‡æ–°è¿è¡Œæµæ°´çº¿æµ‹è¯•ã€‚åœ¨ç•Œé¢ä¸­ç›´æ¥æ˜¾ç¤ºç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆæµæ°´çº¿ç‰ˆæœ¬ã€è¯„ä¼°åˆ†æ•°ã€å®¡æŸ¥è€…ä¿¡æ¯ï¼‰ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚ä¸‹é¢æ˜¯è¿™äº›æƒ³æ³•çš„å›¾ç¤ºï¼š

![](https://hamel.dev/blog/posts/evals-faq/images/ci.png)

A trace view that allows you to quickly see auto-evaluator verdict, add traces to dataset or open issues. Also shows metadata like pipeline version, reviewer info, and more.

ä¸€ä¸ª trace è§†å›¾ï¼Œå¯ä»¥è®©ä½ å¿«é€ŸæŸ¥çœ‹è‡ªåŠ¨è¯„ä¼°å™¨çš„ç»“è®ºã€å°† trace æ·»åŠ åˆ°æ•°æ®é›†æˆ–å¼€å¯ issueã€‚åŒæ—¶è¿˜æ˜¾ç¤ºäº†å¦‚æµæ°´çº¿ç‰ˆæœ¬ã€å®¡æŸ¥è€…ä¿¡æ¯ç­‰å…ƒæ•°æ®ã€‚

### General Principle: Keep it minimal

### ä¸€èˆ¬åŸåˆ™ï¼šä¿æŒç®€çº¦

Keep your annotation interface minimal. Only incorporate these ideas if they provide a benefit that outweighs the additional complexity and maintenance overhead.

ä¿æŒä½ çš„æ ‡æ³¨ç•Œé¢ç®€çº¦ã€‚åªæœ‰å½“è¿™äº›æƒ³æ³•å¸¦æ¥çš„å¥½å¤„è¶…è¿‡äº†é¢å¤–çš„å¤æ‚æ€§å’Œç»´æŠ¤å¼€é”€æ—¶ï¼Œæ‰å°†å®ƒä»¬èå…¥è¿›æ¥ã€‚

## é—®ï¼šæˆ‘åº”è¯¥å°†å¤šå°‘å¼€å‘é¢„ç®—åˆ†é…ç»™è¯„ä¼°ï¼Ÿ

Itâ€™s important to recognize that evaluation is part of the development process rather than a distinct line item, similar to how debugging is part of software development.

é‡è¦çš„æ˜¯è¦è®¤è¯†åˆ°ï¼Œè¯„ä¼°æ˜¯å¼€å‘è¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é¢„ç®—é¡¹ç›®ï¼Œå°±åƒè°ƒè¯•æ˜¯è½¯ä»¶å¼€å‘çš„ä¸€éƒ¨åˆ†ä¸€æ ·ã€‚

You should always be doing [error analysis](https://www.youtube.com/watch?v=qH1dZ8JLLdU). When you discover issues through error analysis, many will be straightforward bugs youâ€™ll fix immediately. These fixes donâ€™t require separate evaluation infrastructure as theyâ€™re just part of development.

ä½ åº”è¯¥ä¸€ç›´åœ¨åš[é”™è¯¯åˆ†æ](https://www.youtube.com/watch?v=qH1dZ8JLLdU)ã€‚å½“ä½ é€šè¿‡é”™è¯¯åˆ†æå‘ç°é—®é¢˜æ—¶ï¼Œè®¸å¤šå°†æ˜¯ä½ ä¼šç«‹å³ä¿®å¤çš„ç›´æ¥çš„ bugã€‚è¿™äº›ä¿®å¤ä¸éœ€è¦ç‹¬ç«‹çš„è¯„ä¼°åŸºç¡€è®¾æ–½ï¼Œå› ä¸ºå®ƒä»¬åªæ˜¯å¼€å‘çš„ä¸€éƒ¨åˆ†ã€‚

The decision to build automated evaluators comes down to [cost-benefit analysis](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find). If you can catch an error with a simple assertion or regex check, the cost is minimal and probably worth it. But if you need to align an LLM-as-judge evaluator, consider whether the failure mode warrants that investment.

æ„å»ºè‡ªåŠ¨åŒ–è¯„ä¼°å™¨çš„å†³å®šå½’ç»“äº[æˆæœ¬æ•ˆç›Šåˆ†æ](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find)ã€‚å¦‚æœä½ èƒ½ç”¨ä¸€ä¸ªç®€å•çš„æ–­è¨€æˆ–æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥æ¥æ•è·ä¸€ä¸ªé”™è¯¯ï¼Œæˆæœ¬æ˜¯æœ€å°çš„ï¼Œå¹¶ä¸”å¯èƒ½å€¼å¾—ã€‚ä½†å¦‚æœä½ éœ€è¦æ ¡å‡†ä¸€ä¸ª LLM-as-judge è¯„ä¼°å™¨ï¼Œå°±è¦è€ƒè™‘è¿™ç§å¤±è´¥æ¨¡å¼æ˜¯å¦å€¼å¾—é‚£ä»½æŠ•èµ„ã€‚

In the projects weâ€™ve worked on, **weâ€™ve spent 60-80% of our development time on error analysis and evaluation**. Expect most of your effort to go toward understanding failures (i.e.Â looking at data) rather than building automated checks.

åœ¨æˆ‘ä»¬å‚ä¸çš„é¡¹ç›®ä¸­ï¼Œ**æˆ‘ä»¬èŠ±è´¹äº† 60-80% çš„å¼€å‘æ—¶é—´åœ¨é”™è¯¯åˆ†æå’Œè¯„ä¼°ä¸Š**ã€‚é¢„è®¡ä½ çš„å¤§éƒ¨åˆ†ç²¾åŠ›å°†ç”¨äºç†è§£å¤±è´¥ï¼ˆå³æŸ¥çœ‹æ•°æ®ï¼‰ï¼Œè€Œä¸æ˜¯æ„å»ºè‡ªåŠ¨åŒ–æ£€æŸ¥ã€‚

Be [wary of optimizing for high eval pass rates](https://ai-execs.com/2_intro.html#a-case-study-in-misleading-ai-advice). If youâ€™re passing 100% of your evals, youâ€™re likely not challenging your system enough. A 70% pass rate might indicate a more meaningful evaluation thatâ€™s actually stress-testing your application. Focus on evals that help you catch real issues, not ones that make your metrics look good.

è¦[è­¦æƒ•ä¸ºé«˜è¯„ä¼°é€šè¿‡ç‡è€Œä¼˜åŒ–](https://ai-execs.com/2_intro.html#a-case-study-in-misleading-ai-advice)ã€‚å¦‚æœä½ çš„è¯„ä¼°é€šè¿‡ç‡æ˜¯ 100%ï¼Œä½ å¾ˆå¯èƒ½æ²¡æœ‰ç»™ä½ çš„ç³»ç»Ÿè¶³å¤Ÿçš„æŒ‘æˆ˜ã€‚70% çš„é€šè¿‡ç‡å¯èƒ½æ„å‘³ç€ä¸€ä¸ªæ›´æœ‰æ„ä¹‰çš„è¯„ä¼°ï¼Œå®ƒçœŸæ­£åœ¨å¯¹ä½ çš„åº”ç”¨è¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚ä¸“æ³¨äºé‚£äº›èƒ½å¸®åŠ©ä½ æ•æ‰çœŸå®é—®é¢˜çš„è¯„ä¼°ï¼Œè€Œä¸æ˜¯é‚£äº›è®©ä½ çš„æŒ‡æ ‡çœ‹èµ·æ¥å¥½çœ‹çš„è¯„ä¼°ã€‚

## é—®ï¼šä¸ºä»€ä¹ˆâ€œé”™è¯¯åˆ†æâ€åœ¨ LLM è¯„ä¼°ä¸­å¦‚æ­¤é‡è¦ï¼Œä»¥åŠå¦‚ä½•è¿›è¡Œï¼Ÿ

Error analysis is **the most important activity in evals**. Error analysis helps you decide what evals to write in the first place. It allows you to identify failure modes unique to your application and data. The process involves:

é”™è¯¯åˆ†ææ˜¯**è¯„ä¼°ä¸­æœ€é‡è¦çš„ä¸€ç¯**ã€‚é”™è¯¯åˆ†æèƒ½å¸®åŠ©ä½ å†³å®šé¦–å…ˆåº”è¯¥ç¼–å†™å“ªäº›è¯„ä¼°ã€‚å®ƒè®©ä½ èƒ½å¤Ÿè¯†åˆ«å‡ºç‰¹å®šäºä½ çš„åº”ç”¨å’Œæ•°æ®çš„å¤±è´¥æ¨¡å¼ã€‚è¿™ä¸ªè¿‡ç¨‹åŒ…æ‹¬ï¼š

1.  **Creating a Dataset**: Gathering representative traces of user interactions with the LLM. If you do not have any data, you can [generate synthetic data](https://hamel.dev/blog/posts/evals-faq/#q-what-is-the-best-approach-for-generating-synthetic-data) to get started.
2.  **Open Coding**: Human annotator(s) (ideally a [benevolent dictator](https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs)) review and write open-ended notes about traces, noting any issues. This process is akin to â€œjournalingâ€ and is adapted from qualitative research methodologies. When beginning, it is recommended to focus on noting the [first failure](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-debug-multi-turn-conversation-traces) observed in a trace, as upstream errors can cause downstream issues, though you can also tag all independent failures if feasible. A [domain expert](https://hamel.dev/blog/posts/llm-judge/#step-1-find-the-principal-domain-expert) should be performing this step.
3.  **Axial Coding**: Categorize the open-ended notes into a â€œfailure taxonomy.â€. In other words, group similar failures into distinct categories. This is the most important step. At the end, count the number of failures in each category. You can use a LLM to help with this step.
4.  **Iterative Refinement**: Keep iterating on more traces until you reach [theoretical saturation](https://delvetool.com/blog/theoreticalsaturation), meaning new traces do not seem to reveal new failure modes or information to you. As a rule of thumb, you should aim to review at least 100 traces.
5.  **åˆ›å»ºæ•°æ®é›†**ï¼šæ”¶é›†ä¸ LLM ç”¨æˆ·äº¤äº’çš„æœ‰ä»£è¡¨æ€§çš„ traceã€‚å¦‚æœä½ æ²¡æœ‰ä»»ä½•æ•°æ®ï¼Œå¯ä»¥[ç”Ÿæˆåˆæˆæ•°æ®](https://hamel.dev/blog/posts/evals-faq/#q-what-is-the-best-approach-for-generating-synthetic-data)æ¥èµ·æ­¥ã€‚
6.  **å¼€æ”¾å¼ç¼–ç **ï¼šç”±äººç±»æ ‡æ³¨å‘˜ï¼ˆç†æƒ³æƒ…å†µä¸‹æ˜¯ä¸€ä½[â€œä»æ…ˆçš„ç‹¬è£è€…â€](https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs)ï¼‰å®¡æŸ¥ trace å¹¶å†™ä¸‹å¼€æ”¾å¼ç¬”è®°ï¼Œè®°å½•ä»»ä½•é—®é¢˜ã€‚è¿™ä¸ªè¿‡ç¨‹ç±»ä¼¼äºâ€œå†™æ—¥è®°â€ï¼Œæ”¹ç¼–è‡ªå®šæ€§ç ”ç©¶æ–¹æ³•ã€‚å¼€å§‹æ—¶ï¼Œå»ºè®®ä¸“æ³¨äºè®°å½•åœ¨ trace ä¸­è§‚å¯Ÿåˆ°çš„[ç¬¬ä¸€ä¸ªå¤±è´¥](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-debug-multi-turn-conversation-traces)ï¼Œå› ä¸ºä¸Šæ¸¸çš„é”™è¯¯å¯èƒ½å¯¼è‡´ä¸‹æ¸¸çš„é—®é¢˜ï¼Œä¸è¿‡å¦‚æœå¯è¡Œï¼Œä½ ä¹Ÿå¯ä»¥æ ‡è®°æ‰€æœ‰ç‹¬ç«‹çš„å¤±è´¥ã€‚è¿™ä¸€æ­¥åº”è¯¥ç”±[é¢†åŸŸä¸“å®¶](https://hamel.dev/blog/posts/llm-judge/#step-1-find-the-principal-domain-expert)æ¥æ‰§è¡Œã€‚
7.  **ä¸»è½´ç¼–ç **ï¼šå°†å¼€æ”¾å¼ç¬”è®°å½’ç±»åˆ°ä¸€ä¸ªâ€œå¤±è´¥åˆ†ç±»æ³•â€ä¸­ã€‚æ¢å¥è¯è¯´ï¼Œå°†ç›¸ä¼¼çš„å¤±è´¥åˆ†ç»„æˆä¸åŒçš„ç±»åˆ«ã€‚è¿™æ˜¯æœ€é‡è¦çš„ä¸€æ­¥ã€‚æœ€åï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ«ä¸­å¤±è´¥çš„æ•°é‡ã€‚ä½ å¯ä»¥ç”¨ LLM æ¥å¸®åŠ©å®Œæˆè¿™ä¸€æ­¥ã€‚
8.  **è¿­ä»£ä¼˜åŒ–**ï¼šæŒç»­è¿­ä»£æ›´å¤šçš„ traceï¼Œç›´åˆ°è¾¾åˆ°[ç†è®ºé¥±å’Œ](https://delvetool.com/blog/theoreticalsaturation)ï¼Œå³æ–°çš„ trace ä¼¼ä¹ä¸å†ä¸ºä½ æ­ç¤ºæ–°çš„å¤±è´¥æ¨¡å¼æˆ–ä¿¡æ¯ã€‚æ ¹æ®ç»éªŒï¼Œä½ åº”è¯¥è‡³å°‘å®¡æŸ¥ 100 ä¸ª traceã€‚

You should frequently revisit this process. There are advanced ways to sample data more efficiently, like clustering, sorting by user feedback, and sorting by high probability failure patterns. Over time, youâ€™ll develop a â€œnoseâ€ for where to look for failures in your data.

ä½ åº”è¯¥ç»å¸¸å›é¡¾è¿™ä¸ªè¿‡ç¨‹ã€‚æœ‰ä¸€äº›æ›´é«˜çº§çš„æ–¹æ³•å¯ä»¥æ›´æœ‰æ•ˆåœ°æŠ½æ ·æ•°æ®ï¼Œæ¯”å¦‚èšç±»ã€æŒ‰ç”¨æˆ·åé¦ˆæ’åºï¼Œä»¥åŠæŒ‰é«˜æ¦‚ç‡å¤±è´¥æ¨¡å¼æ’åºã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œä½ ä¼šåœ¨æ•°æ®ä¸­åŸ¹å…»å‡ºä¸€ç§å¯»æ‰¾å¤±è´¥çš„â€œç›´è§‰â€ã€‚

Do not skip error analysis. It ensures that the evaluation metrics you develop are supported by real application behaviors instead of counter-productive generic metrics (which most platforms nudge you to use). For examples of how error analysis can be helpful, see [this video](https://www.youtube.com/watch?v=e2i6JbU2R-s), or this [blog post](https://hamel.dev/blog/posts/field-guide/).

ä¸è¦è·³è¿‡é”™è¯¯åˆ†æã€‚å®ƒèƒ½ç¡®ä¿ä½ åˆ¶å®šçš„è¯„ä¼°æŒ‡æ ‡æ˜¯ç”±çœŸå®çš„åº”ç”¨è¡Œä¸ºæ”¯æŒçš„ï¼Œè€Œä¸æ˜¯é‚£äº›é€‚å¾—å…¶åçš„é€šç”¨æŒ‡æ ‡ï¼ˆå¤§å¤šæ•°å¹³å°éƒ½é¼“åŠ±ä½ ä½¿ç”¨è¿™äº›æŒ‡æ ‡ï¼‰ã€‚æƒ³çœ‹é”™è¯¯åˆ†æå¦‚ä½•æœ‰ç”¨çš„ä¾‹å­ï¼Œå¯ä»¥çœ‹[è¿™ä¸ªè§†é¢‘](https://www.youtube.com/watch?v=e2i6JbU2R-s)ï¼Œæˆ–è€…è¿™ç¯‡[åšå®¢æ–‡ç« ](https://hamel.dev/blog/posts/field-guide/)ã€‚

## é—®ï¼šGuardrails å’Œ Evaluators æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

Guardrails are **inline safety checks** that sit directly in the request/response path. They validate inputs or outputs _before_ anything reaches a user, so they typically are:

*   **Fast and deterministic** â€“ typically a few milliseconds of latency budget.
*   **Simple and explainable** â€“ regexes, keyword block-lists, schema or type validators, lightweight classifiers.
*   **Targeted at clear-cut, high-impact failures** â€“ PII leaks, profanity, disallowed instructions, SQL injection, malformed JSON, invalid code syntax, etc.

Guardrails æ˜¯ç›´æ¥ä½äºè¯·æ±‚/å“åº”è·¯å¾„ä¸­çš„**å†…è”å®‰å…¨æ£€æŸ¥**ã€‚å®ƒä»¬åœ¨ä»»ä½•å†…å®¹åˆ°è¾¾ç”¨æˆ·_ä¹‹å‰_éªŒè¯è¾“å…¥æˆ–è¾“å‡ºï¼Œå› æ­¤å®ƒä»¬é€šå¸¸æ˜¯ï¼š

*   **å¿«é€Ÿä¸”ç¡®å®šæ€§çš„** â€“ é€šå¸¸åªæœ‰å‡ æ¯«ç§’çš„å»¶è¿Ÿé¢„ç®—ã€‚
*   **ç®€å•ä¸”å¯è§£é‡Šçš„** â€“ æ­£åˆ™è¡¨è¾¾å¼ã€å…³é”®è¯é»‘åå•ã€æ¨¡å¼æˆ–ç±»å‹éªŒè¯å™¨ã€è½»é‡çº§åˆ†ç±»å™¨ã€‚
*   **é’ˆå¯¹æ˜ç¡®ã€é«˜å½±å“çš„å¤±è´¥** â€“ ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰æ³„éœ²ã€è„è¯ã€ä¸å…è®¸çš„æŒ‡ä»¤ã€SQL æ³¨å…¥ã€æ ¼å¼é”™è¯¯çš„ JSONã€æ— æ•ˆçš„ä»£ç è¯­æ³•ç­‰ã€‚

If a guardrail triggers, the system can redact, refuse, or regenerate the response. Because these checks are user-visible when they fire, false positives are treated as production bugs; teams version guardrail rules, log every trigger, and monitor rates to keep them conservative.

å¦‚æœä¸€ä¸ª guardrail è¢«è§¦å‘ï¼Œç³»ç»Ÿå¯ä»¥ç¼–è¾‘ã€æ‹’ç»æˆ–é‡æ–°ç”Ÿæˆå“åº”ã€‚å› ä¸ºè¿™äº›æ£€æŸ¥ä¸€æ—¦è§¦å‘å¯¹ç”¨æˆ·æ˜¯å¯è§çš„ï¼Œæ‰€ä»¥è¯¯æŠ¥ä¼šè¢«å½“ä½œç”Ÿäº§ç¯å¢ƒçš„ bug æ¥å¤„ç†ï¼›å›¢é˜Ÿä¼šå¯¹ guardrail è§„åˆ™è¿›è¡Œç‰ˆæœ¬æ§åˆ¶ï¼Œè®°å½•æ¯æ¬¡è§¦å‘ï¼Œå¹¶ç›‘æ§è§¦å‘ç‡ä»¥ä¿æŒå…¶ä¿å®ˆæ€§ã€‚

On the other hand, evaluators typically run **after** a response is produced. Evaluators measure qualities that simple rules cannot, such as factual correctness, completeness, etc. Their verdicts feed dashboards, regression tests, and model-improvement loops, but they do not block the original answer.

å¦ä¸€æ–¹é¢ï¼Œevaluators é€šå¸¸åœ¨å“åº”ç”Ÿæˆ**ä¹‹å**è¿è¡Œã€‚Evaluators è¡¡é‡ç®€å•è§„åˆ™æ— æ³•è¡¡é‡çš„è´¨é‡ï¼Œå¦‚äº‹å®æ­£ç¡®æ€§ã€å®Œæ•´æ€§ç­‰ã€‚å®ƒä»¬çš„ç»“è®ºä¼šè¾“å…¥åˆ°ä»ªè¡¨ç›˜ã€å›å½’æµ‹è¯•å’Œæ¨¡å‹æ”¹è¿›å¾ªç¯ä¸­ï¼Œä½†å®ƒä»¬ä¸ä¼šé˜»æ­¢åŸå§‹ç­”æ¡ˆçš„å‘å‡ºã€‚

Evaluators are usually run asynchronously or in batch to afford heavier computation such as a [LLM-as-a-Judge](https://hamel.dev/blog/posts/llm-judge/). Inline use of an LLM-as-Judge is possible _only_ when the latency budget and reliability targets allow it. Slow LLM judges might be feasible in a cascade that runs on the minority of borderline cases.

Evaluators é€šå¸¸æ˜¯å¼‚æ­¥æˆ–æ‰¹é‡è¿è¡Œçš„ï¼Œä»¥ä¾¿è¿›è¡Œæ›´é‡çš„è®¡ç®—ï¼Œæ¯”å¦‚ [LLM-as-a-Judge](https://hamel.dev/blog/posts/llm-judge/)ã€‚åªæœ‰åœ¨å»¶è¿Ÿé¢„ç®—å’Œå¯é æ€§ç›®æ ‡å…è®¸çš„æƒ…å†µä¸‹ï¼Œæ‰å¯èƒ½å†…è”ä½¿ç”¨ LLM-as-a-Judgeã€‚å¯¹äºå°‘æ•°å¤„äºè¾¹ç•Œæƒ…å†µçš„æ¡ˆä¾‹ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªçº§è”ç³»ç»Ÿä¸­è¿è¡Œè¾ƒæ…¢çš„ LLM è¯„åˆ¤æ¨¡å‹ï¼Œè¿™æˆ–è®¸æ˜¯å¯è¡Œçš„ã€‚

Apply guardrails for immediate protection against objective failures requiring intervention. Use evaluators for monitoring and improving subjective or nuanced criteria. Together, they create layered protection.

ä½¿ç”¨ guardrails æ¥å³æ—¶é˜²èŒƒéœ€è¦å¹²é¢„çš„å®¢è§‚å¤±è´¥ã€‚ä½¿ç”¨ evaluators æ¥ç›‘æ§å’Œæ”¹è¿›ä¸»è§‚æˆ–ç»†å¾®çš„æ ‡å‡†ã€‚ä¸¤è€…ç»“åˆï¼Œæ„æˆäº†åˆ†å±‚ä¿æŠ¤ã€‚

Word of caution: Do not use llm guardrails off the shelf blindly. Always [look at the prompt](https://hamel.dev/blog/posts/prompt/).

æé†’ä¸€å¥ï¼šä¸è¦ç›²ç›®åœ°ä½¿ç”¨ç°æˆçš„ LLM guardrailsã€‚ä¸€å®šè¦[çœ‹çœ‹å®ƒçš„ prompt](https://hamel.dev/blog/posts/prompt/)ã€‚

## é—®ï¼šæœ€å°å¯è¡Œçš„è¯„ä¼°é…ç½®æ˜¯æ€æ ·çš„ï¼Ÿ

Start with [error analysis](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed), not infrastructure. Spend 30 minutes manually reviewing 20-50 LLM outputs whenever you make significant changes. Use one [domain expert](https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs) who understands your users as your quality decision maker (a â€œ[benevolent dictator](https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs)â€).

ä»[é”™è¯¯åˆ†æ](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed)å¼€å§‹ï¼Œè€Œä¸æ˜¯åŸºç¡€è®¾æ–½ã€‚æ¯å½“ä½ åšå‡ºé‡å¤§æ”¹åŠ¨æ—¶ï¼ŒèŠ± 30 åˆ†é’Ÿæ‰‹åŠ¨å®¡æŸ¥ 20-50 ä¸ª LLM è¾“å‡ºã€‚è®©ä¸€ä½äº†è§£ä½ ç”¨æˆ·çš„[é¢†åŸŸä¸“å®¶](https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs)ä½œä¸ºä½ çš„è´¨é‡å†³ç­–è€…ï¼ˆä¸€ä½â€œ[ä»æ…ˆçš„ç‹¬è£è€…](https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs)â€ï¼‰ã€‚

If possible, **use notebooks** to help you review traces and analyze data. In our opinion, this is the single most effective tool for evals because you can write arbitrary code, visualize data, and iterate quickly. You can even build your own [custom annotation interface](https://hamel.dev/blog/posts/evals-faq/#q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs) right inside notebooks, as shown in this [video](https://youtu.be/aqKUwPKBkB0?si=5KDmMQnRzO_Ce9xH).

å¦‚æœå¯èƒ½çš„è¯ï¼Œ**ä½¿ç”¨ notebook** æ¥å¸®åŠ©ä½ å®¡æŸ¥ trace å’Œåˆ†ææ•°æ®ã€‚åœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œè¿™æ˜¯è¿›è¡Œè¯„ä¼°æœ€æœ‰æ•ˆçš„å•ä¸€å·¥å…·ï¼Œå› ä¸ºä½ å¯ä»¥ç¼–å†™ä»»æ„ä»£ç ã€å¯è§†åŒ–æ•°æ®å¹¶å¿«é€Ÿè¿­ä»£ã€‚ä½ ç”šè‡³å¯ä»¥åœ¨ notebook å†…éƒ¨æ„å»ºè‡ªå·±çš„[è‡ªå®šä¹‰æ ‡æ³¨ç•Œé¢](https://hamel.dev/blog/posts/evals-faq/#q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs)ï¼Œå¦‚è¿™ä¸ª[è§†é¢‘](https://youtu.be/aqKUwPKBkB0?si=5KDmMQnRzO_Ce9xH)æ‰€ç¤ºã€‚

## é—®ï¼šæˆ‘è¯¥å¦‚ä½•è¯„ä¼° Agentic å·¥ä½œæµï¼Ÿ

We recommend evaluating agentic workflows in two phases:

æˆ‘ä»¬å»ºè®®åˆ†ä¸¤ä¸ªé˜¶æ®µæ¥è¯„ä¼° agentic å·¥ä½œæµï¼š

**1\. End-to-end task success.** Treat the agent as a black box and ask â€œdid we meet the userâ€™s goal?â€. Define a precise success rule per task (exact answer, correct side-effect, etc.) and measure with human or [aligned LLM judges](https://hamel.dev/blog/posts/llm-judge/). Take note of the first upstream failure when conducting [error analysis](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed).

**1\. ç«¯åˆ°ç«¯çš„ä»»åŠ¡æˆåŠŸã€‚** å°† agent è§†ä¸ºä¸€ä¸ªé»‘ç›’ï¼Œç„¶åé—®â€œæˆ‘ä»¬æ˜¯å¦è¾¾æˆäº†ç”¨æˆ·çš„ç›®æ ‡ï¼Ÿâ€ã€‚ä¸ºæ¯ä¸ªä»»åŠ¡å®šä¹‰ä¸€ä¸ªç²¾ç¡®çš„æˆåŠŸè§„åˆ™ï¼ˆå‡†ç¡®çš„ç­”æ¡ˆã€æ­£ç¡®çš„å‰¯ä½œç”¨ç­‰ï¼‰ï¼Œå¹¶ç”¨äººå·¥æˆ–[æ ¡å‡†è¿‡çš„ LLM è¯„åˆ¤æ¨¡å‹](https://hamel.dev/blog/posts/llm-judge/)æ¥è¡¡é‡ã€‚åœ¨è¿›è¡Œ[é”™è¯¯åˆ†æ](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed)æ—¶ï¼Œè®°ä¸‹ç¬¬ä¸€ä¸ªä¸Šæ¸¸çš„å¤±è´¥ã€‚

Once error analysis reveals which workflows fail most often, move to step-level diagnostics to understand why theyâ€™re failing.

ä¸€æ—¦é”™è¯¯åˆ†ææ­ç¤ºäº†å“ªäº›å·¥ä½œæµæœ€å¸¸å¤±è´¥ï¼Œå°±è½¬å‘æ­¥éª¤çº§åˆ«çš„è¯Šæ–­ï¼Œä»¥ç†è§£å®ƒä»¬å¤±è´¥çš„åŸå› ã€‚

**2\. Step-level diagnostics.** Assuming that you have sufficiently [instrumented your system](https://hamel.dev/blog/posts/evals/#logging-traces) with details of tool calls and responses, you can score individual components such as: - _Tool choice_: was the selected tool appropriate? - _Parameter extraction_: were inputs complete and well-formed? - _Error handling_: did the agent recover from empty results or API failures? - _Context retention_: did it preserve earlier constraints? - _Efficiency_: how many steps, seconds, and tokens were spent? - _Goal checkpoints_: for long workflows verify key milestones.

**2\. æ­¥éª¤çº§åˆ«çš„è¯Šæ–­ã€‚** å‡è®¾ä½ å·²ç»ç”¨å·¥å…·è°ƒç”¨å’Œå“åº”çš„è¯¦ç»†ä¿¡æ¯å……åˆ†åœ°[æ£€æµ‹äº†ä½ çš„ç³»ç»Ÿ](https://hamel.dev/blog/posts/evals/#logging-traces)ï¼Œä½ å°±å¯ä»¥å¯¹å•ä¸ªç»„ä»¶è¿›è¡Œè¯„åˆ†ï¼Œä¾‹å¦‚ï¼š- _å·¥å…·é€‰æ‹©_ï¼šé€‰æ‹©çš„å·¥å…·æ˜¯å¦åˆé€‚ï¼Ÿ- _å‚æ•°æå–_ï¼šè¾“å…¥æ˜¯å¦å®Œæ•´ä¸”æ ¼å¼æ­£ç¡®ï¼Ÿ- _é”™è¯¯å¤„ç†_ï¼šagent æ˜¯å¦ä»ç©ºç»“æœæˆ– API å¤±è´¥ä¸­æ¢å¤ï¼Ÿ- _ä¸Šä¸‹æ–‡ä¿ç•™_ï¼šå®ƒæ˜¯å¦ä¿ç•™äº†æ—©å‰çš„çº¦æŸï¼Ÿ- _æ•ˆç‡_ï¼šèŠ±è´¹äº†å¤šå°‘æ­¥éª¤ã€ç§’æ•°å’Œ tokenï¼Ÿ- _ç›®æ ‡æ£€æŸ¥ç‚¹_ï¼šå¯¹äºé•¿å·¥ä½œæµï¼ŒéªŒè¯å…³é”®çš„é‡Œç¨‹ç¢‘ã€‚

Example: â€œFind Berkeley homes under $1M and schedule viewingsâ€ breaks into: parameters extracted correctly, relevant listings retrieved, availability checked, and calendar invites sent. Each checkpoint can pass or fail independently, making debugging tractable.

ä¾‹å¦‚ï¼šâ€œå¯»æ‰¾ä¼¯å…‹åˆ© 100 ä¸‡ç¾å…ƒä»¥ä¸‹çš„æˆ¿å±‹å¹¶å®‰æ’çœ‹æˆ¿â€å¯ä»¥åˆ†è§£ä¸ºï¼šå‚æ•°æå–æ­£ç¡®ã€æ£€ç´¢åˆ°ç›¸å…³æˆ¿æºã€æ£€æŸ¥äº†å¯ç”¨æ€§ï¼Œä»¥åŠå‘é€äº†æ—¥å†é‚€è¯·ã€‚æ¯ä¸ªæ£€æŸ¥ç‚¹éƒ½å¯ä»¥ç‹¬ç«‹é€šè¿‡æˆ–å¤±è´¥ï¼Œè¿™ä½¿å¾—è°ƒè¯•å˜å¾—æ˜“äºå¤„ç†ã€‚

**Use transition failure matrices to understand error patterns.** Create a matrix where rows represent the last successful state and columns represent where the first failure occurred. This is a great way to understand where the most failures occur.

**ä½¿ç”¨è½¬æ¢å¤±è´¥çŸ©é˜µæ¥ç†è§£é”™è¯¯æ¨¡å¼ã€‚** åˆ›å»ºä¸€ä¸ªçŸ©é˜µï¼Œå…¶ä¸­è¡Œä»£è¡¨ä¸Šä¸€ä¸ªæˆåŠŸçš„çŠ¶æ€ï¼Œåˆ—ä»£è¡¨ç¬¬ä¸€æ¬¡å¤±è´¥å‘ç”Ÿçš„åœ°æ–¹ã€‚è¿™æ˜¯ç†è§£å¤±è´¥æœ€å¸¸å‘ç”Ÿåœ¨å“ªé‡Œçš„å¥½æ–¹æ³•ã€‚

![](https://hamel.dev/blog/posts/evals-faq/images/shreya_matrix.png)

Transition failure matrix showing hotspots in text-to-SQL agent workflow

è½¬æ¢å¤±è´¥çŸ©é˜µæ˜¾ç¤ºäº† text-to-SQL agent å·¥ä½œæµä¸­çš„çƒ­ç‚¹åŒºåŸŸ

Transition matrices transform overwhelming agent complexity into actionable insights. Instead of drowning in individual trace reviews, you can immediately see that GenSQL â†’ ExecSQL transitions cause 12 failures while DecideTool â†’ PlanCal causes only 2. This data-driven approach guides where to invest debugging effort. Here is another [example](https://www.figma.com/deck/nwRlh5renu4s4olaCsf9lG/Failure-is-a-Funnel?node-id=2009-927&t=GJlTtxQ8bLJaQ92A-1) from Bryan Bischof, that is also a text-to-SQL agent:

è½¬æ¢çŸ©é˜µå°†ä»¤äººä¸çŸ¥æ‰€æªçš„ agent å¤æ‚æ€§è½¬åŒ–ä¸ºå¯è¡Œçš„æ´è§ã€‚ä½ ä¸å†éœ€è¦æ·¹æ²¡åœ¨å•ä¸ª trace çš„å®¡æŸ¥ä¸­ï¼Œè€Œæ˜¯å¯ä»¥ç«‹å³çœ‹åˆ° GenSQL â†’ ExecSQL çš„è½¬æ¢å¯¼è‡´äº† 12 æ¬¡å¤±è´¥ï¼Œè€Œ DecideTool â†’ PlanCal åªå¯¼è‡´äº† 2 æ¬¡ã€‚è¿™ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•æŒ‡å¯¼ä½ å°†è°ƒè¯•ç²¾åŠ›æŠ•å‘ä½•å¤„ã€‚è¿™æ˜¯ Bryan Bischof çš„å¦ä¸€ä¸ª[ä¾‹å­](https://www.figma.com/deck/nwRlh5renu4s4olaCsf9lG/Failure-is-a-Funnel?node-id=2009-927&t=GJlTtxQ8bLJaQ92A-1)ï¼ŒåŒæ ·æ˜¯ä¸€ä¸ª text-to-SQL agentï¼š

![](https://hamel.dev/blog/posts/evals-faq/images/bischof_matrix.png)

Bischof, Bryan â€œFailure is A Funnel - Data Council, 2025â€

Bischof, Bryan â€œå¤±è´¥æ˜¯ä¸€ä¸ªæ¼æ–— - Data Council, 2025â€

In this example, Bryan shows variation in transition matrices across experiments. How you organize your transition matrix depends on the specifics of your application. For example, Bryanâ€™s text-to-SQL agent has an inherent sequential workflow which he exploits for further analytical insight. You can watch his [full talk](https://youtu.be/R_HnI9oTv3c?si=hRRhDiydHU5k6ikc) for more details.

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒBryan å±•ç¤ºäº†ä¸åŒå®éªŒä¸­è½¬æ¢çŸ©é˜µçš„å˜åŒ–ã€‚ä½ å¦‚ä½•ç»„ç»‡ä½ çš„è½¬æ¢çŸ©é˜µå–å†³äºä½ åº”ç”¨çš„å…·ä½“æƒ…å†µã€‚ä¾‹å¦‚ï¼ŒBryan çš„ text-to-SQL agent æœ‰ä¸€ä¸ªå›ºæœ‰çš„é¡ºåºå·¥ä½œæµï¼Œä»–åˆ©ç”¨è¿™ä¸€ç‚¹æ¥è·å¾—æ›´æ·±å…¥çš„åˆ†ææ´è§ã€‚ä½ å¯ä»¥è§‚çœ‹ä»–çš„[å®Œæ•´æ¼”è®²](https://youtu.be/R_HnI9oTv3c?si=hRRhDiydHU5k6ikc)ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚

**Creating Test Cases for Agent Failures**

**ä¸º Agent å¤±è´¥åˆ›å»ºæµ‹è¯•ç”¨ä¾‹**

Creating test cases for agent failures follows the same principles as our previous FAQ on [debugging multi-turn conversation traces](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-debug-multi-turn-conversation-traces) (i.e.Â try to reproduce the error in the simplest way possible, only use multi-turn tests when the failure actually requires conversation context, etc.).

ä¸º agent å¤±è´¥åˆ›å»ºæµ‹è¯•ç”¨ä¾‹éµå¾ªä¸æˆ‘ä»¬ä¹‹å‰å…³äº[è°ƒè¯•å¤šè½®å¯¹è¯ trace](https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-debug-multi-turn-conversation-traces) çš„å¸¸è§é—®é¢˜ç›¸åŒçš„åŸåˆ™ï¼ˆå³ï¼Œå°è¯•ç”¨æœ€ç®€å•çš„æ–¹å¼å¤ç°é”™è¯¯ï¼Œä»…åœ¨å¤±è´¥ç¡®å®éœ€è¦å¯¹è¯ä¸Šä¸‹æ–‡æ—¶æ‰ä½¿ç”¨å¤šè½®æµ‹è¯•ç­‰ï¼‰ã€‚

## é—®ï¼šè¯´çœŸçš„ï¼ŒHamelï¼Œåˆ«æ‰¯æ·¡äº†ã€‚ä½ æœ€å–œæ¬¢çš„è¯„ä¼°æœåŠ¡å•†æ˜¯å“ªå®¶ï¼Ÿ

Eval tools are in an intensely competitive space. It would be futile to compare their features. If I tried to do such an analysis, it would be invalidated in a week! Vendors I encounter the most organically in my work are: [Langsmith](https://www.langchain.com/langsmith), [Arize](https://arize.com/) and [Braintrust](https://www.braintrust.dev/).

è¯„ä¼°å·¥å…·å¤„äºä¸€ä¸ªç«äº‰å¼‚å¸¸æ¿€çƒˆçš„é¢†åŸŸã€‚æ¯”è¾ƒå®ƒä»¬çš„åŠŸèƒ½æ˜¯å¾’åŠ³çš„ã€‚å¦‚æœæˆ‘å°è¯•åšè¿™æ ·çš„åˆ†æï¼Œä¸€å‘¨ä¹‹å†…å°±ä¼šè¿‡æ—¶ï¼åœ¨æˆ‘çš„å·¥ä½œä¸­ï¼Œæˆ‘æœ€å¸¸è‡ªç„¶æ¥è§¦åˆ°çš„æœåŠ¡å•†æ˜¯ï¼š[Langsmith](https://www.langchain.com/langsmith)ã€[Arize](https://arize.com/) å’Œ [Braintrust](https://www.braintrust.dev/)ã€‚

When I help clients with vendor selection, the decision weighs heavily towards who can offer the best support, as opposed to purely features. This changes depending on size of client, use case, etc. Yes - itâ€™s mainly the human factor that matters, and dare I say, vibes.

å½“æˆ‘å¸®åŠ©å®¢æˆ·é€‰æ‹©æœåŠ¡å•†æ—¶ï¼Œå†³ç­–çš„é‡å¿ƒä¸¥é‡åå‘äºè°èƒ½æä¾›æœ€å¥½çš„æ”¯æŒï¼Œè€Œä¸æ˜¯çº¯ç²¹çš„åŠŸèƒ½ã€‚è¿™å–å†³äºå®¢æˆ·çš„è§„æ¨¡ã€ç”¨ä¾‹ç­‰ã€‚æ˜¯çš„â€”â€”ä¸»è¦èµ·ä½œç”¨çš„æ˜¯äººçš„å› ç´ ï¼Œæ•æˆ‘ç›´è¨€ï¼Œè¿˜æœ‰â€œæ„Ÿè§‰â€ï¼ˆvibesï¼‰ã€‚

I have no favorite vendor. At the core, their features are very similar - and I often build [custom tools](https://hamel.dev/blog/posts/evals/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf) on top of them to fit my needs.

æˆ‘æ²¡æœ‰æœ€å–œæ¬¢çš„æœåŠ¡å•†ã€‚å®ƒä»¬çš„æ ¸å¿ƒåŠŸèƒ½éå¸¸ç›¸ä¼¼â€”â€”è€Œä¸”æˆ‘å¸¸å¸¸åœ¨å®ƒä»¬ä¹‹ä¸Šæ„å»º[è‡ªå®šä¹‰å·¥å…·](https://hamel.dev/blog/posts/evals/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf)æ¥æ»¡è¶³æˆ‘çš„éœ€æ±‚ã€‚

My suggestion is to explore the vendors and see which one you like the most.

æˆ‘çš„å»ºè®®æ˜¯ï¼Œå»æ¢ç´¢ä¸€ä¸‹è¿™äº›æœåŠ¡å•†ï¼Œçœ‹çœ‹ä½ æœ€å–œæ¬¢å“ªä¸€å®¶ã€‚

## é—®ï¼šè¯„ä¼°åœ¨ CI/CD å’Œç”Ÿäº§ç›‘æ§ä¸­çš„ä½¿ç”¨æœ‰ä½•ä¸åŒï¼Ÿ

The most important difference between CI vs.Â production evaluation is the data used for testing.

CI è¯„ä¼°å’Œç”Ÿäº§è¯„ä¼°ä¹‹é—´æœ€é‡è¦çš„åŒºåˆ«æ˜¯ç”¨äºæµ‹è¯•çš„æ•°æ®ã€‚

Test datasets for CI are small (in many cases 100+ examples) and purpose-built. Examples cover core features, regression tests for past bugs, and known edge cases. Since CI tests are run frequently, the cost of each test has to be carefully considered (thatâ€™s why you carefully curate the dataset). Favor assertions or other deterministic checks over LLM-as-judge evaluators.

ç”¨äº CI çš„æµ‹è¯•æ•°æ®é›†å¾ˆå°ï¼ˆå¾ˆå¤šæƒ…å†µä¸‹æ˜¯ 100 å¤šä¸ªä¾‹å­ï¼‰å¹¶ä¸”æ˜¯ä¸“é—¨æ„å»ºçš„ã€‚ä¾‹å­æ¶µç›–äº†æ ¸å¿ƒåŠŸèƒ½ã€é’ˆå¯¹è¿‡å» bug çš„å›å½’æµ‹è¯•ä»¥åŠå·²çŸ¥çš„è¾¹ç¼˜æ¡ˆä¾‹ã€‚ç”±äº CI æµ‹è¯•è¿è¡Œé¢‘ç¹ï¼Œå¿…é¡»ä»”ç»†è€ƒè™‘æ¯æ¬¡æµ‹è¯•çš„æˆæœ¬ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ è¦ç²¾å¿ƒç­–åˆ’æ•°æ®é›†ï¼‰ã€‚ä¼˜å…ˆä½¿ç”¨æ–­è¨€æˆ–å…¶ä»–ç¡®å®šæ€§æ£€æŸ¥ï¼Œè€Œä¸æ˜¯ LLM-as-judge è¯„ä¼°å™¨ã€‚

For evaluating production traffic, you can sample live traces and run evaluators against them asynchronously. Since you usually lack reference outputs on production data, you might rely more on on more expensive reference-free evaluators like LLM-as-judge. Additionally, track confidence intervals for production metrics. If the lower bound crosses your threshold, investigate further.

å¯¹äºè¯„ä¼°ç”Ÿäº§æµé‡ï¼Œä½ å¯ä»¥æŠ½æ ·å®æ—¶ trace å¹¶å¼‚æ­¥åœ°å¯¹å…¶è¿è¡Œè¯„ä¼°å™¨ã€‚ç”±äºä½ é€šå¸¸åœ¨ç”Ÿäº§æ•°æ®ä¸Šç¼ºå°‘å‚è€ƒè¾“å‡ºï¼Œä½ å¯èƒ½æ›´ä¾èµ–äºåƒ LLM-as-judge è¿™æ ·æ›´æ˜‚è´µçš„æ— å‚è€ƒè¯„ä¼°å™¨ã€‚æ­¤å¤–ï¼Œè¦è¿½è¸ªç”Ÿäº§æŒ‡æ ‡çš„ç½®ä¿¡åŒºé—´ã€‚å¦‚æœä¸‹é™è¶Šè¿‡äº†ä½ çš„é˜ˆå€¼ï¼Œå°±è¦è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚

These two systems are complementary: when production monitoring reveals new failure patterns through error analysis and evals, add representative examples to your CI dataset. This mitigates regressions on new issues.

è¿™ä¸¤ä¸ªç³»ç»Ÿæ˜¯äº’è¡¥çš„ï¼šå½“ç”Ÿäº§ç›‘æ§é€šè¿‡é”™è¯¯åˆ†æå’Œè¯„ä¼°æ­ç¤ºäº†æ–°çš„å¤±è´¥æ¨¡å¼æ—¶ï¼Œå°†æœ‰ä»£è¡¨æ€§çš„ä¾‹å­æ·»åŠ åˆ°ä½ çš„ CI æ•°æ®é›†ä¸­ã€‚è¿™å¯ä»¥å‡è½»åœ¨æ–°é—®é¢˜ä¸Šå‘ç”Ÿå›å½’çš„é£é™©ã€‚

## é—®ï¼šç›¸ä¼¼åº¦æŒ‡æ ‡ï¼ˆBERTScoreã€ROUGE ç­‰ï¼‰å¯¹è¯„ä¼° LLM è¾“å‡ºæœ‰ç”¨å—ï¼Ÿ

Generic metrics like BERTScore, ROUGE, cosine similarity, etc. are not useful for evaluating LLM outputs in most AI applications. Instead, we recommend using [error analysis](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed) to identify metrics specific to your applicationâ€™s behavior. We recommend designing [binary pass/fail](https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-\(passfail-evaluations-instead-of-1-5-ratings-\(likert-scales\)\).) evals (using LLM-as-judge) or code-based assertions.

åƒ BERTScoreã€ROUGEã€ä½™å¼¦ç›¸ä¼¼åº¦ç­‰é€šç”¨æŒ‡æ ‡ï¼Œåœ¨å¤§å¤šæ•° AI åº”ç”¨ä¸­å¯¹è¯„ä¼° LLM è¾“å‡ºå¹¶æ— ç”¨å¤„ã€‚ç›¸åï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨[é”™è¯¯åˆ†æ](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed)æ¥è¯†åˆ«ç‰¹å®šäºä½ åº”ç”¨è¡Œä¸ºçš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å»ºè®®è®¾è®¡[äºŒå…ƒé€šè¿‡/å¤±è´¥](https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-\(passfail-evaluations-instead-of-1-5-ratings-\(likert-scales\)\).)çš„è¯„ä¼°ï¼ˆä½¿ç”¨ LLM-as-judgeï¼‰æˆ–åŸºäºä»£ç çš„æ–­è¨€ã€‚

As an example, consider a real estate CRM assistant. Suggesting showings that arenâ€™t available (can be tested with an assertion) or confusing client personas (can be tested with a LLM-as-judge) is problematic . Generic metrics like similarity or verbosity wonâ€™t catch this. A relevant quote from the course:

ä¸¾ä¸ªä¾‹å­ï¼Œè€ƒè™‘ä¸€ä¸ªæˆ¿åœ°äº§ CRM åŠ©æ‰‹ã€‚å»ºè®®æ— æ³•å®‰æ’çš„çœ‹æˆ¿ï¼ˆå¯ä»¥ç”¨æ–­è¨€æµ‹è¯•ï¼‰æˆ–æ··æ·†å®¢æˆ·ç”»åƒï¼ˆå¯ä»¥ç”¨ LLM-as-judge æµ‹è¯•ï¼‰éƒ½æ˜¯é—®é¢˜ã€‚åƒç›¸ä¼¼åº¦æˆ–å†—é•¿åº¦è¿™æ ·çš„é€šç”¨æŒ‡æ ‡æ˜¯æ— æ³•æ•æ‰åˆ°è¿™äº›çš„ã€‚è¯¾ç¨‹ä¸­çš„ä¸€æ®µç›¸å…³å¼•è¿°ï¼š

> â€œThe abuse of generic metrics is endemic. Many eval vendors promote off the shelf metrics, which ensnare engineers into superfluous tasks.â€

> â€œæ»¥ç”¨é€šç”¨æŒ‡æ ‡çš„ç°è±¡éå¸¸æ™®éã€‚è®¸å¤šè¯„ä¼°æœåŠ¡å•†æ¨å¹¿ç°æˆçš„æŒ‡æ ‡ï¼Œè¿™è®©å·¥ç¨‹å¸ˆé™·å…¥äº†å¤šä½™çš„ä»»åŠ¡ä¸­ã€‚â€

Similarity metrics arenâ€™t always useless. They have utility in domains like search and recommendation (and therefore can be useful for [optimizing and debugging retrieval](https://hamel.dev/blog/posts/evals-faq/#q-how-should-i-approach-evaluating-my-rag-system) for RAG). For example, cosine similarity between embeddings can measure semantic closeness in retrieval systems, and average pairwise similarity can assess output diversity (where lower similarity indicates higher diversity).

ç›¸ä¼¼åº¦æŒ‡æ ‡å¹¶éæ€»æ˜¯æ— ç”¨ã€‚å®ƒä»¬åœ¨æœç´¢å’Œæ¨èç­‰é¢†åŸŸæœ‰å…¶ç”¨å¤„ï¼ˆå› æ­¤å¯¹äº RAG çš„[æ£€ç´¢ä¼˜åŒ–å’Œè°ƒè¯•](https://hamel.dev/blog/posts/evals-faq/#q-how-should-i-approach-evaluating-my-rag-system)ä¹Ÿå¯èƒ½æœ‰ç”¨ï¼‰ã€‚ä¾‹å¦‚ï¼Œembedding ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦å¯ä»¥è¡¡é‡æ£€ç´¢ç³»ç»Ÿä¸­çš„è¯­ä¹‰æ¥è¿‘åº¦ï¼Œè€Œå¹³å‡æˆå¯¹ç›¸ä¼¼åº¦å¯ä»¥è¯„ä¼°è¾“å‡ºçš„å¤šæ ·æ€§ï¼ˆå…¶ä¸­è¾ƒä½çš„ç›¸ä¼¼åº¦è¡¨ç¤ºè¾ƒé«˜çš„å¤šæ ·æ€§ï¼‰ã€‚

## é—®ï¼šæˆ‘åº”è¯¥ä½¿ç”¨â€œå¼€ç®±å³ç”¨â€çš„è¯„ä¼°æŒ‡æ ‡å—ï¼Ÿ

**No.Â Generic evaluations waste time and create false confidence.** (Unless youâ€™re using them for exploration).

**ä¸ã€‚é€šç”¨çš„è¯„ä¼°æµªè´¹æ—¶é—´å¹¶åˆ¶é€ è™šå‡çš„ä¿¡å¿ƒã€‚**ï¼ˆé™¤éä½ ç”¨å®ƒä»¬æ¥åšæ¢ç´¢ï¼‰ã€‚

One instructor noted:

> â€œAll you get from using these prefab evals is you donâ€™t know what they actually do and in the best case they waste your time and in the worst case they create an illusion of confidence that is unjustified.â€[1](https://hamel.dev/blog/posts/evals-faq/#fn1)

ä¸€ä½è®²å¸ˆæŒ‡å‡ºï¼š

> â€œä½¿ç”¨è¿™äº›é¢„åˆ¶è¯„ä¼°ï¼Œä½ å¾—åˆ°çš„åªæ˜¯ä½ ä¸çŸ¥é“å®ƒä»¬åˆ°åº•åœ¨åšä»€ä¹ˆï¼Œæœ€å¥½çš„æƒ…å†µæ˜¯å®ƒä»¬æµªè´¹ä½ çš„æ—¶é—´ï¼Œæœ€åçš„æƒ…å†µæ˜¯å®ƒä»¬åˆ¶é€ äº†ä¸€ç§æ¯«æ— æ ¹æ®çš„ä¿¡å¿ƒå¹»è§‰ã€‚â€[1](https://hamel.dev/blog/posts/evals-faq/#fn1)

Generic evaluation metrics are everywhere. Eval libraries contain scores like helpfulness, coherence, quality, etc. promising easy evaluation. These metrics measure abstract qualities that may not matter for your use case. Good scores on them donâ€™t mean your system works.

é€šç”¨çš„è¯„ä¼°æŒ‡æ ‡æ— å¤„ä¸åœ¨ã€‚è¯„ä¼°åº“é‡ŒåŒ…å«äº†åƒâ€œæœ‰ç”¨æ€§â€ã€â€œè¿è´¯æ€§â€ã€â€œè´¨é‡â€ç­‰åˆ†æ•°ï¼Œæ‰¿è¯ºèƒ½è½»æ¾è¿›è¡Œè¯„ä¼°ã€‚è¿™äº›æŒ‡æ ‡è¡¡é‡çš„æ˜¯æŠ½è±¡çš„å“è´¨ï¼Œå¯èƒ½å¯¹ä½ çš„ç”¨ä¾‹å¹¶ä¸é‡è¦ã€‚åœ¨è¿™äº›æŒ‡æ ‡ä¸Šå¾—åˆ†é«˜å¹¶ä¸æ„å‘³ç€ä½ çš„ç³»ç»Ÿèƒ½ç”¨ã€‚

Instead, conduct [error analysis](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed) to understand failures. Define [binary failure modes](https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales) based on real problems. Create [custom evaluators](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find) for those failures and validate them against human judgment. Essentially, the entire evals process.

ç›¸åï¼Œåº”è¯¥è¿›è¡Œ[é”™è¯¯åˆ†æ](https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed)æ¥ç†è§£å¤±è´¥ã€‚åŸºäºçœŸå®é—®é¢˜å®šä¹‰[äºŒå…ƒå¤±è´¥æ¨¡å¼](https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales)ã€‚ä¸ºé‚£äº›å¤±è´¥åˆ›å»º[è‡ªå®šä¹‰è¯„ä¼°å™¨](https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find)ï¼Œå¹¶å¯¹ç…§äººç±»åˆ¤æ–­è¿›è¡ŒéªŒè¯ã€‚æœ¬è´¨ä¸Šï¼Œå°±æ˜¯æ•´ä¸ªè¯„ä¼°æµç¨‹ã€‚

Experienced practitioners may still use these metrics, just not how youâ€™d expect. As Picasso said: â€œLearn the rules like a pro, so you can break them like an artist.â€ Once you understand why generic metrics fail as evaluations, you can repurpose them as exploration tools to [find interesting traces](https://hamel.dev/blog/posts/evals-faq/#q-how-can-i-efficiently-sample-production-traces-for-review) (explained in the next FAQ).

æœ‰ç»éªŒçš„ä»ä¸šè€…å¯èƒ½ä»ä¼šä½¿ç”¨è¿™äº›æŒ‡æ ‡ï¼Œä½†æ–¹å¼å’Œä½ é¢„æƒ³çš„ä¸åŒã€‚æ­£å¦‚æ¯•åŠ ç´¢æ‰€è¯´ï¼šâ€œåƒä¸“å®¶ä¸€æ ·å­¦ä¹ è§„åˆ™ï¼Œæ‰èƒ½åƒè‰ºæœ¯å®¶ä¸€æ ·æ‰“ç ´è§„åˆ™ã€‚â€ä¸€æ—¦ä½ ç†è§£äº†ä¸ºä»€ä¹ˆé€šç”¨æŒ‡æ ‡ä½œä¸ºè¯„ä¼°ä¼šå¤±è´¥ï¼Œä½ å°±å¯ä»¥å°†å®ƒä»¬é‡æ–°ç”¨ä½œæ¢ç´¢å·¥å…·ï¼Œæ¥[æ‰¾åˆ°æœ‰è¶£çš„ trace](https://hamel.dev/blog/posts/evals-faq/#q-how-can-i-efficiently-sample-production-traces-for-review)ï¼ˆåœ¨ä¸‹ä¸€ä¸ª FAQ ä¸­è§£é‡Šï¼‰ã€‚

## é—®ï¼šæˆ‘å¦‚ä½•èƒ½æœ‰æ•ˆåœ°ä»ç”Ÿäº§ trace ä¸­æŠ½æ ·è¿›è¡Œå®¡æŸ¥ï¼Ÿ

It can be cubersome to review traces randomly, especially when most traces donâ€™t have an error. These sampling strategies help you find traces more likely to reveal problems:

éšæœºå®¡æŸ¥ trace å¯èƒ½å¾ˆéº»çƒ¦ï¼Œå°¤å…¶æ˜¯å½“å¤§å¤šæ•° trace éƒ½æ²¡æœ‰é”™è¯¯æ—¶ã€‚è¿™äº›æŠ½æ ·ç­–ç•¥å¯ä»¥å¸®åŠ©ä½ æ‰¾åˆ°æ›´æœ‰å¯èƒ½æ­ç¤ºé—®é¢˜çš„ traceï¼š

*   **Outlier detection:** Sort by any metric (response length, latency, tool calls) and review extremes.
*   **User feedback signals:** Prioritize traces with negative feedback, support tickets, or escalations.
*   **Metric-based sorting:** Generic metrics can serve as exploration signals to find interesting traces. Review both high and low scores and treat them as exploration clues. Based on what you learn, you can build custom evaluators for the failure modes you find.
*   **Stratified sampling:** Group traces by key dimensions (user type, feature, query category) and sample from each group.
*   **å¼‚å¸¸å€¼æ£€æµ‹ï¼š** æŒ‰ä»»ä½•æŒ‡æ ‡ï¼ˆå“åº”é•¿åº¦ã€å»¶è¿Ÿã€å·¥å…·è°ƒç”¨ï¼‰æ’åºï¼Œå¹¶å®¡æŸ¥æç«¯å€¼ã€‚
*   **ç”¨æˆ·åé¦ˆä¿¡å·ï¼š** ä¼˜å…ˆå¤„ç†å¸¦æœ‰è´Ÿé¢åé¦ˆã€æ”¯æŒå·¥å•æˆ–å‡çº§çš„ traceã€‚
*   **åŸºäºæŒ‡æ ‡çš„æ’åºï¼š** é€šç”¨æŒ‡æ ‡å¯ä»¥ä½œä¸ºæ¢ç´¢ä¿¡å·æ¥æ‰¾åˆ°æœ‰è¶£çš„ traceã€‚å®¡æŸ¥é«˜åˆ†å’Œä½åˆ†ï¼Œå¹¶å°†å…¶è§†ä¸ºæ¢ç´¢çº¿ç´¢ã€‚æ ¹æ®ä½ å­¦åˆ°çš„ä¸œè¥¿ï¼Œä½ å¯ä»¥ä¸ºä½ å‘ç°çš„å¤±è´¥æ¨¡å¼æ„å»ºè‡ªå®šä¹‰è¯„ä¼°å™¨ã€‚
*   **åˆ†å±‚æŠ½æ ·ï¼š** æŒ‰å…³é”®ç»´åº¦ï¼ˆç”¨æˆ·ç±»å‹ã€åŠŸèƒ½ã€æŸ¥è¯¢ç±»åˆ«ï¼‰å¯¹ trace è¿›è¡Œåˆ†ç»„ï¼Œå¹¶ä»æ¯ä¸ªç»„ä¸­æŠ½æ ·ã€‚

As you get more sophisticated with how you sample, you can incorporate these tactics into the design of your [annotation tools](https://hamel.dev/blog/posts/evals-faq/#q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs).

éšç€ä½ æŠ½æ ·æ–¹æ³•çš„æ—¥è¶‹æˆç†Ÿï¼Œä½ å¯ä»¥å°†è¿™äº›ç­–ç•¥èå…¥åˆ°ä½ çš„[æ ‡æ³¨å·¥å…·](https://hamel.dev/blog/posts/evals-faq/#q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs)è®¾è®¡ä¸­ã€‚

## è„šæ³¨

1.  [Eleanor Berger](https://www.linkedin.com/in/intellectronica/)ï¼Œæˆ‘ä»¬å‡ºè‰²çš„åŠ©æ•™ã€‚[â†©ï¸](https://hamel.dev/blog/posts/evals-faq/#fnref1)

![](https://t.co/1/i/adsct?bci=4&dv=Asia%2FShanghai%26en-US%26Google%20Inc.%26MacIntel%26127%261280%26720%2612%2624%261280%26720%260%26na&eci=3&event=%7B%7D&event_id=69a38a9c-f4e0-4361-81fd-501ab8a1923d&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=4421f776-1708-4fd2-8c1e-9eb73cf66ad3&tw_document_href=https%3A%2F%2Fhamel.dev%2Fblog%2Fposts%2Fevals-faq%2F&tw_iframe_status=0&txn_id=poz16&type=javascript&version=2.3.33)![](https://analytics.twitter.com/1/i/adsct?bci=4&dv=Asia%2FShanghai%26en-US%26Google%20Inc.%26MacIntel%26127%261280%26720%2612%2624%261280%26720%260%26na&eci=3&event=%7B%7D&event_id=69a38a9c-f4e0-4361-81fd-501ab8a1923d&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=4421f776-1708-4fd2-8c1e-9eb73cf66ad3&tw_document_href=https%3A%2F%2Fhamel.dev%2Fblog%2Fposts%2Fevals-faq%2F&tw_iframe_status=0&txn_id=poz16&type=javascript&version=2.3.33)
