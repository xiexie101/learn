---
title: "译：我如何在本地运行大语言模型"
date: 2024-12-31
url: https://sorrycc.com/how-i-run-llms-locally
---

发布于 2024年12月31日

# 译：我如何在本地运行大语言模型

> 原文：[https://abishekmuthian.com/how-i-run-llms-locally/](https://abishekmuthian.com/how-i-run-llms-locally/)  
> 作者：Abishek Muthian  
> 译者：Claude 3.5 Sonnet

**编者注：作者的电脑是 4090 显卡和 96GB 内存，主要通过 Ollama、Open WebUI 等开源工具运行模型。1) 对于代码补全，他使用 Continue 和 Qwen2.5-coder 等模型；2) 对于图像生成，他使用 AUTOMATIC1111 和 Fooocus；3) 对于笔记查询，他在 Obsidian 中使用 Smart Connections 插件。**

一位 HN 用户向我询问[0](#sources)如何在本地运行 LLM，并提出了一些具体问题，我在这里为大家记录下来。

在开始之前，我想感谢成千上万或数百万未知的艺术家、程序员和作家，他们的作品被用于训练大语言模型 (LLM)，但往往没有得到应有的认可或补偿。

## 入门

r/LocalLLaMA 子版块[1](#sources)和 Ollama 博客[2](#sources)是开始本地运行 LLM 的绝佳去处。

## 硬件

我使用一台运行 Linux 的笔记本电脑，配备 Core i9 CPU (32 线程)、4090 GPU (16GB 显存)和 96GB 内存。能够装入显存的模型可以生成更多的 token/秒，更大的模型则会被卸载到内存中（显卡卸载），从而降低 token/秒的速度。我将在下面的章节中讨论这些模型。

本地运行 LLM 并不需要如此强大的计算机，较小的模型在较旧的 GPU 或 CPU 上也能运行，只是速度较慢，且会产生更多幻觉。

## 工具

有许多高质量的开源工具可以支持本地运行 LLM。以下是我经常使用的工具。

Ollama[3](#sources)是一个带有 Python、JavaScript 库的 llama.cpp[4](#sources)中间件，有助于运行 LLM。我在 docker[5](#sources)中使用 Ollama。

Open WebUI[6](#sources)是一个前端界面，提供熟悉的聊天界面用于文本和图像输入，与 Ollama 后端通信并将输出流式传输给用户。

llamafile[7](#sources)是一个包含 LLM 的单一可执行文件。这可能是开始使用本地 LLM 最简单的方式，但我在 llamafile 中遇到了显卡卸载的问题[8](#sources)。

我不是图像/视频生成模型的重度用户，但在需要时，我使用 AUTOMATIC1111[9](#sources)进行需要一些自定义的图像生成，使用 Fooocus[10](#sources)进行简单的图像生成。对于包含图像生成的复杂工作流自动化，还有 ComfyUI[11](#sources)。

对于代码补全，我在 VSCode 中使用 Continue[12](#sources)。

我在 Obsidian[14](#sources)中使用 Smart Connections[13](#sources)通过 Ollama 查询我的笔记。

![Screenshot of Obsidian with chat smart connections extension showing the last journal I write](https://abishekmuthian.com/images/obsidian-smart-connections.jpg)

我向 Smart Connections 询问我上次写日记的时间，我希望在 2025 年能每天写日记。

## 模型

我使用 Ollama 模型页面[15](#sources)下载最新的 LLM。我使用 Thunderbird 的 RSS 功能跟踪模型更新。我使用 CivitAI[16](#sources)下载特定风格的图像生成模型（例如用于世界构建的等距视图）。但请注意，CivitAI 上的大多数模型似乎都倾向于成人图像生成。

我根据性能/大小选择 LLM。由于 LLM 的快速发展，我目前的选择经常变化。

```ts
•	Llama3.2 用于 Smart Connections 和通用查询
•	Deepseek-coder-v2 用于 Continue 中的代码补全
•	Qwen2.5-coder 用于在 Continue 中讨论代码
•	Stable Diffusion 用于在 AUTOMATIC1111 或 Fooocus 中生成图像
```

## 更新

我使用 WatchTower[17](#sources)更新 docker 容器，并在 Open Web UI 中更新模型。

## 微调和量化

由于我的 Intel CPU 可能存在制造缺陷[18](#sources)，所以我还没有在我的机器上进行任何模型的微调或量化，因为我不想在训练期间让它长时间处于高温状态。

## 结论

本地运行 LLM 让我能够完全控制自己的数据，并获得更低的响应延迟。这一切都离不开开源项目、开源免费模型以及这些模型训练数据的原始所有者的贡献。

我会在使用新的工具/模型时更新这篇文章。

\[0\] [https://news.ycombinator.com/item?id=42537024](https://news.ycombinator.com/item?id=42537024)  
\[1\] [https://www.reddit.com/r/LocalLLaMA/](https://www.reddit.com/r/LocalLLaMA/)  
\[2\] [https://ollama.com/blog](https://ollama.com/blog)  
\[3\] [https://ollama.com/download](https://ollama.com/download)  
\[4\] [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)  
\[5\] [https://hub.docker.com/r/ollama/ollama](https://hub.docker.com/r/ollama/ollama)  
\[6\] [https://github.com/open-webui/open-webui](https://github.com/open-webui/open-webui)  
\[7\] [https://github.com/Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile)  
\[8\] [https://github.com/Mozilla-Ocho/llamafile/issues/611](https://github.com/Mozilla-Ocho/llamafile/issues/611)  
\[9\] [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)  
\[10\] [https://github.com/lllyasviel/Fooocus](https://github.com/lllyasviel/Fooocus)  
\[11\] [https://github.com/comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)  
\[12\] [https://docs.continue.dev/getting-started/overview](https://docs.continue.dev/getting-started/overview)  
\[13\] [https://github.com/brianpetro/obsidian-smart-connections](https://github.com/brianpetro/obsidian-smart-connections)  
\[14\] [https://obsidian.md](https://obsidian.md)  
\[15\] [https://ollama.com/search](https://ollama.com/search)  
\[16\] [https://civitai.com/models/63376/isometric-chinese-style-architecture-lora](https://civitai.com/models/63376/isometric-chinese-style-architecture-lora)  
\[17\] [https://containrrr.dev/watchtower/](https://containrrr.dev/watchtower/)  
\[18\] [https://en.wikipedia.org/wiki/Raptor\_Lake#Instability\_and\_degradation\_issue](https://en.wikipedia.org/wiki/Raptor_Lake#Instability_and_degradation_issue)

### 新闻订阅

我致力于创作频率较低但高质量的内容，涉及健康、产品开发、编程、软件工程、DIY、安全、哲学等领域。如果您想通过电子邮件收到这些内容，请考虑订阅我的[新闻通讯](http://eepurl.com/if97pb)。
