---
title: "557 - 《读书笔记：AI Engineering，Building Application with Foundation Models》"
date: 2025-04-28
url: https://sorrycc.com/book-ai-engineering-building-application-with-foundation-models
---

发布于 2025年4月28日

# 557 - 《读书笔记：AI Engineering，Building Application with Foundation Models》

关于作者：Chip Huyen 是一位专注于机器学习（ML）系统的知名作家和计算机科学家，曾在 NVIDIA、Snorkel AI 工作，并创立了一家 AI 基础设施初创公司（后被收购）。她还在斯坦福大学教授 ML 系统课程。她的专业知识体现在 2022 年亚马逊畅销书《Designing Machine Learning Systems》（O’Reilly 出版社）中，该书已被翻译成超过 10 种语言。此外，Huyen 还撰写了四本越南语畅销书，包括旅游系列《Xach ba lo len va Di》。

**1、Chapter 1：介绍构建基于基础模型的 AI 应用程序。**

比较早的模型是大语言模型（LLMs），后面发展到基础模型（Foundation Models）。基础模型源于语言模型，通过自监督学习（Self-Supervision）的方式实现规模化，并扩展到多模态（Multimodal），成为 AI 工程的基础。基于基础模型发展出来了 AI 工程，因其基础模型的通用能力、投资热潮和低进入门槛而迅速发展。

基础模型广泛应用于编码、图像视频生成、写作（MIT研究（Noy & Zhang, 2023）显示 ChatGPT 使写作时间减少 40%，质量提升 18%）、教育（Duolingo 个性化课程）、对话机器人（客服机器人、3D 智能NPC）、信息聚合和数据组织（74% 用户用 AI 做总结），以及工作流自动化（AI Agent 显著提升个人和企业效率）等方面。

**2、Chapter 2：理解基础模型。**

基础模型的构建涉及复杂的训练过程和高昂成本，普通用户无需掌握开发细节，但是，理解模型之间的差异尤其重要。由于模型的训练过程缺乏透明度，因此了解模型的差异主要依赖于数据来源、架构选择和后训练（Post-Training）测试。

模型性能高度依赖训练数据的质量和分布，数据不足或不匹配可能导致模型无法胜任特定任务。OpenAI 的 GPT-3 和 Google 的 Gemini 均使用 Common Crawl 数据；OpenAI 在训练 GPT-2 时仅使用 Reddit 上至少获3个点赞的链接以提高数据质量。

英语在互联网数据中占主导地位，导致通用模型在英语任务上的表现远优于其他语言，尤其是在低资源语言上。Common Crawl中英语占比45.88%，是第二大语言俄语（5.97%）的 8 倍。

模型架构选择和规模直接影响性能和可用性。当前 Transformer 架构主导语言模型领域，但存在局限性，替代架构正在兴起，比如 RWKV（基于RNN）、Mamba（状态空间模型，线性时间序列建模）和Jamba（混合Transformer-Mamba）显示出长序列处理潜力，Mamba-3B 在语言建模上超越同规模 Transformer 模型。

模型规模（参数数量、训练令牌数、计算成本）是性能提升的关键，但存在数据和电力瓶颈。Llama 系列模型参数从 7B 到 405B，训练令牌数从 Llama 1 的 1.4 万亿增至 Llama 3 的 15 万亿。GPT-3-175B 训练计算量为 3.14×10^23 FLOPs，成本约 400万 美元（70%利用率下）。训练数据集增长速度远超新数据生成速度，同时电力消耗预计到 2030 年占全球用电 4% - 20%，这会限制模型规模进一步扩展。

后训练通过监督微调（SFT）和偏好微调（如RLHF）对齐模型与人类偏好，解决预训练数据质量低导致的问题。InstructGPT 后训练仅占计算资源的 2%，90% 标注员拥有大学学位，生成一组（提示，响应）对成本约 10 美元。RLHF 标注成本为每对比 3.5美元，标注一致性约 73%。

采样（Sampling）是模型生成输出的关键过程，其概率性质导致创造性与不一致性并存，影响性能和用户体验。温度（Temperature）调节创造性，0.7 常用于平衡创造力和预测性；Top-p 采样累计概率通常设为 0.9-0.95 。

**3、Chapter 3：评估方法论。**

AI 的广泛应用增加了灾难性失败的风险，如聊天机器人鼓励自杀、AI 生成虚假证据、以及提供错误信息导致赔偿等案例（具体案例比如律师提交 AI 幻觉证据、加拿大航空因 AI 错误信息赔偿）。「评估」是降低风险和发现机会的关键，但基础模型的开放性和复杂性使其评估比传统机器学习模型更加困难。

精确评估（如功能正确性和相似性度量）提供无歧义的判断结果，适用于开放性响应评估。功能正确性评估系统是否完成预期功能（如代码生成中的执行准确性，AI生成代码通过单元测试验证），常见于代码生成和游戏机器人任务（基准如 HumanEval、MBPP 使用 pass@k 分数，10 个问题中解决 5 个，pass@3 为 50%）。相似性度量对比生成响应与参考数据，包括精确匹配（适用于简单任务，如“2+3=5”）、词汇相似性（基于文本重叠，如BLEU、ROUGE）和语义相似性（基于嵌入，如BERTScore）。

嵌入是将数据转化为数值表示以捕捉语义，用于语义相似性评估和多模态任务。嵌入是向量（如“the cat sits on a mat”可能为 `[0.11, 0.02, 0.54]`），常见模型包括BERT（嵌入大小768-1024）、CLIP（512）和OpenAI嵌入API（1536-3072）。嵌入质量通过任务效用评估（如MTEB基准），应用包括分类、推荐系统和RAG（检索增强生成）。

AI 作为评判者（Judge）是一种主观评估方法，利用 AI 模型评估其他 AI 响应，因其速度快、成本低和灵活性而流行，但存在一致性、标准模糊和偏见等问题。AI 评判者无需参考数据，可基于任意标准评估，与人类评估相关性高（2023年Zheng et al.研究显示GPT-4与人类一致性达85%，高于人类间一致性81%）。然而，AI 评判者不一致（同一输入可能多次评分不同）、标准不统一（不同工具如MLflow、Ragas对“忠实度”定义不同）、成本和延迟增加（使用GPT-4评估可能翻倍API调用）、以及偏见。2023 年 LangChain 报告显示 58% 平台评估由 AI 评判者完成。

比较评估（Comparative Evaluation）通过模型间直接对比排名（如LMSYS Chatbot Arena），适用于主观质量判断，但面临扩展性、标准化和绝对性能判断的挑战。比较评估优于逐点评估（Pointwise Evaluation）在于易于判断两模型优劣（如Anthropic 2021年首次使用，ChatGPT也收集用户对比反馈）。

**4、Chapter 4：评估 AI 系统 。**

评估是AI应用开发中的关键环节，缺乏可靠的评估流程是AI采用的主要障碍之一。

AI 应用的评估应基于明确的标准，并采用“评估驱动开发”（evaluation-driven development）的理念，即在开发前定义评估标准。评估标准分为领域特定能力、生成能力、指令遵循能力）以及成本与延迟四大类。1）「领域特定能力」是针对特定任务（如编程、翻译）的能力评估，常用公共或私有基准测试（benchmarks），如 BIRD-SQL（评估SQL查询效率），2）「生成能力」聚焦开放式输出质量，评估指标包括流畅性、连贯性、事实一致性和安全性，3）「指令遵循能力」评估模型是否能按指令生成预期输出，涉及结构化输出（如JSON格式）和角色扮演，4）「成本与延迟」强调平衡模型质量与运行成本。

模型选择需基于应用需求，而非单纯追求“最佳模型”。决策涉及是否自托管开源模型或使用模型API，需权衡硬性属性（如隐私、许可证）和软性属性（如准确性）。自建 vs. 购买？开源模型与商业API的比较涉及七大维度，包括数据隐私（API可能泄露数据，如三星事件）、性能（商业模型通常领先，如 MMLU 趋势图）、成本（API 按使用计费，自托管需工程投入）等。

可靠的评估流程是 AI 应用成功的基础，需涵盖系统各组件、明确评估准则和方法，并持续迭代以适应变化。强调评估系统各部分（端到端、每轮、每任务），如 PDF 简历提取雇主信息需分别评估文本提取和雇主识别步骤。任务评估（如BIG-bench的twenty\_questions）关注整体完成效果。选择评估方法（自动指标、AI法官、人工评估），并标注评估数据，注重数据切片（slicing）以发现偏见或改进点。OpenAI 建议样本量与得分差异相关（如差异3%需1000样本）。评估流程需测试可靠性、成本和指标相关性。

**5、Chapter 5：提示工程。**

提示工程作为最简单且常见的模型适应方法，相较于微调（finetuning）无需更改模型权重，仅通过语言指导即可调整模型行为。

提示工程是指通过设计指令（prompt）引导AI模型完成特定任务的过程。尽管提示工程看似简单，实则涉及复杂挑战，需将其视为人机沟通的艺术，而非单纯的文字调整。提示工程是一项实用技能，但仅依赖提示工程不足以构建生产级AI应用，还需统计学、工程学和传统机器学习知识支持（例如实验追踪、评估和数据集管理）。

提示是向模型发出的任务指令，可能包括任务描述、示例和具体任务内容（如回答问题或总结文本）。提示效果依赖于模型的指令遵循能力和对提示扰动的鲁棒性。文章引用数据表明，模型能力越强，鲁棒性越高，例如智能模型应能理解“5”和“five”的等价性。不同模型对提示结构偏好不同，例如 GPT-4 在任务描述位于提示开头时表现更佳，而 Llama 3 则在结尾时效果更好。

上下文学习（In-Context Learning）源于GPT-3论文（Brown et al., 2020），指通过提示中的示例教导模型新行为，无需更新权重。GPT-3 在少样本学习（Few-Shot Learning）中显著优于零样本学习（Zero-Shot Learning），而微软 2023 年分析显示 GPT-4 等强大模型在零样本场景下已接近少样本效果，但领域特定任务仍需示例支持。

系统提示（System Prompt）通常包含任务描述和角色设定，用户提示（User Prompt）则包含具体任务或问题。不同模型使用不同聊天模板（如Llama 2和Llama 3），模板错误可能导致性能问题。Anthropic文档指出，系统提示可通过角色设定提升模型一致性和创造性，OpenAI论文（Wallace et al., 2024）则揭示模型可能被训练优先关注系统提示。

模型上下文长度快速扩展，从 GPT-2 的 1K 增长到 Gemini-1.5 Pro 的 2M（2019-2024年），足以容纳复杂代码库或大量文档（如PyTorch代码库或2000个维基百科页面）。研究（Liu et al., 2023）表明，模型对提示开头和结尾的信息理解优于中间部分，通过 Needle in a Haystack 测试验证了这一现象。

提示工程最佳实践。

*   **清晰指令**：明确任务目标、评分系统或输出格式，避免歧义。例如，要求模型评分时需指定是否允许小数分数。
*   **角色设定与示例**：通过赋予模型特定角色（如一年级教师）或提供示例（如儿童聊天机器人回应虚构角色问题），引导模型采用合适视角。
*   **任务分解**：将复杂任务拆分为子任务（如客户支持中的意图分类和响应生成），提升性能并便于监控和调试。
*   **思考链（CoT）与自评**：通过“一步步思考”或自评提示，鼓励模型系统性解决问题，研究（Wei et al., 2022）显示 CoT 显著提升模型在数学和推理任务中的表现。
*   **迭代与版本管理**：提示需不断迭代，建议将提示与代码分离，采用提示目录（Prompt Catalog）进行版本管理和搜索。

关于防御性提示工程与提示攻击。攻击类型包括提示提取（获取系统提示）、越狱（Jailbreaking，绕过安全限制）和提示注入（Prompt Injection，注入恶意指令）。攻击可能导致远程代码执行、数据泄露、品牌危机等严重后果，比如 Google AI 搜索“吃石头”事件（2024）和微软 Tay 聊天机器人种族主义言论事件（2016）。攻击手段从手动提示破解（模糊化、角色扮演如DAN攻击）到自动化攻击（如PAIR算法，Chao et al., 2023）和间接提示注入（通过工具或外部数据源）。

怎么防御？1）模型层面：OpenAI（Wallace et al., 2024）提出指令优先级层次，系统提示优先级最高，增强安全性，2）提示层面：明确禁止敏感信息输出，或重复系统提示以强化指令，3）系统层面：隔离代码执行、人工审批关键操作、设置输入输出护栏（Guardrails）等。

**6、Chapter 6：RAG 和 Agents 。**（对于写 Agent 的同学来说，值得细看）

有两种关键的上下文构建模式——检索增强生成（RAG, Retrieval-Augmented Generation）和智能代理（Agents），它们在增强AI模型能力、解决上下文限制以及与外部世界交互方面具有重要意义。

1）RAG。

RAG 是一种通过从外部数据源检索相关信息来增强模型生成能力的模式，旨在解决模型上下文限制问题，并提高生成内容的准确性，减少“幻觉”（hallucination）。RAG 不仅是上下文扩展的手段，更是高效信息利用的策略。RAG 不仅限于文本，还包括多模态（如图像、视频）和表格数据处理。

RAG的概念最早源于2017年的论文[《Reading Wikipedia to Answer Open-Domain Questions》](https://arxiv.org/abs/1704.00051)，提出了“先检索后生成”的模式；2020年Lewis等人正式命名RAG，强调其在知识密集型任务中的应用（Lewis et al., 2020）。RAG通过检索器（Retriever）和生成器（Generator）两个组件协同工作，检索器负责从外部存储中提取相关信息，生成器基于此生成响应。Anthropic 2024 有个建议：对于少于20万token的知识库（约500页材料），可直接将整个知识库纳入提示词，无需 RAG 。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/ni6ZvE.png)

「检索算法」有两种，1）基于术语的检索（Term-based Retrieval）：如Elasticsearch和BM25，利用关键词和TF-IDF（词频-逆文档频率）计算相关性，速度快、成本低，但难以改进且存在术语歧义问题，2）基于嵌入的检索（Embedding-based Retrieval）：通过语义相似性检索，利用向量数据库和近邻搜索（如FAISS、HNSW），性能潜力更大但成本和延迟较高。

「评估指标」也有多种，包括上下文精度（Context Precision）和上下文召回率（Context Recall），以及NDCG、MAP等排名质量指标。

「优化技术」包括分块策略（Chunking Strategy）、重新排序（Reranking）、查询重写（Query Rewriting）和上下文检索（Contextual Retrieval），旨在提高检索相关性。

2）Agent（智能代理）。

Agent 被认为是 AI 的终极目标，能够感知环境并采取行动，通过工具使用和规划能力显著扩展模型功能，不仅解决上下文限制，还能实现自动化和与现实世界的交互。

Agent 概念源于Russell和Norvig的经典著作《Artificial Intelligence: A Modern Approach》（1995），强调 Agent 通过环境和工具定义其能力。Agent 的应用范围广泛，从网站创建到市场研究、行程规划等，经济潜力巨大，但理论框架尚未成熟，存在实验性。以SWE-agent为例（[Yang et al., 2024](https://arxiv.org/abs/2405.15793)），其环境为计算机终端，工具包括导航、搜索和编辑文件。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/OOKW7r.png)

规划（Planning）是 Agent 的核心，涉及任务分解、计划生成和执行。自回归语言模型（LLM）在规划中存在局限性，基于[Yann LeCun（2023）](https://x.com/ylecun/status/1702027572077326505)和[Kambhampati（2023）](https://oreil.ly/8_j7E)的观点，LLM可能不擅长规划。解法是「解耦规划与执行」，通过验证计划（如AI评判或启发式规则）避免无效执行，并支持并行生成多个计划以提高效率。然后，规划粒度（Granularity）和控制流（Control Flow）是关键考量，支持自然语言计划生成以适应工具变化。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/OngTlQ.png)

反思与错误纠正（Reflection and Error Correction）。反思是提升 Agent 性能的重要机制，ReAct框架（[Yao et al., 2022](https://arxiv.org/abs/2210.03629)）和Reflexion框架（[Shinn et al., 2023](https://arxiv.org/abs/2303.11366)）通过交替推理和行动、评估与自我反思，显著改进 Agent 表现。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/21EpSX.png)

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/LVWIWb.png)

Agent 的独特失败模式包括规划失败（如工具使用错误、目标未达成）和效率问题（如步骤过多、成本高）。评估建议包括分析无效计划比例、工具调用错误率及任务完成时间和成本，然后对比基线（如人类操作者）很重要。

3）Memory（内存系统）。

内存系统对于RAG和 Agent 等信息密集型应用至关重要，通过内部知识、短期内存（上下文）和长期内存（外部数据源）三种机制支持信息的存储和利用。

短期内存受限于模型上下文长度，长期内存通过外部存储扩展容量，适用于跨任务持久化信息。内存管理涉及添加和删除操作，常用策略如FIFO（先进先出）可能丢失重要信息，更复杂策略包括总结和去冗余（[Bae et al., 2022](https://arxiv.org/abs/2210.08750); [Liu et al., 2023](https://arxiv.org/abs/2311.08719v1)）。内存系统的优势包括处理信息溢出、保持一致性、维护数据结构完整性等，尤其对个性化服务和多步骤任务有益。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/4ilaJG.png)

**7、Chapter 7：Finetuning 。**

Finetuning 是通过进一步训练模型全部或部分权重，将模型适应特定任务的过程，相较于基于提示的方法（如第5、6章所述），它通过调整模型权重实现定制化。

Finetuning 属于迁移学习（Transfer Learning）的一种形式，旨在利用基础模型的已有知识，应用于数据较少的特定任务（如法律问答或文本转SQL）。它提升了样本效率，例如，OpenAI 的 InstructGPT 论文（2022）指出，Finetuning 可以“解锁”模型已有但难以通过提示访问的能力。Finetuning 包括多种类型，如自监督继续预训练、监督微调和偏好微调，并可用于扩展模型上下文长度（如 Code Llama 模型从 4096 个 token 扩展到 16384 个 token）。（注：迁移学习概念最早由 Bozinovski 和 Fulgosi 在 1976 年提出，Google 的多语言翻译系统（Johnson et al., 2016）是早期成功案例。）

Finetuning 需要更多资源投入（数据、硬件和机器学习专业知识），因此应在提示方法实验充分后考虑，且两者并非互斥，可结合使用。Finetuning 的主要目的是提升模型质量，尤其是在特定任务（如 JSON 格式输出）或领域（如 SQL 方言）上的表现。同时，微调小型模型（如 Grammarly 使用 Flan-T5，仅用 82,000 个样本即超越 GPT-3 变体）更常见且成本较低。

Finetuning 并非总是最佳选择，因其可能导致其他任务性能下降，且前期投入和持续维护成本高，提示方法往往可替代。Finetuning 可能导致“对齐税”（Alignment Tax），即特定任务提升的同时其他任务性能下降。此外，获取标注数据、模型训练知识和部署维护（如监控和更新策略）都是挑战。所以，建议从提示工程入手，仅在提示不足时探索高级解决方案。

选择 Finetuning 还是 RAG（检索增强生成）取决于模型失败是信息缺失还是行为问题，前者适合 RAG，后者适合 Finetuning。RAG 适用于信息缺失或过时问题，如 Ovadia et al. (2024) 研究显示，在当前事件问答任务中，RAG 性能（Mistral-7B 提升至 0.875）优于 Finetuning（最高 0.588）。Finetuning 则适用于行为问题，如输出格式不符或无关内容。所以，“Finetuning 针对形式，RAG 针对事实”，同时两者可结合使用。

Finetuning 不太用得到，离我太远了，剩下的就简略地过一遍吧。1）Finetuning 内存需求高，许多技术（如 PEFT 和量化）旨在降低内存占用，理解内存瓶颈有助于选择合适的微调方法，2）数值表示（如 FP32、FP16、BF16）直接影响内存占用，量化（如减少位数）是降低内存足迹的有效方法，3）为应对内存挑战，参数高效微调（PEFT）成为主流，LoRA 作为最受欢迎的适配器方法，因其模块化和效率广受关注，4）模型合并通过组合多个模型创建定制模型，降低内存占用并支持多任务微调，相较于单独使用各模型更具价值，5）Finetuning 实践涉及选择基础模型、方法和框架，以及调整超参数以优化效率和性能。

**8、Chapter 8：数据集工程。**

高质量的数据集是训练优秀模型的基础，尤其在资源有限的情况下，如何高效构建数据集成为关键问题。

数据集工程的目标是创建能够训练出最佳模型的数据集，尤其是在预算限制内。随着越来越多的公司无法从头开发模型，数据成为区分AI性能的关键因素。数据操作已从边缘任务演变为专业角色，许多AI公司雇佣数据标注员、数据集创建者和数据质量工程师。数据景观比模型景观更复杂，涉及不断增长的数据集和技术的多样性。从GPT-3到GPT-4，数据工作的重视程度显著提升，GPT-3仅2人负责数据收集，而GPT-4有80人参与数据相关工作（不包括外包标注员）。

数据策展（Data Curation）是解决AI模型问题的重要手段，需理解模型学习机制和可用资源，数据构建者应与应用和模型开发者密切合作。数据需求因任务和训练阶段而异，例如自监督微调需要序列数据，指令微调需要（指令，响应）格式，偏好微调需要（指令，优胜响应，劣势响应）格式。复杂行为如链式推理（CoT）和工具使用的数据获取尤为挑战。研究表明，加入CoT响应数据可显著提升模型在CoT任务上的表现，某些任务准确率几乎翻倍（Chun et al., 2024）。

关于数据质量（Data Quality）。少量高质量数据可胜过大量噪声数据，数据质量定义因任务而异，但普遍需具备相关性、任务一致性、一致性、正确格式、独特性及合规性。Yi模型团队发现，10K精心设计的指令优于数十万噪声指令（Young et al., 2024）。LIMA研究表明，用1000个精心策展的提示和响应微调的65B参数Llama模型，在43%的案例中可媲美或优于GPT-4（Zhou et al., 2023）。

剩下的不太关注，粗略总结下。1）训练数据需覆盖模型预期解决的问题范围，数据多样性对模型性能至关重要，2）数据需求量因情况而异，少至单个示例，多至数百万示例，受微调技术、任务复杂性和基础模型性能等因素影响，3）数据获取目标是生成符合质量和多样性需求的大数据集，同时尊重隐私和合规性，应用自身数据是理想来源，4）数据增强和合成是解决数据短缺的重要手段，增强基于真实数据，合成则模拟真实数据特性，5）尽管合成数据日益重要，但无法完全取代人类生成数据，因质量控制、模仿局限、模型崩溃及数据谱系模糊等问题，6）模型蒸馏（Model Distillation）通过大模型（教师）生成数据训练小模型（学生），以降低部署成本并保持性能，7）数据处理需根据用例要求进行，包括数据检查、去重、清洗、过滤及格式化。

**9、Chapter 9：推理优化。**

无论模型性能如何优越，若推理速度过慢或成本过高，将影响用户体验和商业价值。例如，股票价格预测模型若计算时间超过一天，其预测将失去意义。推理优化是一个跨学科领域，涉及模型研究者、应用开发者、系统工程师等多方协作。优化可以在模型层面（如减小模型大小）、硬件层面（如设计更强大的硬件）和服务层面（如资源分配优化）展开。

推理是AI模型生命周期中将输入转化为输出的阶段，通常由推理服务器执行。计算瓶颈分为计算受限（Compute-bound）和内存带宽受限（Memory Bandwidth-bound）。计算受限任务（如密码解密）受计算能力限制；内存带宽受限任务（如数据在CPU与GPU间传输）受数据传输速率限制。可以通过“Roofline”模型（Williams et al., 2009）来区分两种瓶颈，同时不同模型架构有不同瓶颈，例如图像生成模型（如Stable Diffusion）多为计算受限，而语言模型（如Transformer）多为内存带宽受限。语言模型推理分为预填充（Prefill，计算受限）和解码（Decode，内存带宽受限）两个步骤。

推理API分为在线API（优化延迟）和批量API（优化成本），适用于不同场景。在线API适合低延迟需求场景（如聊天机器人），而批量API适合延迟要求不高的场景（如合成数据生成），成本可降低50%（以Google Gemini和OpenAI为例）。在线API还支持流式模式（Streaming Mode），减少用户等待首个token的时间，但可能增加不良响应的风险。

推理性能指标包括延迟（Latency）、吞吐量（Throughput）和利用率（Utilization）。1）延迟：包括首次token时间（TTFT）和每输出token时间（TPOT），TTFT对聊天机器人需瞬时响应，而TPOT需接近人类阅读速度（约120ms/token），2）吞吐量：以token/秒（TPS）或请求/分钟（RPM）衡量，与成本直接相关，例如，硬件成本2美元/小时，吞吐量100 token/秒，则每百万token成本约5.556美元，3）利用率：包括模型FLOP/s利用率（MFU）和带宽利用率（MBU），例如，PaLM模型在TPU v4上MFU达46.2%（Chowdhery et al., 2022），而训练MFU通常高于推理。

硬件对推理速度和成本有决定性影响，AI加速器（如GPU、TPU）专为AI工作负载设计。GPU与CPU的主要区别在于并行处理能力，GPU拥有数千个小核心，适合矩阵乘法等AI任务。NVIDIA H100芯片在不同精度下的FLOP/s性能（例如FP8 Tensor Core达3958 teraFLOP/s）；GPU内存带宽（HBM）高达1.5 TB/s以上，远超CPU内存（25-50 GB/s）；NVIDIA H100年耗电约7000 kWh，接近美国家庭平均年耗电（10000 kWh）。

模型优化通过修改模型本身提高效率，可能影响模型质量。1）模型压缩：包括量化（Quantization）、蒸馏（Distillation）和剪枝（Pruning）。量化（如从32位减到16位）可将内存占用减半；剪枝可减少90%非零参数（Frankle and Carbin, 2019），但实际应用较少。2）自回归解码优化：针对语言模型解码瓶颈，提出推测解码（Speculative Decoding，使用较弱模型生成草稿token并验证）、参考推理（Inference with Reference，从输入复制token）和并行解码（Parallel Decoding，同时生成多个token）。3）注意力机制优化：针对Transformer模型，优化KV缓存（存储键值向量）和注意力计算。技术包括局部窗口注意力（Local Windowed Attention）、多查询注意力（Multi-Query Attention）和FlashAttention内核。

服务优化通过资源管理提高效率，不改变模型输出质量。1）批处理（Batching）：包括静态、动态和连续批处理，提升吞吐量但可能增加延迟。2）预填充与解码分离：将两者分配到不同实例，避免资源竞争，显著提升请求处理量（Zhong et al., 2024）。3）提示缓存（Prompt Caching）：缓存重叠提示段（如系统提示），降低延迟和成本。Anthropic报告称缓存可减少90%成本和75%延迟（2024）。4）并行化：包括副本并行（Replica Parallelism）、张量并行（Tensor Parallelism）和流水线并行（Pipeline Parallelism），提升处理能力但可能增加通信开销。

**10、Chapter 10：AI 工程架构与用户反馈。**

1）AI 工程架构的逐步构建。

先构建最简单架构：初始架构仅包括用户查询输入、模型API生成响应和返回结果的直接流程。无上下文增强、防护措施或优化，适用于基础应用，但存在局限性。

然后逐步增强架构。

STEP 1：上下文增强。通过检索机制（如文本、图像、表格数据）和工具（如API获取天气、新闻等）为模型提供必要信息，类比为特征工程，提升输出质量。不同模型API提供商在上下文支持上存在差异，如文件上传数量限制和工具执行模式（并行或长任务）。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/Fz55LY.png)

STEP 2：防护措施。添加输入和输出防护措施。输入防护防止隐私泄露（如员工将公司机密输入第三方API）和恶意提示攻击；输出防护捕捉模型失败（如空响应、格式错误、毒性内容）并制定应对策略（如重试逻辑、并行调用或转接人工）。防护措施需权衡可靠性和延迟，部分团队因延迟问题放弃防护。文中提及的防护工具包括Meta的Purple Llama和NVIDIA的NeMo Guardrails。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/D7YHb3.png)

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/lOxVe2.png)

STEP 3：模型路由与网关。通过路由器根据用户意图（intent classifier）将查询分配至不同模型或解决方案，优化性能和成本（如将简单查询路由至廉价模型）；网关提供统一接口、访问控制和成本管理，增强安全性与维护便利性。文中列举的网关工具包括Portkey的AI Gateway和MLflow AI Gateway。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/chqTO1.png)

STEP 4：缓存优化延迟。引入精确缓存和语义缓存机制以降低延迟和成本。精确缓存针对完全相同的查询重用结果，语义缓存基于查询相似性重用结果，但后者依赖高质量嵌入和向量搜索，效果存疑且计算成本较高。文中警告缓存可能导致数据泄露风险，如个性化响应被错误缓存并共享。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/BFjJCV.png)

STEP 5：智能体模式。通过智能体模式支持复杂应用流程，如循环、并行执行和条件分支，并允许模型执行写操作（如发送邮件、发起转账），但需谨慎管理风险。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/2ZbdZ4.png)

关于监控与可观测性。监控很重要，有三大指标：MTTD（平均检测时间）、MTTR（平均响应时间）和CFR（变更失败率）。监控需围绕失败模式设计指标（如幻觉检测、API成本、延迟），并结合日志和追踪（trace）分析问题根因。基础模型引入了新的失败模式，需定制化指标，如格式失败、毒性内容和用户对话信号（用户中途停止生成、对话轮数）。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/p3CpNC.png)

关于 AI 管道编排。复杂AI应用需编排工具协调多模型、数据库和工具的数据流，分为组件定义和链式组合两步。文中推荐工具包括LangChain和LlamaIndex，并建议初期避免使用编排工具以降低复杂性。

2）用户反馈：AI应用中的数据飞轮与设计策略。

用户反馈作为竞争优势，帮助个性化模型和训练新版本，尤其在数据稀缺背景下更显珍贵。

「收集时机」可以贯穿用户旅程，特别是在初始校准、错误发生和模型低置信度时收集反馈。比如 DALL-E的inpainting功能和Google Gemini的并排比较响应，便于用户提供高质量反馈。避免在正面结果时频繁请求反馈，以免干扰用户体验。「收集方式」需融入用户流程，不干扰体验，易于忽略，并提供激励。

![](https://cdn.jsdelivr.net/gh/sorrycc-bot/image-2025-04@main/uPic/dZUO63.png)

参考：  
[https://github.com/chiphuyen/aie-book](https://github.com/chiphuyen/aie-book)  
[https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302](https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302)
