---
title: "426 - 《Umi Cli 提速》"
date: 2024-03-26
url: https://sorrycc.com/umi-cli-speedup
---

发布于 2024年3月26日

# 426 - 《Umi Cli 提速》

背景是虽然我们内部已切换到 Mako 做构建，但上层框架 Bigfish/Umi 有大量的 Node 逻辑依旧比较慢，对于开发者来说，体感提升并没有很强烈，所以需要排查下 Umi Cli 的卡点问题并做出改进。

记录下我的排查步骤。

1、找典型项目。

需要多找几个，我找了 5 个，包含脚手架项目、大型和中型项目等。不同类型的项目会暴露出不同的卡点问题。

2、跑 benchmark 记录数据。

找个现状的数据，记录下来，方便修改后知道有多少提升。我是用 hyperfine 跑 setup 命令。umi 的 setup 命令剔除了 build，用来做非 build 性能验证非常合适。

```bash
$ hyperfine -w 1 --runs 10 './node_modules/path/to/bin/bigfish.js setup'
  Time (mean ± σ):     10.647 s ±  0.318 s    [User: 9.415 s, System: 1.838 s]
  Range (min … max):   10.509 s … 11.549 s    10 runs
```

3、跑 node --cpu-prof + speedscope 。

方法在 [424 - 《Node 性能优化（2）》](https://sorrycc.com/node-performance-optimization-02) 里有过说明，就不重复了，跑完传到 [https://www.speedscope.app/](https://www.speedscope.app/) 上分析。

```bash
node --cpu-prof ./node_modules/path/to/bin/bigfish.js setup
```

4、分析。

某个典型项目的「Time Order」图如下。

![](https://img.alicdn.com/imgextra/i2/O1CN01ULx5Xe1HMncAlaysN_!!6000000000744-2-tps-3466-1748.png)

我加了一些标注：

*   **网络请求**，Bigfish 框架会有个前置请求获取当前项目信息，后续会基于此做强约束校验等
*   **配置读取**，Umi 的配置支持 ts 等高级语法，编译时会用 esbuild 做 JIT 编译
*   **model 解析**，Dva 和 use-model 的插件会分析 models 目录下的文件和 model.(j|t)sx? ，基于 Babel，决定是否为合法的 model 文件，如果是，会被自动载入，这个操作比想象中慢，尤其是 model 文件比较多的时候
*   **源码预编译**，Umi 会对源码部分的代码做预编译，以此得到一些信息，用于 mfsu、icons、强约束等场景的使用
*   **源码预编译产物 deepClone**，应该是可以去掉的

5、Action。

上面这张图的场景，感觉可以优化到 2-3 s。

*   优化网络请求部分逻辑，利用缓存，由于项目信息基本上不会二次变更，所以可以存起来，有缓存时先用缓存，然后异步更新数据，这部分时间就可以省掉了
*   配置读取的部分还待细看，现在已经用了 esbuild 来编译配置文件了，为啥还需要几百毫秒，考虑换方案（比如 [c12](https://github.com/unjs/c12)）或者基于文件内容 hash 加缓存
*   model 解析有几种思路，1）现在基于 babel 分析太慢，可以手写 ast 分析或者换成特征分析，或者用正则？2）并行，model 较多的时候应该效果比较好，继续用 node 的话可以利用 worker，3）缓存，基于文件内容 hash
*   源码预编译，长远看，应该要集成到 Mako 的流程里，换 rust 实现应该会比 esbuild + resolve 的单线程要快不少，Mako 拆 build 和 generate 的 node api
*   deepClone 要去掉，可能得舍去部分功能

6、总结。

TODO，等完成后再更新此篇文档。
