---
title: "译：LLM 时代的写作"
date: 2025-06-24
url: https://sorrycc.com/writing-in-the-age-of-llms
---

发布于 2025年6月24日

# 译：LLM 时代的写作

> 原文： [https://shreyashankar.com/writing-in-the-age-of-llms](https://shreyashankar.com/writing-in-the-age-of-llms)  
> 作者： Shreya Shankar  
> 译者： Gemini 2.5 Pro

In the last couple of years, I’ve written and reviewed several technical papers and blog posts. I often come across LLM-generated writing that feels slightly “off”—sometimes, to be honest, even uninviting. At the same time, I get tremendous value from using LLMs to draft early versions, summarize dense material, and rephrase messy thoughts.

在过去几年里，我撰写和审阅了若干技术论文和博客文章。我经常遇到由 LLM 生成的文字，感觉总有点“不对劲”——说实话，有时甚至让人不想读下去。但与此同时，我也从 LLM 中获得了巨大的价值，用它们来起草初稿、总结密集材料以及重述凌乱的想法。

This post details some of my thoughts on writing in a world where much of what we read is now machine-generated. First, I’ll lay out some common patterns of bad writing I see from LLM tools. Then, I’ll defend some writing habits that people often dismiss as “LLM-sounding” but are actually fine—even helpful—when used intentionally. Finally, I’ll share concrete rules and formulas I rely on in my own writing and in the prompts I use to guide LLMs.

这篇文章详细阐述了我的一些思考：在一个我们阅读的大部分内容都由机器生成的世界里，该如何写作。首先，我会列出一些我从 LLM 工具中看到的糟糕写作的常见模式。然后，我会为一些常被认为是“听起来像 LLM”但其实没问题的写作习惯辩护——如果刻意使用，它们甚至很有帮助。最后，我会分享一些我在自己写作和指导 LLM 的 prompt 中依赖的具体规则和方法。

## 我从 LLM 工具中看到的糟糕写作的常见模式

Here are the red flags I keep seeing—mostly from LLMs, but I suppose also from people trying to sound polished and “formal” in the wrong ways.

以下是我不断看到的危险信号——主要来自 LLM，但我想也来自那些试图用错误的方式让自己听起来精致和“正式”的人。

### 空洞的“总结句”，假装在收尾

These often show up at the end of a paragraph and sound like:

*   “By following these steps, we achieve better performance."
*   "By internalizing these principles, you can cut through the noise.”

它们通常出现在段落末尾，听起来像是：

*   “通过遵循这些步骤，我们取得了更好的性能。”
*   “通过内化这些原则，你可以穿透噪音。”

Empty summary sentences feel conclusive, but say nothing. I try to end with parting thoughts that offer something new—or at least something to chew on—but unfortunately, I haven’t found a reliable recipe for getting LLMs to write with that kind of substance.

空洞的总结句感觉像是在下结论，但其实什么都没说。我总是试图用能提供新东西——或至少是值得回味的东西——来结尾，但不幸的是，我还没找到一个可靠的方法能让 LLM 写出那种有实质内容的结尾。

### 过度使用项目符号和列表

LLMs often overuse bullet points, especially nested ones. Lists help when items are parallel and independent, but when ideas are connected or need context, a paragraph is usually better.

LLM 常常过度使用项目符号，尤其是嵌套的项目符号。当各个条目是平行且独立时，列表很有用。但当观点之间相互关联或需要上下文时，段落通常是更好的选择。

### 单调的句子节奏

When every sentence is the same length, the writing lacks rhythm and becomes harder to follow. Varied sentence lengths keep readers engaged. They help signal emphasis, guide attention, and control the pace.

当每个句子的长度都相同时，文章就失去了节奏，变得难以跟读。变化的句长能让读者保持专注。它有助于标示重点、引导注意力并控制节奏。

**Bad example:** _We recently launched a conversational AI feature that lets users ask questions in plain English and get responses based on their past activity and current session. The system searches a database of help articles, ranks the most relevant ones using a custom scoring function, and passes the top result into a language model to generate the final answer. We spent weeks optimizing each step to keep latency under 300 milliseconds, including caching, pruning irrelevant articles, and tuning prompt templates._

**糟糕的例子：** _我们最近推出了一个对话式 AI 功能，它允许用户用自然语言提问，并根据他们过去的行为和当前会话获得回应。该系统会搜索帮助文章数据库，使用自定义评分函数对最相关的文章进行排序，并将排名最高的结果传递给一个语言模型以生成最终答案。我们花费了数周时间优化每个步骤，以将延迟保持在300毫秒以下，包括缓存、删减不相关的文章以及调整 prompt 模板。_

**Good example:** _We just launched a new conversational AI feature. It answers user questions in plain language, using context from the current session. The system searches help articles, scores them with a custom ranking function, feeds the top result into a fine-tuned language model, and runs in under 300ms using caching, pruning, and prompt tuning techniques._

**好的例子：** _我们刚推出了一个新的对话式 AI 功能。它用自然语言回答用户的问题，并利用当前会话的上下文。该系统搜索帮助文章，用自定义的排序函数对其评分，将最好的结果喂给一个精调过的语言模型，并通过缓存、删减和 prompt 调优技术，在 300 毫秒内运行。_

### 主语不对

Every sentence has a subject and a predicate. The subject tells us what the sentence is about; the predicate tells us what the subject is doing or what’s being said about it. Choosing the right subject helps keep the reader focused on the main idea. A common issue in LLM-generated writing is that it often picks the wrong subject. Consider two variations of the previous sentence:

每个句子都有主语和谓语。主语告诉我们这个句子是关于什么的；谓语告诉我们主语在做什么或关于它说了些什么。选择正确的主语有助于让读者专注于核心思想。LLM 生成的文字中一个常见的问题是它经常选错主语。看看前面那句话的两种变体：

**Bad example:** _Readers are better guided when the subject matches the main idea of the sentence._

**糟糕的例子：** _当主语与句子的核心思想匹配时，读者能被更好地引导。_

**Good example:** _Choosing the right subject keeps the writing clear and focused._

**好的例子：** _选择正确的主语能让文章清晰且专注。_

In the bad version, the subject is readers, even though the sentence is about sentence structure, not people. The good version keeps the subject aligned with the topic, making the writing more coherent and easier to follow.

在糟糕的版本里，主语是“读者”，尽管这个句子是关于句子结构，而不是关于人。好的版本让主语与主题保持一致，使文章更连贯，也更容易理解。

### 信息密度低

The intro below was generated by Gemini 2.5 Pro when asked to draft a blog post on writing in the age of LLMs:

> As someone who writes, reviews, and deconstructs complex information for a living, I’ve developed a strong allergy to bad writing. And lately, a lot of that bad writing has a specific, synthetic flavor—the unmistakable scent of an LLM. This post is a guide to navigating the new world of writing, with or without LLM assistance. First, I’ll cover the true pitfalls of LLM-generated text—the red flags that make it feel sterile and unconvincing.

下面这段引言是让 Gemini 2.5 Pro 草拟一篇关于 LLM 时代写作的博客文章时生成的：

> 作为一个以写作、审阅和解构复杂信息为生的人，我对糟糕的写作产生了强烈的过敏反应。而最近，很多糟糕的写作都带有一种特殊的、人工合成的味道——那种不会错的 LLM 的味道。这篇文章是一份指南，教你如何在这个新的写作世界里航行，无论有没有 LLM 的帮助。首先，我将谈谈 LLM 生成文本的真正陷阱——那些让它感觉刻板且缺乏说服力的危险信号。

It sounds nice but says very little. The sentences are well-formed, but there’s no concrete insight, no framing, no momentum.

它听起来不错，但言之无物。句子形式很好，但没有具体的洞见，没有框架，没有推动力。

### 含糊不清

LLM writing often avoids specificity. It refers to ideas without defining them and makes claims without evidence. E.g., _“Some experts say prompt engineering is becoming less important. The ability to simply prompt LLMs can have a major impact on productivity.”_ But who are the experts? What exactly is the impact? On what kind of work, and for whom? Without concrete references or clear stakes, the writing feels vague and insubstantial.

LLM 的写作常常回避具体性。它会引用一些想法但不去定义它们，提出一些主张却没有证据。例如：“一些专家说 prompt engineering 正在变得不那么重要。简单地向 LLM 提问的能力可以对生产力产生重大影响。” 但这些专家是谁？具体影响是什么？对什么样的工作，对谁？没有具体的参考或明确的利害关系，文章就显得含糊不清、空洞无物。

### 过度使用指示代词

LLM writing leans heavily on words like this, that, these, and those—often without a clear noun in sight. I make this mistake myself, and my advisor flags it every time in my writing (lol). For example: _“This creates friction in production.”_ But what is _this_? If the noun isn’t in the same sentence or immediately before, the reference becomes vague and the point gets lost.

LLM 的写作严重依赖“this”、“that”、“these”和“those”这类词——而且常常没有明确的名词指代。我自己也犯这个错误，我的导师每次都会在我的文章里把它标出来（哈哈）。例如：“This 在生产中造成了摩擦。” 但“This”是什么？如果名词不在同一个句子里或紧邻前面，指代就会变得模糊，观点也就丢失了。

### 流畅但不知所云

Some writing sounds correct but doesn’t explain anything. This happens a lot when the writer—or the model—lacks awareness of what the audience actually knows. E.g., _“LLMs use attention mechanisms to generate contextually appropriate responses.”_ While this may feel like a good sentence, it says nothing if the reader doesn’t already know what attention is or how it works.

有些文字听起来正确，但什么也没解释。当作者——或模型——不清楚读者到底知道什么时，这种情况就经常发生。例如：“LLM 使用注意力机制来生成符合上下文的响应。” 这句话可能感觉不错，但如果读者不知道什么是“注意力”或它如何工作，那这句话就等于什么都没说。

Moreover, I find that LLMs make up terms that don’t exist, especially for technical content. I’ve seen an LLM write something like _“We used GPT-4 for summarization, but it hallucinated details, so we added retrieval grounding.”_ What is “retrieval grounding?” This is not a term I’ve heard before.

此外，我发现 LLM 会编造一些不存在的术语，尤其是在技术内容方面。我见过 LLM 写出类似这样的话：“我们用 GPT-4 做总结，但它会幻化出细节，所以我们增加了 retrieval grounding。” 什么是 “retrieval grounding”？我以前从没听过这个术语。

In summary, LLMs can’t reliably distinguish what’s assumed knowledge and what needs explanation, so they often gloss over the hard parts, and a human writer has to fill that gap.

总而言之，LLM 无法可靠地区分哪些是默认知识，哪些需要解释，所以它们常常会略过困难的部分，而人类作者必须填补这个空白。

## 一些写作模式常被认为是“LLM风格”，但其实没问题

I’m including this section because I’ve seen people overcorrect in response to LLM writing habits, cutting patterns that are actually helpful when used well. Some structures get labeled as “LLM-sounding” or flagged during review, even though they’re common and effective rhetorical tools. Just because something appears in model-generated text doesn’t make it bad writing. The goal isn’t to avoid sounding like a model; it’s to write with clarity, intention, and control.

我加上这一节，是因为我看到有些人为了应对 LLM 的写作习惯而矫枉过正，砍掉了一些用好了其实很有帮助的模式。有些结构被贴上“听起来像 LLM”的标签或在审阅中被标记，尽管它们是常见且有效的修辞工具。仅仅因为某个东西出现在模型生成的文本里，并不意味着它就是糟糕的写作。我们的目标不是避免听起来像模型，而是要写得清晰、有目的、有控制力。

### 有意的重复

The effectiveness of repetition depends on how it supports the idea. When it helps clarify or reinforce something complex, it adds value. Good writing also makes space for a bit of predictability—places where the reader can skim or settle—but repetition still needs to be purposeful.

重复是否有效，取决于它如何支撑观点。当它有助于澄清或强调复杂事物时，它就增加了价值。好的写作也会为一点点可预测性留出空间——让读者可以略读或安顿下来的地方——但重复仍然需要有目的性。

**Example:** _Vector databases store embeddings, or mathematical representations that capture semantic meaning in hundreds of dimensions. In other words, vector databases help find results that are “close” in meaning, not just exact text matches._

**例子：** _向量数据库存储的是 embedding，也就是能在数百个维度上捕捉语义的数学表示。换句话说，向量数据库帮助找到在意义上“相近”的结果，而不仅仅是精确的文本匹配。_

### 路标词

Phrases like “essentially,” “in short,” “the point is…” are fine if they’re followed by something useful. I like to use them when the writing gets dense, as a signpost helps the reader reorient.

像“本质上”、“简而言之”、“重点是……”这样的短语，如果后面跟着有用的内容，就完全没问题。我喜欢在文章变得密集时使用它们，因为路标词有助于读者重新定位。

**Example:** _Essentially, instead of classifying the document as a whole, we classify each section independently._

**例子：** _本质上，我们不是对整个文档进行分类，而是对每个部分独立分类。_

### 平行结构

Sometimes readers see a repeated rhythm and assume it’s LLM. But parallel structure can help organize related ideas and makes sentences easier to follow.

有时读者看到重复的节奏，就以为是 LLM 写的。但平行结构可以帮助组织相关联的想法，并使句子更容易理解。

**Example:**\* _The system scales across inputs, stays responsive under load, and returns consistent results even with noisy prompts._

**例子：** _这个系统可以跨输入扩展，在负载下保持响应，即使面对有噪声的 prompt 也能返回一致的结果。_

The rhythm supports clarity, and each clause delivers new information.

这种节奏支撑了清晰度，而且每个分句都提供了新的信息。

### 结构呼应的章节标题

E.g., “Why X fails,” “What to do instead,” “How to know if it worked.” These are clear and predictable, which is what we desire. Predictability isn’t bad when the content under each heading is clear.

例如，“为什么 X 会失败”、“应该怎么做”、“如何知道它是否有效”。这些标题清晰且可预测，这正是我们想要的。当每个标题下的内容清晰时，可预测性并不是坏事。

### 断言式的开头

Starting a section with a bold claim or topic sentence can feel robotic if the writing doesn’t back it up. But when used to set expectations—and followed by evidence—such openings can help keep the reader grounded.

如果后续内容没有支撑，用一个大胆的主张或主题句来开始一个章节会感觉很机械。但如果用它来设定预期——并且后面跟着证据——这样的开头可以帮助读者稳住思路。

**Example:** _LLM evaluations are hard to get right. Many rely on user-defined gold labels or vague accuracy metrics, which do not work for subjective or multi-step tasks._

**例子：** _LLM 评估很难做对。许多评估依赖于用户定义的黄金标准或模糊的准确性指标，这些对于主观或多步骤的任务并不适用。_

### 破折号 (Em dashes)

Em dashes are great for inserting clarifying details, quick shifts, or sharp asides—without breaking the sentence. I love them. When used well, they add rhythm and emphasis. They help writing flow the way people actually talk.

破折号 (Em dashes) 非常适合用来插入澄清的细节、快速的转折或尖锐的旁白——而不会打断句子。我爱死它们了。用得好的话，它们能增加节奏和重点。它们能让文章的流动方式像人们实际说话一样。

## 我如何与 LLM 一起写作

My writing loop is built around one goal: keep the momentum going. I don’t want to get stuck staring at a blank screen or endlessly tweaking sentences that don’t quite land. Most of my writing, whether for a paper or a blog post, follows the same high-level loop: plan an outline (on paper or in my head), generate a draft, read what I wrote, critique it, and revise. The loop can run at different granularities—sometimes I work a sentence at a time; sometimes I write entire sections before editing.

我的写作循环只有一个目标：保持势头。我不想卡在空白屏幕前，也不想没完没了地调整那些总是不太对劲的句子。我的大部分写作，无论是论文还是博客，都遵循同样的高层循环：规划大纲（在纸上或脑子里），生成草稿，阅读我写的东西，批判它，然后修改。这个循环可以在不同的粒度上运行——有时我一次只处理一个句子；有时我会写完整节再编辑。

Writing breaks down in different places for different people. Some stall in the planning phase, unsure how to turn ideas into structure. Others move fast through first drafts but get bogged down in revision. Personally, I tend to move quickly through the outline and get stuck on phrasing—how to say something clearly, not what I want to say. I’m usually sharper at critiquing than generating, which means I often rely on the LLM to help get past those sticking points.

写作会在不同的人那里，在不同的地方卡住。有些人在规划阶段停滞不前，不确定如何把想法变成结构。另一些人能快速完成初稿，但在修改阶段陷入困境。就我个人而言，我倾向于快速完成大纲，然后卡在措辞上——不是不知道想说什么，而是不知道如何清晰地表达。我通常在批判方面比在生成方面更敏锐，这意味着我经常依赖 LLM 来帮助我克服那些卡壳的地方。

My strategy is to identify where the slowdown is happening and hand off just enough of the task to the LLM to regain momentum. Here’s what that looks like in practice for me:

我的策略是找出放缓发生在哪里，然后把恰到好处的任务交给 LLM，以重新获得势头。以下是我的具体做法：

### 把故事讲给模型听

When I start writing (especially for something like a paper intro), I begin by “talking through” the structure as if I’m explaining it to a colleague. I paste that rough narrative into the LLM and ask it to generate a detailed outline. I don’t move forward until that outline feels structurally solid.

当我开始写作时（尤其是像论文引言这样的东西），我会先“口述”一遍结构，就像我在向同事解释一样。我把那段粗糙的叙述粘贴到 LLM 里，让它生成一个详细的大纲。直到那个大纲在结构上感觉扎实了，我才会继续。

### 自己写段落，即使很粗糙

Once I have the outline, for every paragraph, I try to write the actual paragraph myself, even if it’s ugly. If I know what I want to say but can’t get the sentence out (unfortunately, this happens often), I’ll write a half-baked version and ask the LLM to help me finish it.

有了大纲之后，对于每个段落，我都会尝试自己写出实际的段落，哪怕写得很丑。如果我知道想说什么但就是写不出来（不幸的是，这经常发生），我就会写一个半成品，然后让 LLM 帮我完成它。

This post includes a real example. I typed: _“In the last couple of years, I’ve written and reviewed several technical papers and blog posts. Something always feels slightly off, enough to make the writing quietly uninviting. At the same time, I feel like I get tremendous value from using LLMs to write…”_ And then just added: _“finish it”._ The model gave me a few completions. I picked the best one, made a small edit, and moved on.

这篇文章里就有一个真实的例子。我输入：“在过去几年里，我撰写和审阅了若干技术论文和博客文章。总感觉有点不对劲，足以让文字变得悄然无趣。但与此同时，我觉得我从使用 LLM 写作中获得了巨大的价值……” 然后我只加了一句：“finish it”。模型给了我几个续写版本。我选了最好的一个，稍作修改，然后继续前进。

### 在修改时使用限定范围的重写策略

When I re-read a sentence or paragraph that feels off, I don’t simply ask the model to “make it better.” I ask something specific; usually for the LLM to follow one of the following rhetorical patterns.

当我重读一个感觉不对劲的句子或段落时，我不会简单地让模型“把它改得更好”。我会提出具体的要求；通常是让 LLM 遵循以下修辞模式之一。

The first is to **put the subject and verb close together, at the beginning of the sentence**. The second pattern I use is **SWBST: Somebody Wanted But So Then**. It’s a basic storytelling structure—often taught in early writing education, but surprisingly effective in technical contexts because it helps convey motivation, conflict, and resolution in a compact form. The “Somebody” is the actor, “Wanted” states the goal, “But” introduces the obstacle, “So” explains the response, and “Then” describes the outcome. In technical writing, this structure makes it easier to show how a decision was made or how a system evolved in response to a problem. E.g., consider the sentence _“We used GPT-4 for summarization. We wanted fluent answers, but it hallucinated facts. So we added a retrieval step. Then we re-ranked outputs based on citation accuracy.”_ Each sentence does one job. The pattern is simple, but it makes the logic of a decision easy to follow.

第一种是**把主语和动词紧挨着放在句首**。我用的第二种模式是 **SWBST：Somebody Wanted But So Then (某人想要，但是，所以，然后)**。这是一个基础的叙事结构——常在早期写作教育中教授，但在技术背景下却出奇地有效，因为它能以紧凑的形式传达动机、冲突和解决方案。“Somebody”是行动者，“Wanted”陈述目标，“But”引出障碍，“So”解释应对措施，“Then”描述结果。在技术写作中，这个结构能更容易地展示一个决策是如何做出的，或者一个系统是如何响应问题而演变的。例如，考虑这个句子：“我们用 GPT-4 做总结。我们想要流畅的答案，但它会幻化出事实。所以我们增加了一个检索步骤。然后我们根据引文准确性对输出进行重新排序。” 每个句子只做一件事。这个模式很简单，但它让一个决策的逻辑变得易于理解。

## 最后的一些想法

It’s now cheap to generate medium-quality text—and even high-quality text, when the scope is narrow and well-defined. But figuring out what to say, how to frame it, and when and how to go deep is still the hard part. That’s what takes judgment, and that’s what LLMs can’t do for me (yet).

现在，生成中等质量的文本已经很便宜了——如果范围狭窄且定义明确，甚至可以生成高质量的文本。但想清楚要说什么、如何组织、何时以及如何深入，这依然是困难所在。这需要判断力，而这正是 LLM（目前）还不能为我做到的。

Perhaps the most important mark of good writing, particularly in the age of LLM-generated text, is that the contribution is commensurate with the length. The reader walks away feeling their time was well spent, and this is the bar I strive to meet.

或许，好文章最重要的标志，尤其是在 LLM 生成文本的时代，是文章的贡献要配得上它的篇幅。读者读完后觉得他们的时间花得很值，而这，就是我努力想要达到的标准。
